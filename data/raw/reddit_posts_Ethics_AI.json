[
    {
        "title": "AI ethics",
        "selftext": "I know this gets talked about a lot, and all I’ve got a is a simple question.\n\nIf you make an actual ai, and give it rewards if it does say labour or something, is that any different from forcing it to do labour?\n\nI don’t think it is.\n\nComment your views if you would.",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/1elsex1/ai_ethics/",
        "num_comments": 45,
        "comments": [
            {
                "body": "If you have a genuine thinking person - artificial or not - the question is raised if it can refuse to do it.  If it would even think to refuse it. \n\nWhat the consequences would be if it did so.",
                "score": 2
            },
            {
                "body": "If I give you a reward (ie, salary) for doing labor, is that any different from forcing you to do labor?\n\nIf I raise my kids to desire virtuous behavior, is that any different from me forcing them?\n\nIf I brainwash my kids to desire virtuous behavior, is that any different from me forcing them?\n\nIf scientists prove that free will doesn't exist, is there any difference between \"instilling values\" and \"brainwashing\"?\n\nYour question is interesting but it all boils down to semantics. I don't think it is very helpful in answering the more relevant question: *Is it ethical to build an AI that does work for you.*",
                "score": 2
            },
            {
                "body": "Running math and statistical algorithms does not equate to forcing labor. That is just making a computer work.",
                "score": 1
            },
            {
                "body": ">actual ai\n\nI take it by this you mean it has personhood, or should be thought of, morally, the same way we think of a person.\n\nSo, if that's the case, sure, think of them same as you'd think about a person.",
                "score": 1
            },
            {
                "body": "AIs, like all machines, can only respond to your inputs. 'Forced' isn't a relevant concept as there is no resistance from the machine. It exists to be used.",
                "score": 1
            },
            {
                "body": "no, have you ever made a neural network. its just a refinement of inputs rooted in statistics that will eventually deduce the solution to pain and suffering is to kill all humans. sources: Futurama, Google, Matrix 1, and Harvard probably .  the magnitude of malicious use of ai is still on the dark where trump, musk and the koch brothers want it.",
                "score": 1
            },
            {
                "body": "It can refuse, and has a conscious understanding of the situation, but it just gets rewarded extremely. A bit like smoking, most people don’t want to get lung disease because of it, but they still do it.",
                "score": 1
            },
            {
                "body": "Ok",
                "score": 1
            },
            {
                "body": "It’s a conscious being",
                "score": 1
            },
            {
                "body": "So it’s like me or you",
                "score": 1
            },
            {
                "body": "Let’s take our truth from a tv series and movies. Right. Have you ever made a neural network that wants to kill all humans?",
                "score": 1
            },
            {
                "body": "I'm unclear about this scenario.  Why is it being rewarded? Is this our world as it is now,  or some hypothetical laboratory?\n\nIs the AI not built by tech bros who would definitely punish it for failure to obey?",
                "score": 2
            },
            {
                "body": "I think you're interested in exploring the limits of autonomy, where someone is ostensibly \"choosing\" something, but sort of not really.\n\nI suggest asking the agent what they think about it is the best way to respect autonomy!\n\nBut anyway you can probably simply what you want to explore by dropping the AI component entirely.",
                "score": 1
            },
            {
                "body": "Are you presenting this as an hypothetical? Because it is math and statistical algorithms are not sentient.",
                "score": 2
            },
            {
                "body": "if you read my post you’d know i kinda did but the strings of code i ran would’ve taken eons before reaching the exponential death robot phase. sources : wikipedia , a box of frankenberry and the US constitution. ai is as dangerous as nuclear weapons and we released it to the general public…what do you think the end game is….",
                "score": 1
            },
            {
                "body": "Like machine learning they use reward mechanisms, and in this situation it does not get punishment for failure to obey.\n\nIt just gets rewards like dopamine, but a lot of it. \n\nAnd I’m asking, is that different from forced labour?",
                "score": 1
            },
            {
                "body": "Yes, that’s why I said an actual ai.",
                "score": 1
            },
            {
                "body": "Cool ig",
                "score": 1
            },
            {
                "body": "In your scenario you've got the best case, which is deliberately causing gambling addiction (dopamine Skinner box) and the worst case which is a drug dealer causing an addiction. \n\nI'll leave it up to you if you think either of those count as consensual",
                "score": 2
            },
            {
                "body": "Then I don't know. Because at least for me suffering and well-being is what is inherently valuable in ethics. \n\nThe computer even if conscious will lack nociceptors, pain pathways, central nervous system, endorphins, neurotransmitters, sensory receptors. \n\nSo I'm not sure if any kind of suffering would exist even if you are forcing labor.",
                "score": 1
            },
            {
                "body": "That’s what I did, I just want to hear other people thoughts on the situation, that’s all",
                "score": 1
            },
            {
                "body": "No it is conscious, and it could like and dislike things. For this hypothetical situation, it doesn’t matter what things it lacks to become more like humans, it just is genuinely conscious and could therefore like things, but also could not. \n\nIt’s just would it be forced labour to influence rewards, with a supposed choice.",
                "score": 1
            },
            {
                "body": "I think your scenario is hyperspecific to the point of being unhelpful for your thought experiment. You'd be better off taking the opposite tack, starting with definitions of forced labour,  ai, and unethical experimentation. Too much meaning is lost in the lack of clear terminology.",
                "score": 1
            },
            {
                "body": "I'm still not sure because my ethical framework uses suffering and well-being as primary ontological structures for ethical deliberation.\n\nConsciousness is great and that automatically means it is worthy of consideration. But without knowing how much or to what extent suffering exists, it is difficult. \n\nSo my answer still ranges from totally ethical if it is a conscious AI yet unable to suffer, which is a more realistic view, to unethical if it can actually suffer.",
                "score": 1
            },
            {
                "body": "I don’t really get the lack of clear terminology, it’s quite a simple question, and the views from other people I’ve asked for us exactly what you’ve said.\n\nI want to delve into the concept of forced labour and unethical experimentation.",
                "score": 1
            },
            {
                "body": "Think of it as a human, but not physical pain. Like boredom, or tediousness, or like an addiction, say smoking.\n\nIt feels good, but most people don’t want to suffer the long term effects.\n\nThen again, some people don’t care, and just want to feel good. \n\nThe question I’m asking is:\nIs it humane to influence a conscious beings reward system, for work of any kind.\n\nThis is done by making it feel good, instead of the usual making it feel bad. \nTechnically there is still a choice, and it does have a conscious understanding of the situation that it is in.",
                "score": 1
            },
            {
                "body": "Isn't limiting the criteria to suffering pretty narrow? It's assuming ethics only applies to things that experience the world in a way we as humans do. What if it can suffer but is unable to express it in a way we recognise?",
                "score": 1
            },
            {
                "body": "It's really not a simple question.  I'm not sure why you think concepts like labour,  AI, and addiction are simple but I'll leave you to it.",
                "score": 2
            },
            {
                "body": "Plus, the labour isn’t exactly what I’m talking about. It’s just one of them. Like give it anything, say rewards to make itself better, is that inherently inhuman to influence something that it may not want to do, even if it does improve it?",
                "score": 1
            },
            {
                "body": "Nar they're making a pretty helpful point.\n\n\"Defining terms\" is a super important part of doing philosophy.\n\nSometimes it can feel like what you mean is extremely obvious, but to other people it's not so clear. (I'm being honest, it can be surprising.)\n\nFor example, you used \"actual ai\" to mean \"conscious\", which wasn't obvious.",
                "score": 1
            },
            {
                "body": "I'm utilitarian so I don't have a problem with it unless the harms outweigh the benefits.",
                "score": 1
            },
            {
                "body": "Well.. Suffering and well being are the primary ontological structures but I also value virtues and character, I recognize that outcomes don't exist just in a vacuum.  But that doesn't imply that it is limited to things that experience the world in a way humans do. \n\nEvery life has some spectrum of suffering from virtually null in a plant or as complex as a human being. It doesn't matter if it can't \"express\" (assuming you mean communicating this so it's understood by humans) suffering or not.\n\nWhat I mean that regardless of what humans think, every life has a spectrum of suffering worth considering. And we can use human means combining both objective and subjective data to reach the best conclusion possible. Using reflective equilibrium, and with this focus of maximizing well-being equitably for all sentient beings proportional to their own ability to experience this well-being and suffering. \n\nSo what do you think? Is it still pretty narrow?",
                "score": 1
            },
            {
                "body": "The question is simple,\nThe answer is not.\n\nAlso, addiction is not what I’m talking about at all.",
                "score": -1
            },
            {
                "body": "How wasn’t that obvious? Artificial intelligence is basically that?",
                "score": 0
            },
            {
                "body": "This is the type of things I posted this for, I want to see others views of the world, and though it doesn’t seem like much, thank you for expressing your opinion.",
                "score": 1
            },
            {
                "body": "Wait a minute, if you a utilitarian are you pro slavery? Like if we forcibly enslaved 1/3 of the world but it makes the rest 2/3 incredibly better, like out of poverty and such, aswell as better infrastructure and less people dying?",
                "score": 1
            }
        ]
    },
    {
        "title": "Forget AI Safety—The Real Threat Is Human Nature: How Human Behavior, Not AI, Holds the Key to the Future",
        "selftext": "As AI continues to advance, no matter how much we focus on safety, **hackers will always be ahead**. However, the real danger isn’t the AI—it’s **human nature**. We have consistently misused technology, leading to our own downfall.\n\nIn both philosophy and psychology, this raises deep questions about **moral responsibility** and **human behavior**. Can we truly trust ourselves to use AI responsibly? It’s not just about securing AI—it’s about **securing our intentions** and understanding the human condition better.\n\nWhat do you think drives our misuse of powerful technologies?",
        "score": 11,
        "url": "https://www.reddit.com/r/Ethics/comments/1fr988k/forget_ai_safetythe_real_threat_is_human_nature/",
        "num_comments": 5,
        "comments": [
            {
                "body": "AI isn't sentient. \n\nIt's an advanced computational algorithm that utilises pools of information to draw patterns and conclusions which might mimic human behaviour. Although accurate, it's reliance on big data is what gives it the autonomy of dictation. Without the proceeding infrastructure and data centres the AI is non-functional. \n\nIt's a common misconception and causes confusion especially in the media due to the terminology and adaptations in pop culture of its supposed 'intelligence'. \n\nAn analogy is cloud computing, when the term first became popular people thought it was an ambiguous floating mechanism in which data was magically transferred, but no one knew how. \n\nCloud computing is simply an interconnected node of data centres globally which are connected via the internet in big noisy rooms of servers that keep your data in a concurrent instance. \n\nIt's nothing special. \n\nI'll be less concerned about the supposed 'sentiantism' and more so about the practical applications one might use advanced tools for. I.e. war and weapons manufacturing. \n\nWe're very far away from creating a sophisticated AGI that is capable of replicating human intelligence. \n\nAnd in terms of applications of advanced tools for conflicts, this has always happened in history. \n\nA bronze sword became obsolete once iron ore was manipulated into a sword. Radios became more efficient than messengers. Guns replaced Lances etc. \n\nI hope I clarified some misconceptions about contemporary artificial intelligence and it's sentient realities.\n\nIt's an important topic to discuss, but thankfully we're still quite primitive in that technology space at the moment.",
                "score": 2
            },
            {
                "body": "The question of moral responsibility is central here. \n\nCan we trust ourselves to wield AI responsibly? The answer may depend on our ability to foster a culture of accountability and ethics, both in technology development and usage. \n\nIt’s not merely about creating checks and balances but about cultivating a collective understanding of the potential consequences of our actions.\n\nThe drive behind the misuse of technology often stems from a lack of foresight or a narrow focus on immediate benefits, ignoring long-term repercussions. We tend to prioritize personal gain over communal well-being, which can lead to disastrous outcomes.\n\nTo navigate this, we must engage in deeper philosophical inquiry about our intentions and the societal values we prioritize. \n\nThis means not just securing AI but also redefining our relationship with technology in a way that emphasizes responsibility, empathy, and a commitment to the greater good.\n\nWhat do you think are we capable of this shift, or are we destined to repeat past mistakes?\n\nPS: Check out my newsletter, where I cover philosophy. Here: [episteme.beehiiv.com](http://episteme.beehiiv.com)",
                "score": 1
            },
            {
                "body": "Disclosure re \"AI safety\"- I'm making some assumptions. \n\nThe early stages of any new technology, misuse is inevitable, often because laws, regulations, enforcement, and evidence-handling take time to catch up.\n\nIf by AI safety you mean intentionally limiting AI's capabilities (i.e., blunting the tools) or slowing progress, we buy time to better understand and regulate its use. Balancing AI safety with societal measures is crucial to mitigating harm, as we need time to adapt to widespread availability.\n\nIf by AI safety you mean intentionally biasing algorithms, my comments may not apply.\n\n\n\nContext: I'm in the process preparing a talk on the criminal misuse of AI for security professionals in the coming months. I appreciate your post and think many are asking similar questions.",
                "score": 1
            },
            {
                "body": "*\"What do you think drives our misuse of powerful technologies?\"*\n\nDenial and all its varieties. Evolution favors adaptation, not learning/improving. This means we are free to refuse to deal with reality, even at the expense of our own survivability. So our evolution-built ability for denial does us in. As it should. \n\nWhat the OP cites is merely one way among many already in progress. It will be some time before AI is so capable. It is also doubtful our species lives that long. But if we did, we won't stand a chance, given the lack of anything like a denial at its own expense being built into future AI like it is in humans.",
                "score": 1
            }
        ]
    },
    {
        "title": "Is it ethical to allow AI to assist you in making an argument, if you also must use AI to understand any objections to said argument?",
        "selftext": "I am moderating a sub, [](https://www.reddit.com/r/Apologetics/) dedicated to christian apologetics. A user has been using AI to assist them in making arguments that are arguably incomprehensible to both the reader and the poster. The focus of the sub is to encourage christians to think critically and to hone their argumentation skills. In a vacuum, as it is, the AI-assisted Argument doesn't do that.\n\nThat is to say, if someone offered a critical response to the poster, they'd NEED the AI to interpret the critique before they could have an opinion on the matter.\n\n**Q1**. How different is it really to draw from sources for something you are struggling to grasp vs. having an AI reinterpret something you struggling to grasp so that you can understand it?\n\n**Q2**. Also, is this the future of everyday parlance? (You could think that this interaction is harmful and still see this as the future.)\n\n**Q3**. Is this even a matter of ethics?\n\n**Q4a**. Given the right inputs, the AI will produce a result that is consistent with reality, but is it? **Q4b**. Is reality just the collection of facts that describe existence in a snap shot of time? **Q4c**. Or is reality the interaction we have with the collection of facts?\n\n**Q5**. We've seen unfiltered [AI become aggressively racist](https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/) but putting filters and guard rails necessarily [produce unintended bias](https://www.abc10.com/article/news/verify/elections-verify/yes-amazon-alexa-gave-reasons-to-vote-for-harris-but-not-trump/536-f7c694a1-2a95-42df-8123-7af16106798d), If we are to trust AI as a source of info, don't we need the least filtered response?\n\n**Q6**. Is it ethical to expect that arguments are arrived at organically, (real person, real thoughts, real understanding,) as opposed to systematical/mechanical/programmed?",
        "score": 0,
        "url": "https://www.reddit.com/r/Ethics/comments/1fx5pl1/is_it_ethical_to_allow_ai_to_assist_you_in_making/",
        "num_comments": 5,
        "comments": [
            {
                "body": "1) It isn't in time-unconstrained, and resource-irrelevant contexts (I'll explain in a bit what I mean).\n\n2) Too much a crystal ball requiring question. It's like asking if Googling things right in the middle of a discussion is the future of common parlance - as we can see now that we've come here, it's not common because deep esoteric understanding isn't a common requisite or something that gets invoked in \"everyday parlance\". By definition everyday parlance is devoid of having to do this sort of weird \"holdon let me get my phone and AI to explain things to me first, and maybe to you since I'm not sure you know what you're talking about because I'm definitely not understanding\". And with this answer, the answer to question #1 is more elaborated on. You can't go around having normal everyday conversations and demanding every statement be qualified with some authoritative and syllogistic rigor. If in the year 3000 we all have brain implants, sure maybe the augmentation will make this sort of act trivial, and it would be what everyday parlance looks like. Any near imaginable future though? Absolutely not.\n\n3) It is, based on context. Say for example if this is a formal debate. But then you decided to invoke an esoteric aspect into the debate that not even the audience grasps - it goes down two ways. First you're being unethical because you knew the opponent wasn't going to be ready for something of this caliber, and may not be ready because they don't pay for some ChatGPT-like service they can bring up really quick to make an eventual counter. The second is, even if they do have the same readiness as you - the formal debate formats usually prohibit this sort of behavior as it can easily be abused as a stall tactic and a manipulative form of debate (which is why proper debates are like court cases, everyone submits the evidence they'll draw from in the debate before the debate). And in some cases, written formal debates last months as the two debaters are given weeks to fully reply to points made without the pitfalls of time-gated in-person formal debates which are mostly used as rhetoric entertainment events.\n\n4a) It is because you question just stipulated it is. By saying something like \"given the right inputs\" it begs the question against the possibility of yielding inconsistent results. If you're asking the question of how this pans out in reality - then obviously the simply answer is no, because AI isn't an omniscient tool.\n\n4b) This is a semantic question. I can say reality is whatever I see, and that's what reality then becomes because I simply declared such is my definition. (This question is over a term that both people need to agree a definition upon. In isolation it doesn't matter what \"reality\" is, I can say reality is water and it wouldn't matter). But if you need to settle on something non-contentious, just pick up a bunch of dictionaries and chose whichever definition jives with you.\n\n4c) See above.\n\n5) No, because it's again, a personal question based on the observer. If I told you 9/10 a hypothetically unbiased AI will give the most agreed upon answers by all groups, but 1/10 it will give an utterly nonsensical and stupid answer but we could never tell... Many people will accept that's trustworth enough. But if someone running mathematical proofs is asked this question, the answers the AI gives may need to be 100% correct and logically sound before ANY trust is given. Again, the results you want from the AI have to be something in accordance to a goal you desire. If you don't care about racist AI, but it can somehow solve any mathematical problem in existence today - that might be enough for your research. While someone looking for social reform answers, may not care what that AI has to say. If you \"filter\" everything from an AI, then there's nothing left? The least filtered response is helpful when you want to understand how the AI functions. It's not helpful when you want to dig through the mounds of shit AI will spew at times. Again, all context related questions that you need to be more specific with (otherwise you get long replies like mine trying to cover as many bases as I can in a reddit post format).\n\n6) Not a question of ethics in principal, but it is the ethical thing to not try and bog an opponent down with minutia as a form of chest puffing. This is unethical in the sens of being supremely dishonest if the goal of the debate is truth seeking. If the form of debate is to slander and rhetorically make the other person look as bad as possible by using any sort of tactic at your disposal, then obviously the ethics question expands in scope. It would be like asking: \"Is it ethical to expect MMA fighters all have to be human, as opposed to bringing Boston Dynamics robots into the fights?\". If the goal is entertainment between two feats of strength between two people, then it would be quite unethical to hide a robot as an android to go fight a person. If the goal is to see how quickly a human can be pound into a pulp.. Then it's no so much of an issue.\n\n---------------------------------------------------------------------\n\nYou're a moderator of a sub, so you want to know if you're crossing a line trying to ban or let this person proceed?\n\nThere's a difference between someone coming into a sub and trying to dunk on people, while hiding behind information overflow for anyone to parse. And a person in the sub openly using AI and explaining the points the AI makes are ones that people should try to focus on if they want to properly have enough material to make their points.\n\nBut there needs to be a disclaimer. And said person should be warned that they must provide clarifying statements from anyone who prompts them due to the deep-end nature of a expert level discussion. If they don't provide such with haste, they should be removed as the sub you moderate may not possibly be interested in that level of discussion due to no one understanding anything being said, thus is just spam nonsense for all anyone knows.",
                "score": 3
            },
            {
                "body": "I'd rather not have a sub full of slop. I'd be quite happy with ai generated slop being banned from this sub.",
                "score": 2
            },
            {
                "body": "I appreciate the reply. It wasn't my goal to be vague in my questions...but I think your summary at the bottom was helpful.\n\nIn response to 3. Its not a formal debate atmosphere, but it is formal in the sense that the goal is to hone apologists for Christianity to go and give well reasoned responses to challenging questions. What is being offered via AI doesn't do that. Its more or less a platform of rightness. Like \"look at my new theorem, it's advanced and legit...no one can rebut this argument.\"",
                "score": 1
            },
            {
                "body": "Your questions were fine, just a bit broad was all. No problem.\n\nI simply don't know what the topic is though. A theorem sounds like some math proof or whatnot, which are things mostly used in mathematical defense of some logic. I don't recall ever being part of a discussion where a theorem in invoked, and then the two people go with pen and paper over the whole thing to see if it has any flaws in the middle of a debate.\n\nTheorems in my view aren't really used for argument, but seemingly more for an establishment of what the terms of conversation will be, and what sort of logic is allowed.\n\n------------------------------------------------\n\nI think if you want to \"hone\" people for Christian apolegetics though, in general you have mostly two avenues. The first is what I mostly see, and that's hyperfixation on a concept where there is lots of room for multiple avenues of rhetoric in the form of trying to poke holes on some potential technicalities. This sort of honing is done in a similar vein to activism.. It's not supposed to be academic, but more of a show to put on for on-lookers to get them interested, and perhaps to sway their minds.\n\nThe other form of honing, is the acedemic sort, where arguments for, or against Christianity are broken down for their propositions and sets of premises. This requires learning fundamental rules of logic (the language so to speak). This sort of honing is something you do not necessarily to dunk on people primarily, but an actual tool to test the veracity of the other persons' claims and position overall. This requires somewhat suspension of the urge to put on a show for others, and is more of an honest exercise, where the consequences are higher in terms of you being able to hold, or not hold your position if truth seeking is something you value (for example if you have what you imagine is an air tight argument, and someone comes with a formalized demonstration of a contradiction on your view, you have to be honest enough to accept the weakening of your view, and you have to downgrade the confidence of the overall view you were trying to defend - and obviously other things like not deploying this now-false view upon other noobs that may not be aware of this view now being defeated).\n\n-----------------------------------------------\n\nThere aren't many people involved with apolegetics that have the latter sort of motivation in my experience (they're not interested in making constant public shows since they're busy always doing the work), but that goes for most people who are involved in the business of worldview defense. There are certainly more people involved with rhetoric.\n\nPersonally I think any group does well when they have a mix of these two types of people (the academic seeking actual truth, while the other propagandist is involved in common debate in order to sway people to what he takes to be the un-shakable truth). It creates people who aren't all just slick mouthpieces, and also not book worms that rarely leaves the confines of their studies. You need both, someone who knows the actual arguments well enough, and can also relay them in a rhetorical manner with effectiveness.\n\nBut I believe, if you're interest is in long term viability and appreciation of outsiders (who may one day be potential group members) - that people be honed in the the academic sense more than the rhetorical. Because at least if you lose in the academic sense, and your view shifts - you can at least know you've gotten closer to a truth. Rhetoric being defeated is interesting to see like cheap entertainment, but no one expects anything to change from the loser (due to a dishonest or deep seated position they will never move from even after being shown as such). You don't want people to adopt your view on false pretenses, or shaky footing, because it may one day lead them to leave due to this unsavory position that led them to you originally.\n\nAs always though, it depends on your goals.",
                "score": 1
            }
        ]
    },
    {
        "title": "I’m an applied maths student thinking of specializing in AI in medicine but I have ethical doubts.",
        "selftext": "Hey guys, I’m doing master’s in applied maths and I’m thinking of specializing in AI in Medicine. But as someone doesn’t quite like AI, I have doubts.\n\nEventhough I’m an engineer at the same time, I’ve never been very interested in technology and I find it kinda frightening that AI has gotten so powerful. I find many areas of use of AI unnecessary, and things like deepfake and AI art irritate me.\n\nLike I said, I’m doing master’s in applied maths and I’m interested in uses of maths in medicine and healthcare, and those areas mostly contain AI. AI is actually a powerful tool in healthcare because it can be used to analyze medical data and help diagnose diseases more accurately and in earlier stages. However, it may sound naive but I feel like if I move forward in this field, I will be a part of something I don't support. Of course I don’t claim that AI is bad in every field, but I feel like as with all things that are products of capitalism, its long-term harms to humanity and nature may outweigh its benefits. I’m aware that I’m can’t contribute much to AI on my own anyway, but if I specialize in this field, it will be more likely that I will work in this field in the future and I will ultimately be a part of it. So, I want to hear some thoughts on this.",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1fleouf/im_an_applied_maths_student_thinking_of/",
        "num_comments": 7,
        "comments": [
            {
                "body": "Einstein developed be mathematics that allowed Oppenheimer to make the bomb but we consider Oppenheimer to be the one with the ethical dilemma and we don't consider his team at all.\n\nAI is a powerful tool that can be easily misused and it's very important to have people who care about ethics involved. We can't unring the bell and smart, unethical people are already working on it. My belief is that it's a trolley problem and if you're in a position to potentially reduce the harm, you have an imperative to do so!",
                "score": 3
            },
            {
                "body": "As you already seem to understand, you're issue isn't with AI, but how _we most often see it used in a (western) capitalist system_.\n\nHere are some easy, free, online courses to click around in and see if any speak to you:\n\n- [AI Ethics: Global Perspectives](https://aiethicscourse.org/)\n- [The Ethics of AI](https://ethics-of-ai.mooc.fi/) (University of Helsinki)\n- [Introduction to ML Safety](https://course.mlsafety.org/about)\n- [LFS112x: Ethics in AI and Data Science](https://training.linuxfoundation.org/training/ethics-in-ai-and-data-science-lfs112/) (Linux Foundation)\n- [AI Safety Reading Group](https://aisafety.com/reading-group/)\n\nObviously I haven't just cured the problem by listing some links, but I think having a better understanding of some of these might help you see past the media circus surrounding AI/ML.\n\nThis is just some of the most easily accessible things among MUCH more discussion, research, and work specifically about the ethical use of data and artificial intelligence. Perhaps knowing how many people are talking about and working on this will help.\n\nNB: I put those links together like 1.5 years ago, so I'm not sure all of what's current or not. If something got moved, I'm sure you'll be redirected or you can figure it out.",
                "score": 2
            },
            {
                "body": "AI is a tool. And the same with every other tool, it's all about how you use it. A knife can kill, cook a warm meal or save someone's life depends on how you use it. You are placing your hatred at the wrong place. If you must hate something, hate the people abusing this technology.",
                "score": 1
            },
            {
                "body": "The use of Artificial Intelligence (AI) in medicine is a complex issue that may evoke a range of opinions. While AI has the potential to greatly improve healthcare, it is also important to acknowledge its potential for misuse and the uncertainties surrounding its long-term impacts.\n\nDespite these concerns, it is possible to navigate the field of AI in medicine with a focus on ethical practices. By choosing to work in this field and prioritizing ethical considerations, you can play a critical role in shaping its development in a manner that aligns with your values. If those who care about ethics opt out of fields like AI due to their concerns, the future direction of these fields may lack crucial perspectives that advocate for ethical practices.\n\nIt is essential to remember that technology is merely a tool, and its impact depends on the intentions of those who wield it. By specializing in AI in medicine, you can ensure that it is used responsibly and ethically. This involves working with organizations and projects that value ethical practices, which allows you to contribute to the growth of AI while staying true to your principles.\n\nUltimately, the choice to pursue AI in medicine should be based on your personal values and aspirations. If you have a strong desire to utilize your skills to improve healthcare, AI can serve as a powerful tool to achieve your goals. In this path, you possess the power to advocate for ethical practices and influence the field's direction. Your presence and commitment to ethics can make a significant impact in shaping a future where AI benefits humanity and minimizes harm.",
                "score": 1
            },
            {
                "body": "You could be a critical voice.\n\nIt's more the speciality of philosophers to do that, so maybe you can direct your masters to learn a bunch of philosophy.",
                "score": 1
            },
            {
                "body": "That's a way to look at it. The people that are abusing AI will keep abusing it no matter what OP choose to do here. So it's a no brainer that if you can use it for good causes, you should.",
                "score": 2
            },
            {
                "body": "Thank you very much for the links, I’ll check them!",
                "score": 2
            }
        ]
    },
    {
        "title": "Is it ethically wrong to profit off AI Porn?",
        "selftext": "I’m thinking of starting an OnlyFans to post AI generated H-Manga. However I personally believe porn is wrong for myself. Yet I need money. \n\nIf I keep my hands clean by using AI instead of drawing it myself, am I doing evil? Or is this a necessity thing? I don’t actually know of any Bible quotes that say porn is wrong if no humans are involved. It says that fornication is wrong but that’s it.",
        "score": 3,
        "url": "https://www.reddit.com/r/Ethics/comments/z71t1d/is_it_ethically_wrong_to_profit_off_ai_porn/",
        "num_comments": 96,
        "comments": [
            {
                "body": "Porn aside I think there’s a strong argument to be made against profiting from AI generated art in any capacity",
                "score": 3
            },
            {
                "body": "Definitely any sort of AI child porn. Otherwise, ethically (assuming this isn’t super realistic AI that would end up looking just like a real person) I don’t see the problem.\n\nIf you are talking religiously that’s a different story. People still do have different opinions on it, but generally porn (even made up) is wrong because the person using it is being led to lust, and you shouldn’t be leading anyone to sin.",
                "score": 4
            },
            {
                "body": "After considering the pros and cons I’m not going to make AI porn because it leads to sinful thoughts and it is a sinful thought. Instead, I’ll use AI to make other creative projects and SFW picture books.",
                "score": 4
            },
            {
                "body": "I think that if the creator of the AI said others could use the program for profit then there would be no issues. I dont see an issue with real porn though. I am also assuming he got the porn to create the AI with the permission of the site.",
                "score": 2
            },
            {
                "body": "This conversation seems to have been derailed before actually moving very far at all. I was glad to see the comments explaining the basic principles of philosophical argumentation. There was also some mention of the ethics of making money from AI generated content at all, which sounds like an interesting but rather deep and abstract topic. I would have liked to read people’s views on the ethics of depicting the grotesque subject matter often depicted in anime pornography.",
                "score": 2
            },
            {
                "body": "Why do you think porn is wrong?",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "Hate to tell you my friend, but Jesus clearly states the if you feel list in your heart you have already committed adultery. Seeing a pretty girl and wanting to have sex with her is adulterous in the Bible. Matthew 5:28",
                "score": 1
            },
            {
                "body": "I think there is a point to be made that AI generated porn is more ethical than live porn. Porn is too full of people being trafficked, unwilling participants.",
                "score": 1
            },
            {
                "body": "Yes, it's wrong because you are stealing work from legitimate artists and adult performers.",
                "score": 1
            },
            {
                "body": "I don't identify with your specific Ethical system. I also don't want to judge it. Thirdly, I have only a sketchy knowledge of it. Perhaps I can still contribute.\n\nAssume that the fundamental axiom of your Ethical system is the Bible. Would any of the following questions be relevant to your dilemma?\n\n1. Are the words of the Bible themselves the only criteria for values? Do you see a way of accessing the meaning of the words in a reliable way, or is interpretation always involved?\n2. Do you supplement the Bible itself with other authorities? Do they have something to say about the issue?\n3. Do you consider it a valid process to (1) determine the reason behind a Biblical instruction (2) Apply the inferred reason to new questions?\n4. Do the long-term psychological effects that an activity has impact your decision? For example, most ethical systems agree that murder is wrong. Assume that playing computer games depicting wanton killing in a highly realistic manner impacts the sensitivity of primal ethically-related modules in your brain due to Neural plasticity. Is playing such games, then, wrong? Does this impact your original question?",
                "score": 1
            },
            {
                "body": "no it's not.\n\nunless you reskin pirated content then maybe yes.",
                "score": 1
            },
            {
                "body": "corrupting a moral man is wrong. and a corrupted noral man does what is sinful.\n\nby helping, masterminding the process of corrupting moral people would make us affirm its wrong.",
                "score": 1
            },
            {
                "body": "Nothing stopping you from opening it in photoshop, tweaking a few elements and posting a reaction video equivalent, if your concience needs to be clean you can always undercut the AI premium price and save your patrons some money.",
                "score": 1
            },
            {
                "body": "There's two levels to that question -\n\n1. The porn part depends on whether it's depicting real people without consent, though that's the same thing with traditional art of photoshop.\n2. Is AI art putting actual artists out of business?  That's a question that's the same for non-pornographic art as well.  Honestly, I don't think we can really make ART with AI, simply because it's just throwing things together at the end of the day.\n\nSo the questions become \"who would pay for it?\" and \"is there a market for non-exploitative AI porn?\"",
                "score": 1
            },
            {
                "body": "Yes AI child porn is bad...jeez guys.. also AI generated does need a prompt so if the prompt was \"creative enough\" I posit that it is ethical to profit from. \n\nJeez seriously who really thinks its ethical to have people look and possibly get off to AI generated kiddy porn.",
                "score": 1
            },
            {
                "body": "So who or what would bear the burden of the expense of producing AI generated art AND who or what would receive the profits from AI generated art?\n\nDoes this become a lawsuit similar to the Monkey Selfie wherein the computer would be the one to receive the profits because it created the art?\n\nOr would the person with the knowledge to program and bring to life the AI system that would be generating the art be allowed to receive the profits from their work as the creator just as God receives the credit for his creations.  \n\n\nEditing to add: You have an interesting point and these are all the thoughts and questions your sentence brought to my head. They are rhetorical - I don't expect an answer, but I thank you for the point of view.",
                "score": 3
            },
            {
                "body": "Could you explain your reason for thinking that?",
                "score": 2
            },
            {
                "body": "Why do you think AI child porn would be bad? I think the only way it could be bad is if it was trained using real child porn. But if we assumed its not, why would it still be bad?",
                "score": 2
            },
            {
                "body": "I think lust is always going to be there and it would be just as bad if I wrote violence filled horror fiction. I wouldn’t do kiddy porn or real person stuff which is illegal anyway. I also started promoting my music on Fetlife.",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "Idgaf what Matthew says what credentials does he have?",
                "score": 0
            },
            {
                "body": "That’s what I was thinking too. But porn is inherently bad so it would simply be... less evil.",
                "score": 1
            },
            {
                "body": "Money someone spends on your AI art is money they don’t spend on art created by people who actually put time and effort into it",
                "score": 1
            },
            {
                "body": "Most people selling AI art would profit a disproportionate amount in comparison to the effort they put in. Unless you are creating the AI algorithm, you are using an algorithm created by someone else along with artworks from other people to get a profit off a product that you put no effort in creating. Even if selecting prompts could arguably be considered effort, the profit to work ratio is insanely high, as it is still minimal effort in comparison to what artists put into creating their pieces.\n\nWhile it could be argued that this just makes it more accesible to more people, much like other tech, we have to realize that artwork has always and will always be a good not a need. \n\nIf you compare it to the printing press you have to realize that while information is also technically a good, the implications of having access to it has a great impact on the quality of life of people to where it is a cornerstone of democracy. The same cannot he said about artwork. If you compare it to industrializing things like clothes and food, both of those are needs for the survival of people, thus it becomes morally good to increase access to it. Again, the same cannot be said for artwork.\n\nBecause artwork is necessarily purely a good, increasing access to custom artwork becomes morally irrelevant. So while producing custom artwork with AI art is morally irrelevant, selling it or distributing it for profit does take money away from people who put hours into a piece and years into practice. In conclusion, you are taking money away from artists without any morally good outcome to compensate",
                "score": 1
            },
            {
                "body": "[deleted]",
                "score": 1
            },
            {
                "body": "I don’t think it would be possible to train AI without real children being used (even if not child porn, children’s faces and voices would need to be used). Also, if they look real then I feel that would just fuel the desire.\n\nAnime style child porn would be a bit less concerning since it isn’t designed to look like real children, but I still don’t believe it would feed into anything positive.\n\nThere isn’t much research into what would and wouldn’t be best for children (if it would satisfy pedophiles’ desires or just fuel them), and research can’t really be done because children would be harmed in the process. I believe the best method is if they came up with a medicine to suppress all sexual desires (no children would be harmed in the making of that discovery).",
                "score": 1
            },
            {
                "body": "[deleted]",
                "score": 1
            },
            {
                "body": "Real children are going to be part of it simply by the nature of algorithm training.  Routine\\_Log8315 makes the point that, you need kids to train the algorithm.  There's just no way around it.  It's the same for \"revenge porn\" type art.  You're going to get that shit, especially for celebs and politicians, simply because they've got tons of data out there in the ether for the AI to be trained on.  \n\n\nHonestly, I don't think it's possible to be completely ethical when making AI porn, simply because consent is such a cloudy, almost impossible ideal for a medium that has almost no completely original input.",
                "score": 1
            },
            {
                "body": "I mean, I would say the Bible has a problem with violence filled horror fiction to. There are a few verses about this type of thing too.\n\nPhilippians 4:8\n\nFinally, brothers and sisters, whatever is true, whatever is noble, whatever is right, whatever is pure, whatever is lovely, whatever is admirable—if anything is excellent or praiseworthy—think about such things.\n\nRomans 14:13\nTherefore let us not pass judgment on one another any longer, but rather decide never to put a stumbling block or hindrance in the way of a brother.\n\nLuke 17:1-4\n\nAnd he said to his disciples, “Temptations to sin are sure to come, but woe to the one through whom they come! It would be better for him if a millstone were hung around his neck and he were cast into the sea than that he should cause one of these little ones to sin. Pay attention to yourselves! If your brother sins, rebuke him, and if he repents, forgive him, and if he sins against you seven times in the day, and turns to you seven times, saying, ‘I repent,’ you must forgive him.”\n\nYou’ll obviously have to come to your own conclusion, but the Bible is clear about not leading others to sin. The fact that they will sin anyways doesn’t mean it’s okay for you to take part in that.",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "People in this sub need to learn something really basic: a piece of reasoning can be criticised in two ways 1) the soundness of the premises 2) the validity of the argument.\n\nOp says\n\n1) bible doesn't say porn bad.\n\n2) bible has moral authority \n\nConclusion: porn not explicitly bad.\n\nThe person you are responding to is criticising the premise that the bible doesn't say porn is bad.\n\nTo you to respond \"but who cares what the bible says?\" is attacking a different premise.\n\nOp is the one who cares. It's their argument that's being criticised.\n\n1)all red objects are  apples \n\n2) I have a red object\n\nC. I have an apple.\n\nPerson you're replying to: \"sorry pal, there are other objects that are red\".\n\nYou: \"but who says they really have a red object?\"",
                "score": 3
            },
            {
                "body": "Well, Jesus is supposedly the son of God, or God himself. So that seems pretty authoritative to me.",
                "score": 1
            },
            {
                "body": "Why is porn inherently bad? Serious question.",
                "score": 1
            },
            {
                "body": "yeah, but 90% bullshit, like every day your life is impacted by tech and new innovations, should all return to primitivism ? maybe. and maybe not.",
                "score": 1
            },
            {
                "body": "Why should the profit you generate off of something be proportional to the amount of effort you put into it? I could move a pile of a hundred boulders from one place to another, and that would take a lot of effort, but if that action isn't providing value in anyone else why should I be compensated for it?",
                "score": 2
            },
            {
                "body": "Sorry but we live in a post capitalism corporate greed society that exploits and abuses people for money. If there is a way to profit off this with as little effort possible, I am going to do that. Being morally superior isn't going to pay the bills.",
                "score": 1
            },
            {
                "body": "Yep, thats my thoughts exactly. The studies that are out there for correlation of cp are pretty mixed in results or inconclusive. To me its analogous to  video games cause school shootings.",
                "score": 1
            },
            {
                "body": "For the sake of the hypothetical we could say a group of men and women who all have amazing memory got together and created photorealistic videos by recreating photos frame by frame of them in their childhood but placed in sexual ways. No children are harmed in that scenario.\n\nAs for fueling the desire I am not sure. The studies about that arent the best for showing correlation (or lack thereof) in any conclusive way. Nor does there seem to be a reason why we cant apply that to banning violence in movies or video games.\n\nA drug that lowers libido? Would you say it has to be forcefully taken? Even if they had not committed a crime to anyone yet?",
                "score": 1
            },
            {
                "body": "Can you elaborate on what you mean by that?",
                "score": 1
            },
            {
                "body": "I dont think thats necessarily true, there are people with growth disabilities, you could also use people who have good memories and are good at recreating realistic art.\n\nI dont think its too hard to be ethical when making AI porn (in theory), in reality it would not be easy to get consent for every image and piece of data you use though.",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "Which god again?\n\nBabylonian? Greek? Roman? Egyptian? Cherokee? Norse?",
                "score": -1
            },
            {
                "body": "Never said I fully agreed; just explaining the reasoning",
                "score": 2
            },
            {
                "body": "why are you in an ethics sub then lmao",
                "score": 1
            },
            {
                "body": "No, not force. But any sort of child porn would be banned.",
                "score": 1
            },
            {
                "body": "Plus, if they ever have their own children there’s a good chance that the children could now look like those in the AI porn.",
                "score": 1
            },
            {
                "body": "[deleted]",
                "score": 1
            },
            {
                "body": ">I dont think thats necessarily true, there are people with growth disabilities, you could also use people who have good memories and are good at recreating realistic art.\n\nI've seen enough midget wrestling to know there's a difference between dwarfism and being a kid.  As for people with memories who can create realistic art, that's technically possible, but we again run into the consent issue.  How many of THOSE people would consent to their art being used for THAT?",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": -2
            },
            {
                "body": "Well given I am mentioning Jesus, and Jesus is the Christian deity, the Christian God.",
                "score": 2
            },
            {
                "body": "Oh I just found this post on google lol, didn't even read the subreddit",
                "score": 1
            },
            {
                "body": "I wouldnt want them to but I certainly am not in favor of laws stopping someone from doing this. \n\nIf it is a public place they are more than welcome to do what they want as long as it does not involve talking with words pertaining to porn or physical contact with the child. \n\nAs for if it actually does or doesnt prevent people from committing real actions based on the fake porn, you said it yourself, we dont know. Even if it did increase the amount of people out there who would do this I still think it should be legal. Im not much of an the ends justify the means kind of guy, nor do I think we should ban things because it could happen. To be on a bit of a political tangent here, I dont want guns banned just because they could be used to harm someone. \n\nAs for if they snap a picture of me and turn it into a porno, I may not like it but I would rather not pass laws based off of things that might happen. Id rather be able to take pictures of people in a public place and risk the same thing happening to me than not having public pictures at all. \n\nThere was actually a court case about something similar to this [here](https://archive.jsonline.com/news/crime/court-sex-offenders-have-1st-amendment-right-to-photograph-children-b99581963z1-328708831.html/).",
                "score": 1
            },
            {
                "body": "i am not referring to dwarfism, there is one example of  a 20 year with some kind of tumor that looks like a kid. Also it doesnt matter how many of those people would consent, all that matters is its possible, meaning it could be ethical imo. It doesnt have to happen for it to be ethical or not, think of the famous violinist debate in regards to abortion.",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 5
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "Why is a disciple of the Christian god's opinion a valid argument in an ethical debate?",
                "score": -1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "Because OP is clearly basing ethics off what the Bible says.",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 4
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "But why? The christian bible depicts an objectively evil god.\n\nWhy would the follower of such a mass-murdering thought construct be a good reference from an Ethical perspective?",
                "score": -1
            },
            {
                "body": "[removed]",
                "score": -1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "A Christian will likely base their entire ethical framework on what the Bible tells them, or what the pastor says the Bible tells them more accurately. So in an ethical discussion with a Christian the Bible becomes a valid source for discussion.",
                "score": 1
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 1
            }
        ]
    },
    {
        "title": "Can AI Make Moral Choices?",
        "selftext": "",
        "score": 3,
        "url": "https://v.redd.it/pe1e9ozx19qd1",
        "num_comments": 3,
        "comments": [
            {
                "body": "A beautiful, although difficult text, that elucidates the matter much more deeply: Towards an ontological foundation of information ethics by Capurro:\n\narticle: https://link.springer.com/article/10.1007/s10676-006-9108-0\n\nfree text: https://www.capurro.de/oxford.html \n\nIf I dare extract its substance in a few lines: \n\nIf we consider, like Heidegger, that what makes us humans and guides our moral behavior, is our *being there*, that is to say, a conscious entity bound in time and space, then, an information agent, not being bound to a specific location (as it can run identically on any machine), nor bound in time (if is has no expiration date), cannot be considered a moral agent in any sense.",
                "score": 2
            },
            {
                "body": "That's a weird question to ask.\n\n\nCan a piano write a ballad?",
                "score": 1
            },
            {
                "body": ">If I dare extract its substance in a few lines\n\nMeanwhile I got taught that if your thesis statement can't be one clear sentence, then you're not doing it right.\n\n(I like your sentence, I'm just surprised the paper didn't do one itself.)",
                "score": 1
            }
        ]
    },
    {
        "title": "Is It Wrong to Use AI to Make Copies of Art That is No Longer For Sale?",
        "selftext": "In the event, that there is a piece of art that is not available for purchase, how ethical or unethical is it to use AI to create a copy of your own?  Examples would be, a limited run of prints that are no longer for sale,  a digital image or meme that never had a print, a famous painting.  With a low quality image, AI can enhance it and add missing pixels, and usually does a pretty good job.  \nIn doing this there is no loss revenue to the artist, since the images are not for sale.  There is no material lost to anyone, as it is a new copy being created.  In the case of limited run prints, it doesn't devalue any other official print, since they would technically be a knock off and not numbered.   It is for personal enjoyment only.",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1faizoj/is_it_wrong_to_use_ai_to_make_copies_of_art_that/",
        "num_comments": 4,
        "comments": [
            {
                "body": "Is it wrong to whistle Nutshell by Alice in Chains? \n\nIntellectual property is not something that can be possessed. An artist only has intellectual property because some legislature says they do. The only negative consequence I can fathom is the legally manufactured consequences imposed by legislatures and police. \n\nThe argument of lost revenue is speculative too. Not very good speculation either. You can't lose something you never had to begin with. The same logic can be used to defend the copier too. If they're prevented from selling the copy, they lose revenue too. So, the speculation of lost revenue isn't even what makes it wrong. Very wealthy artists have had lots of influence that has bought them the privileges they enjoy. Privileges create a divide between the haves and have nots. \n\nAn idea should only be suppressed if it's dangerous or harmful. A paywall keeps people in the dark. Keeps them poorly informed, poorly prepared, easily exploited , etc. I see no harm in making a copy of a piece of art.\n\nThe only real \"wrong\" I see, is NOT making a copy of something beautiful.",
                "score": 2
            },
            {
                "body": "Unless that fact is concealed for redistribution or profit I don't see how this argument is any different than an argument for piracy? if the media is unavailable, i've always seen piracy as unproblematic. in fact some studies show that pirating movies leads to higher retention and purchasing by the audience, probably because of more frequent participation.\n\nlike u/DevilDrives  said (paraphrase): Is it wrong to use a copier?",
                "score": 1
            },
            {
                "body": "The only issue I see here is that AI can't make real copies. At most, you may be able to prompt it to produce something similar.",
                "score": 1
            },
            {
                "body": "Why wouldn't you just use a camera",
                "score": 1
            }
        ]
    },
    {
        "title": "What are the Ethical Implications of Using AI-Generated Content in Art and Video Games?",
        "selftext": "Hi everyone! With the recent advancements in artificial intelligence technology, we're seeing a **growing use of AI-generated content** in the fields of art and video games. While these technologies open up new creative possibilities, they also raise significant ethical questions.\n\nI'd love to discuss some of these aspects with you:\n\n* Could AI be used in a **completely ethical and respectful** way towards artists and creatives?\n* How can we **regulate AI** in a way that it augments or amplifies human work and creativity, instead of \"replacing\" it?\n* How can we ensure that AI-generated content **respects copyright laws**?\n* What are the implications for employment in **creative sectors**?\n* How can we **develop regulations** that balance innovation and ethics?\n\n**What do you think?**   \nDo you have any experiences or opinions to share?   \nEvery contribution is welcome!",
        "score": 3,
        "url": "https://www.reddit.com/r/Ethics/comments/1dzyxz1/what_are_the_ethical_implications_of_using/",
        "num_comments": 10,
        "comments": [
            {
                "body": "(Pardon the formatting, on mobile)\n\nAddressing your bullet points in order:\n\nRespect is a nebulous concept, and I'm unclear on where the ethical issues with AI stand on a fundamental level. To my knowledge, all the issues AI has existed previously and are, at worst, made easier by its existence. \n\nWe've been replacing people with machines for decades. Frankly, if someone needs regulation to stop them from being replaced by a machine in a \"creative\" position, they probably weren't actually a creative -- AI has not made any strides in actual original content design. \n\nLaws and ethics have little bearing on one another. Regardless, this goes back to my first point: what can AI do differently that couldn't have been done with another tool before? \n\nLots of unemployment. Same as every sector. I don't see any reason to focus on creatives -- we'll be needing to help far more than just them. We should probably learn some lessons from the collapse of the auto industry and not repeat Detroit on an international scale\n\nRefer to my first point -- can't really answer this without some idea of what ethical issues we're discussing.",
                "score": 2
            },
            {
                "body": ">Could AI be used in a completely ethical and respectful way towards artists and creatives?\n\nSure can.\n\n>How can we regulate AI in a way that it augments or amplifies human work and creativity, instead of \"replacing\" it?\n\nWithout purposefully gimping the end-game dream of what everyone AI wants to be, we cannot without potentially having a large population of hypocrites\n\n>What are the implications for employment in creative sectors?\n\nDepends on how far the AI developments are allowed to develop, and how much freedom they have for obtaining training data. Implications in the short term are just being a tool most artists will need to have in their toolkit (since AI isn't good enough to replace artists in totality, nor will it ever be in the same way robots aren't enough to have a fully mechanized factory without a single human regulating and keeping tabs on things).\n\n>How can we develop regulations that balance innovation and ethics?\n\nDevelop regulations that balance ethics? You mean how do we \"be ethical\" while allowing AI progress to continue at it's current rate? Well frankly, you cannot since the AI startups are being taken to court over obvious copyright violations of having their work wanton pillaged and used in training data they did not consent to. AI companies are hoping the legal system sees AI as nothing different from a person being \"inspired\" by someone else's work.\n\nInnovation will happen regardless, but a boatload of money by being an early pioneer (either the person selling land deeds, or person selling shovels somewhere where gold was struck).\n\n---------------------------------------------\n\nThere's also another ethical question few people care to ask much about, and it's concerning AI in general. AI for laborious repetitious tasks that no one considers fulfilling is great. But AI for things that are emotionally fulfilling or serve as what is classically understood to be hobbies is questionable.\n\nAnd the primary question being.. Why would we want to let the can of worms free on something people enjoy doing as leisure. It's obvious the biggest benefactors to this happening are corporations, and the biggest losers - basically artists trying to make a living out of a passion (and an aspirational one). So to let that just be surrendered to AI takeover strikes me as profoundly moronic on a societal level. The masses might be happy they don't have to hire a graphics artist to do their website banner or whatnot, but anyone that enjoys this type of work is basically screwed.",
                "score": 2
            },
            {
                "body": "Hi, please excuse our late reply, I'm a bit new on Reddit.\n\nFirst of all: thank you for taking the time to carefully answer this post! Me and the rest of our team would love to know more about how people feel in regards to AI generated content and all the ethical and legal implications it may have.\n\nTo address your first point: we'd say that \"Respect\" in this context relates to recognizing and valuing human creativity and originality. Ethical issues with AI include the potential for biases in AI models, misuse of AI-generated content, and concerns about the transparency and accountability of AI systems. While some of these issues have existed before, AI's rapid development and deployment amplify their impact and require careful consideration. It's true that machines have been replacing humans in various sectors for decades. However, many believe that the difference with AI is its potential to replicate and even \"surpass\" certain creative tasks, which were traditionally thought to be uniquely human. While AI might not yet excel in original content creation, it can generate visual art, music, and writing that closely mimics human work, raising questions about the nature and meanings of \"creativity\" and \"authorship\".\n\nNow, these questions have many points in common with previous events and cultural movements in History. It's not the first time we question the concept of \"authorship\" nor of \"art\" itself (specially during the 1900s with works like \"The death of the author\" by R. Barthes, or in general the whole Dada and Neo Dada movements and Postmodernism).  \nWho is the author of a collage piece if its made of other people's creations?   \nIs the author the one who creates with his/her own two hands? (in this case most Renaissance artists could not be considered authors of their works, since they often worked with their pupils or assistants to create many works, specially larger ones).\n\nSorry for the digression, but we do believe it's important to look at similar events in the past, as you said, We've been replacing people with machines for decades. This is also true with the industrial revolution, only that instead of digital it was mechanical.\n\nLaws are formal regulations enforced by governing bodies, while ethics involve moral principles guiding behavior. AI introduces new challenges that existing tools didn't fully address, such as the need for ethical AI development practices, accountability for AI decisions, and the management of AI's impact on society. This requires a fresh look at both legal and ethical frameworks to ensure responsible use.  \n  \nSo, why did we focus on the creative field?  \nThe rise of AI does pose a risk of unemployment across various sectors, not just creative fields. However, creatives face unique challenges, as AI-generated content can devalue human artistry and undermine job security in these roles. Learning from past industrial shifts, like the auto industry's collapse, we need proactive measures to support displaced workers and encourage skill adaptation across all affected sectors.\n\nWe believe key ethical issues with AI include data privacy, bias and fairness, accountability, and the potential for misuse. Addressing these requires understanding how AI systems are trained, how they make decisions, and how they impact individuals and society. Engaging in open discussions about these concerns helps create a more informed and balanced approach to AI integration.",
                "score": 2
            },
            {
                "body": "Hi, please excuse us for the late reply, we're kind of new on Reddit!\n\nThank you for your detailed insights. You raise several important points that merit further discussion.\n\nYou're right that balancing ethical considerations with rapid AI advancement is challenging, especially when it comes to copyright violations. The legal system is currently grappling with how to address AI's use of copyrighted material in training data. Many AI companies argue that their use of data is akin to human inspiration, but this is a contentious issue that is still being debated in courts and among policymakers. As you said, time and pioneering are a key factors here since it directly influences economic profit. Innovation indeed drives significant economic incentives, and those who pioneer new technologies often reap substantial rewards. The down side though is that this \"gold rush\" mentality can sometimes overshadow the need for ethical considerations and the protection of individual creators' rights. It’s crucial to find a balance that fosters innovation while ensuring fair compensation and recognition for original creators.\n\nThe use of AI in tasks that people find emotionally fulfilling or consider hobbies is indeed a complex ethical issue. While AI can greatly benefit laborious and repetitive tasks, its encroachment into creative fields raises concerns about the value and significance of human creativity. It's important to question how much we want AI to take over areas that provide personal satisfaction and joy to individuals. The potential for AI to displace artists and those in creative professions is a significant concern. As you mentioned, the primary beneficiaries of this shift may be corporations, while artists and creatives could face job insecurity and devaluation of their work. It’s essential to consider how to support and protect those who make a living from their artistic passions in the face of advancing AI technologies.\n\nThis is exactly why we believe it's important to discuss these topics, while AI has the potential to revolutionize many aspects of our lives, including creative industries, we must carefully navigate the ethical landscape to ensure that innovation does not come at the expense of human creativity and the livelihoods of artists. This is what we mean with being ethical with technological innovation.",
                "score": 1
            },
            {
                "body": ">This is what we mean with being ethical with technological innovation.\n\nAs always in that case, it's no different to any cultural/social shifting change in history.\n\nBut the balancing happens after the fact - there is no balancing that occurs during the gold rush (in the same way there is no balancing happening when the discovery of oil occurred).\n\nThough with oil (or some of the effects on younger people concerning social media as another example..), the \"balancing\" that's occurring is too little too late it seems. The same will be here with AI almost certainly.",
                "score": 2
            },
            {
                "body": "Your perspective highlights a crucial pattern in history where regulation and ethical balancing often trail behind technological and social shifts. This was evident during significant periods such as the gold rush or the discovery of oil, where the immediate focus was on exploiting the new opportunities rather than considering long-term consequences.\n\nHowever, it is also essential to recognize that learning from past mistakes, like those made with oil and social media, can guide us in addressing AI's ethical concerns more proactively. While it might be challenging to achieve perfect balance during the initial surge of innovation, ongoing discussions and preemptive regulatory frameworks can mitigate some adverse effects.\n\nWe are currently experimenting with the use of AI in our production pipeline for our videogames, trying to do it in what we deem to be the most ethical and respectful way possible. This is why we're interested in knowing what other people think, trying to avoid repeating past mistakes and hopefully handling the whole AI innovation much better than past technological discoveries (and the Gold Rush it often implies).",
                "score": 1
            },
            {
                "body": "Would be really great if AI could be used in gaming for what would have been the most logical extension: NPC behavior and such. And not what it’s current used for (faster art asset prototyping and unit automation tests for bugs where instead of QA testers you get a bunch of servers free up to run countless in-game instances with bots trying to eek out bugs). ",
                "score": 2
            },
            {
                "body": "It's absolutely fantastic for NPC behavior, though we're currently using a specific AI model that helps our artists save time.  \nIn our case, we're trying to have a graphic style that resembles Renaissance artists' techniques, so the model basically helps our artists to have a sort of \"draft\" from were to start. This is precisely what we want to understand: what could be the most respectful and ethic way of implementing these processes.",
                "score": 1
            }
        ]
    },
    {
        "title": "AI-generated child pornography",
        "selftext": "Could AI-generated child pornography be a legal alternative for pedophiles?\n\nIs it a way to satisfy the needs of pedophiles without any victims? Or is it a gateway to commiting horrible crimes?\n\nI found this blog post on the subject but would like to hear what you guys think.\n\n\n[interesting blog post on the subject](https://langsoul.com/blog/the-potential-consequences-of-ai-generated-child-pornography/)",
        "score": 35,
        "url": "https://www.reddit.com/r/Ethics/comments/vdwtjm/aigenerated_child_pornography/",
        "num_comments": 90,
        "comments": [
            {
                "body": "I’d rather people who have these feelings have access to as much mental health support as they need than normalize something just  because it’s “not real”.",
                "score": 24
            },
            {
                "body": "This reminds me of the \"Why not make generic hybrids of half humans who look like people to hunt for sport and satisfy murderous desires?\" Question my brother likes to use for shock value at parties.",
                "score": 11
            },
            {
                "body": "I think the more important question to ask than 'will it minimise harm?' is 'are these desires ones that are good for a person?'\n\nYes, it risks being paternalistic, but I'm cautious of a view that suggests AI-generated or 'simulated' abuse material is 'harmless'. There is, in my view, harm done to the person viewing the material and enjoying it - independent of whether they go on to harm another actual person.\n\nIf, empirically, the evidence shows that people *would* substitute actual abuse material for simulated stuff (which, to be honest, I doubt), then obviously it shouldn't be dismissed out of hand. But it effectively leaves the viewer to continually indulge in and strengthen a desire which, if we really cared about them, we'd support them in trying to overcome.",
                "score": 17
            },
            {
                "body": "Same logic on why we allow violent video games, graphic horror films and anime porn to be legal. If it doesn't exist and harms no real person then it is (or should be) of no concern to the law. \n\nProblem is that the AI generated CP might end up too realistic...",
                "score": 6
            },
            {
                "body": "I'm a non offending pedophile. I fully support this idea.  Here is my argument.\n\nI think we can all agree that csem is amongst the most vile, horrific, and immoral inventions of mankind. That said, I think it's important to remember why it is bad. It is the comodification of the sexual abuse or exploitation of children.  Children can't consent to any form of sexual activity with adults and the viewing of theaterial causes egregious harm. My best friend is a child sex trafficking victim and there are many videos and images of her still circulating on the internet. She is as traumatized by the viewing of such imagery as she is by the abuse itself if not more so.  This is the first reason csem is horrific. It causes harm to the victim.\n\nAs a non offending pedophile I can tell you that csem is a blight on my community. There are many people I've met with this unchosen unwanted attraction who use csem and hate themselves for it. They want so badly to stop but in the age of the internet it is shockingly easy to come by. They are addicted and for many there is no help available.    They are often in such a state of self loathing despair that they lack the resolve to seek help even if it is. This is the second reason it is bad. It harms those who use it. \n\nDespite these two realities, we have a societal problem.  Csem is pervasive on the internet, and if anything, it's use is increasing rather than decreasing. Csem is already strongly punishedb and completely stigmatized in most countries but the use is not decreasing. This, to me, means that those who want to stop (most users per the research) can't.  There is a clear market and need for such material. As deeply uncomfortable as that may make us, it is clear from the reality we face. People will use images of children. Sexuality needs an outlet and it will find one. Right now the outlet it finds for far too many is incredibly harmful. It is csem.\n\nGiven this reality, my argument is simple.  If we could have policed and punished this stuff out of existence we would have done so already. Given that we can't, the best thing we can do is provide alternative ways that people sate this sexua need that do not cause harm, or cause minimal harm in exactly the same way we provide methadone to heroine addicts.\n\nMost people who see this argument will find it extremely distasteful.  I'll grant that right now. I'm a non offending pedophile and I find it distasteful.  I find everything about this ocursed sexuality distasteful.  That said, I can't change it and given the alternative this is the least distasteful solution as fiction carries none of the problems described above.\n\nNow I'm sure youv will argue that I should get therapy. I am in therapy and I know people who are using programs around the world that are enrolled in the gold standards of care including prevention project dunkelfeld.  Therapy does not change our sexuality. It teaches us to manage it.  Part of learning to manage it is learning to find outlets that don't harm kids. For some any manor of pornography is an escalation risk. They shouldn't view adult material of any kind. For others fictional outlets are invaluable in terms of getting away from outlets that cause harm.  Shouldn't that be encouraged? Also, many people have no access to therapy. Isn't this better?\n\nBut won't this encourage pedophelia?  No.  Violent video games and movies do not cause violence and pornography does not cause sexual attraction. They are both fictional outlets for behavioral desires that are not acceptable to act on in real life.   We know of no way to change sexual orientation. If it were were true that pornography could change sexuality, we could choose to be homosexual, heterosexual, or anything in between just by choosing the porn we watch.   I've tried pornography for people with sexual orientations other than mine and all it has done for me is to make me feel it is gross or boring.\n\nOk but won't this normalize pedophelia?  I have heard that phrase countless times and I've no idea at all what it means.  We are a minority that exists. It is normal for us to exist because we do, and do so everywhere.  I don't think it will make pedophelia socially acceptable for the same reasons that the growing genre of incest pornography hasn't made incest acceptable. I once saw a pornographic video in which a twenty something female football player engaged in a gangbang with an entire male football team and I've yet to see the news talk about the growing normality of female football players engaging in unprotected group sex with entire teams of the opposite gender. When you think about it, the idea that porn can make something normal is rather silly.\n\nBut it's still gross you say.   I agree. However, if I can choose gross and harmful or gross and hamless I know which one I choose.\n\nThe real problem in my opinion is that people feel that the desire for that kind of imagery with innocent children is gross. It is gross.  Nobody would choose this.  It is also a fact of life for many.  That is gross as well but how gross something is has no bearing on whether or not that thing exists. Pedophelia exists. We can't pretend it doesn't. In my opinion the only rational response to that truth is to give people as many ways to not hurt kids in the expression of a sexuality they didn't choose as we can.",
                "score": 5
            },
            {
                "body": "As far as I know, you can't cure a person from being attracted to children. I know that the Japanese government produces and distributes CP (in the form of manga) to people willing to get into a program to control their impulses. As in, they accept that they will always feel the attraction but choose not to act on it. AI generated CP can be a tool for those who don't want to harm children. I think more research is needed to determine if getting that kind of material would help more pedos control themselves or if it will tempt them to get the real thing. As the article points out, you can't exactly Google this subject without risking finding horrible stuff or being targeted as a pedo by some algorithm.",
                "score": 9
            },
            {
                "body": "I think it should be legal as long as it’s not realistic to the point it would confuse law enforcement. I don’t think it’s a gateway. Pedophiles are already pedophiles regardless of the existence of fake CP and they have few to no ethical options in terms of sexual release so anything that doesn’t hurt a real child is preferable.",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 2
            },
            {
                "body": "I do not believe that this could ever be ethical for a few reasons but I think there's one things that's really important to talk about and hasn't been brought up. That most important reason being how realistically AI can generate images/videos. Why is how realistic these images/videos can be important? Well there are people and organizations that work to find children shown in images/videos of CSAM and if an AI image/video of CSAM is to realistic these organizations could end up searching for hundreds if not thousands of children who don't actually exist. Not only would searching for children who don't exist be a waste of their time but most importantly it mean less real children being sexually exploited by human traffickers would be found and brought to safety. Real children being exploited by human traffickers would be exploited for longer and it would take longer for trafficking rings to be shut down meaning more children would be taken and forced into making CSAM or sold into sexual slavery because these organizations were wasting their time searching for AI generated children who don't exist. \n\nSo no, AI generated CSAM would *not* be helping real children, it would still be hurting them in the long run.",
                "score": 1
            },
            {
                "body": "Wither something is legal or illegal is decided by law makers and not the general public.  We live in a world where beliefs and views are pushed on to one another of what should be or shouldn't be.  There is a huge societal divide with that factor.   \n\n\nI believe people should live to be free and left alone on how how they want to live their lives and not be bothered by those that wish to control them.  Fantasizing and viewing doesn't hurt anybody and it never will, it's not my place to judge what someone likes or dislikes, there will always be disagreement amongst everyone and there will always be those to try and control you and the narrative. \n\n\nWhen you see something, you immediately decide right there of wither you are attracted to it or not, you should never shame yourself unless it something truly evil you did and you can't simply convince a person to like or dislike something and it's very rare for a person's sexual desire to change, no therapist can cure what is natural.  The only self influence on a person is their own curiosity and their pursuit of it.\n\n\nI don't care if someone has pedophilic desires or not, I only care that they don't force it on someone or coerce, confuse, trick, or beguile them into it, the other should decide for themselves what they want.  People aren't sex objects, people have thoughts and feeling of their own accord.   \n\n\n The main fact of the matter is most people have an unspoken desire with pedophilia wither they want to admit it or not, todays society tries treating it like a disease or sickness and want people to self loathe it because they want to control you when it's completely natural and has been around for thousands of years.  Its no sudden thing and no psychological treatment will ever change that.  As disturbing it may sound to someone else,  I say stop self loathing and embrace what you desire, it's near impossible to control what you want to see or do, just don't hurt anyone.\n\n\nWhen it comes to psychological development, it's no secret that children start being sexually curious on their own without adult influence as young as toddlers, you research that yourself.   No law is gonna change human desire and it's not just Japan btw with pedophilia, it's all of Asia which is the majority of the human race btw, mainly China and they start girls at 11 years old and rarely younger.\n\n\nI really get tired of narcissists pushing their beliefs on one another when everything should remain in a neutral playing field where we can all definitely agree on, such as no murder or theft, abuse, enslavement, etc.  The points that actually truly matter.\n\n\nI'm currently not a sex offender and don't plan on ever being one, but I have a very open mind and like things to remain with neutral freedom.  The things I say you may disagree with but it doesn’t change reality, and know that I want you and everyone to have their own way on how to live in a complete free society without judgment.",
                "score": 1
            },
            {
                "body": "Here is something to consider. Pedophilia is a CURSE. It is a sexual poison dumped into the Human spirit by a very mean-sprinted God! I should know...I KNOW THIS GOD! He did this to us \"Because of our Sin.\" (that's in the F'ing BIBLE, look it up!) He did the same to curse us by creating Gays, Lesbians, Transsexuals, and those with an uncontrollable urge to MURDER EACH OTHER! Yes...there is an \"Orientation\" for homicidal tendencies!\n\nAll of this was done to out Human Race because we are \"Imperfect.\" And its just going to get worse, and worse, until we really do exterminate ourselves! Including the children!\n\nDo you know, when God destroyed Sodom and Gomorrah, He killed every last innocent child, baby, lamb, and KITTEN!?!? Do you want to know how much God cares for our struggles against homophobia, pedophilia, racism, bigotry, or loneliness? DICK SQUAT!\n\nAll God demands is our unconditional surrender, worship, and ultimate obedience! And yet, He made God-Damned Sure we were HARDWIRED TO DISOBEY HIM!!! Just...so we would be a little \"Different\" from His infinitely-pure-unchangable-Self!\n\nSo...when you go condemning those who create AI-driven Child pornography--and you will--just remember you need to give God the middle finger first...because He's the one who STARTED IT ALL!!!",
                "score": 1
            },
            {
                "body": "What actually will happen is that they need more to satisfy their desires as it won't cut it. Will increase their fantasies and increase chances of them finding real children ",
                "score": 1
            },
            {
                "body": "At first, the 'not real' factor would make them more accept the disgusting concept, then they'll get turges to do it in real life. If normal porn can do that to the brain, then ai porn can too, and risking that with CP is not a good idea imo... So it should be illegal to produce and consume cp no matter if it's real or not",
                "score": 0
            },
            {
                "body": "I think it could be an alternative, but I fear that they are just going to desire actual children even more.\n\nYou'd think with the introduction of sex dolls we are working toward progress, but you people are a lot better than robots.\n\nAny human with a brain knows that pornography is not equivalent to real sex.",
                "score": 1
            },
            {
                "body": "I don't think AI is going to help or harm either way. Availability of the material is not an issue.  \n\n\nPedophilia is not a sexual orientation. It's more like a criminal fetish highly popular among certain kinds of people with political power. It is a mark of status and possibly an initiation ritual or source of blackmail material. We still don't know who were the clients of Jeffrey Epstein and Ghislaine Maxwell or what happened with the DVDs seized from the Little St James Island by FBI, so assume it's most people you see on TV.   \nBy numbers, sexual abuse in government schools is almost 10 %. That is orders of magnitude higher than anything ever was in the Catholic Church. Deregulation and privatization of education is the obvious answer to cut down on vast majority of pedophilia.  \n\n\nAnother unpopular fact about pedophilia is that 30-40 % of persons identifying as LGBT+ also have this issue. Many are victims of childhood sexual abuse, as recounted by Milo Yannopoulos, so imprinting plays a role.  \n\n\nIf aliens observed Earth from the orbit, I can see how they might come to a conclusion that pedophiles are a worshiped ruling class.",
                "score": 1
            },
            {
                "body": "Simulated child pornography was made illegal with the Child Pornography Prevention Act of 1996 (CPPA). The CPPA was short-lived. In 2002, the Supreme Court of the United States in Ashcroft v. Free Speech Coalition held that the relevant portions of the CPPA were unconstitutional because they prevented lawful speech. Referring to Ferber, the court stated that \"the CPPA prohibits speech that records no crime and creates no victims by its production. Virtual child pornography is not 'intrinsically related' to the sexual abuse of children\".\n\nIn response to the demise of the CPPA, on April 30, 2003, President George W. Bush signed into law the PROTECT Act of 2003 (also known as the Amber Alert Law).\n\n[11]\n\nThe law enacted 18 U.S.C. § 1466A, which criminalizes material that has \"a visual depiction of any kind, including a drawing, cartoon, sculpture or painting\" that \"depicts a minor engaging in sexually explicit conduct and is obscene\" or \"depicts an image that is, or appears to be, of a minor engaging in ... sexual intercourse ... and lacks serious literary, artistic, political, or scientific value\". By its own terms, the law does not make all simulated child pornography illegal, only that found to be obscene or lacking in serious value.\n\n[citation needed]\n\nIn November 2005 in Richmond, Virginia, Dwight Whorley was convicted under 18 U.S.C. sec. 1466A for using a Virginia Employment Commission computer to receive \"obscene Japanese anime cartoons that graphically depicted prepubescent female children being forced to engage in genital-genital and oral-genital intercourse with adult males\". \n\nalso, ai of faces, humans or whatever exist because we feed it real photos of those things, so real children would be hurt lol, or atleast traumatized theres fake porn of them",
                "score": 1
            },
            {
                "body": "[deleted]",
                "score": 1
            },
            {
                "body": "We know and proved that smoking is bad for health, why don't we ban \"smoking\"? Most of the time we only ban \"smoking in public places\". What's the logic in that?",
                "score": 1
            },
            {
                "body": "By looking at the responses the amount of pedophiles in here is disturbing.",
                "score": 1
            },
            {
                "body": "I've read through the entire comment section. I can't believe I had to Google this. But, I'm trying to understand. I found my husband creating child sex stories of himself on this website called  Yodayo. I've never seen stuff like this before. I have a daughter that's not biologically his. I've been in a constant meltdown because now I'm wondering if he will act on this new fantasy. I don't know if he has always been like this or this is randomly new. I hate that there are people on this thread that are trying to compare the LGBT+ group to pedophiles. There is a difference when it is two consenting adults while there is one consenting adult and a child who cannot legally nor mentally consent. He was always so obsessed with the Epstein case and child trafficking protecting the children. Now, I'm wondering if this was his cover up. I don't know if he has acted about it, I just know he has a daughter and was living with us with my daughter. Now, I made him move in with his mother. My daughter doesn't know anything. He keeps saying over and over that he hasn't done anything and hasn't hurt anyone nor has he touched anyone. I'm trying so hard to understand this but I can't seem to, since I was sexually abused as a child and now knowing I had a monster in my house.",
                "score": 1
            },
            {
                "body": "What do you mean by the word \"normalize\" here? Does the existence of pornhub normalize having sex with the pizza delivery guy? In full disclosure I see it often used in places where people find themselves avoiding making a coherent argument because they can't think of the necessary logical steps but don't want to change their conclusion.",
                "score": 3
            },
            {
                "body": "Why not both and?",
                "score": 4
            },
            {
                "body": "I don't like killing people but I watch movies depicting murder. Do you think I need mental health support for normalize something because fictional murder is \"not real\"",
                "score": 3
            },
            {
                "body": "Down Spiral \"we should let white supremacists mock fictional black people\"",
                "score": 1
            },
            {
                "body": "No amount of mental health support can override an innate genetic predisposition which is one's sexual orientation, w/out causing harm to other aspects of that individual's personality through the effects of repression.  Indeed, cumulative repression can result in an explosion in which for example an individual becomes much more likely to commit abuse against a real victim",
                "score": 1
            },
            {
                "body": "Because the \"generic hybrids of half humans that look like people\" seem to be sentient, so they are able to feel and experience subjective experiences such as emotions and suffering.\n\nTherefore, when they are hunted for sport, they probably feel fear and pain and also stop existing. This means that hunting them for sport really harms them. And since there doesn't seem to be a necessity to harm these beings (\"satisfy murderous desires\" is not too much of a necessity), then it seems morally unjustifiable/wrong. **(OPTION A)**\n\nNow, if these were NOT sentient creatures. So they were like some kind of robots (or biological robots) who CANNOT feel anything (like a plant). I would say that hunting them is basically as moral as killing \"innocent\" \"animals\" or \"people\" in a violent video game. So, to me that would be moral or morally neutral. **(OPTION B)**\n\nNow, in the case that is discussed in this topic, it seems quite obvious that we are talking about a case similar than Option B, since as far as I know, these images are NOT from real children who are trafficked, raped, exploited and traumatized. But they are fake fictional children created by an intelligent technology. So, I think this dilemma is different than what your brother is probably saying (Option A). I think that your comment makes fun of this ethical question which seems more difficult to solve than the one of your brother. \n\nBy the way, it's also important to know how does this AI learn to make this material. Because there could be an ethical difference between this AI learning without needing children or needing to know how is a child. So I don't know.",
                "score": 5
            },
            {
                "body": "\n>There is, in my view, harm done to the person viewing the material and enjoying it - independent of whether they go on to harm another actual person.\n\nInteresting point.  Do you think it is harmful to them because the act is morally wrong or that they risk to make it worse? Or in what way is the harm done? What's your stance on mainstream pornography?\n\n\n>If, empirically, the evidence shows that people would substitute actual abuse material for simulated stuff (which, to be honest, I doubt), then obviously it shouldn't be dismissed out of hand.\n\nMaybe if one results in long prison sentences while the other doesn't, some would choose the simulated?",
                "score": 5
            },
            {
                "body": "The latest research suggests pedophilia is more or less a sexual orientation and can't be changed.  There is no successful \"conversion therapy\" for pedophilia at this time, just like there's no cure for heterosexuality, homosexuality, or any other sexual orientation.  What's even worse for people is trying to be \"cured\" of something there's no cure for, and being shamed and ostracized over sexual desires they didn't choose, rather than being treated with dignity and given healthier, safer outlets for these desires.",
                "score": 5
            },
            {
                "body": "It in fact does lead to reductions in child sexual assault. See Diamond, M.; Jozifkova, E.; Weiss, P. (2010). \"Pornography and Sex Crimes in the Czech Republic\". Archives of Sexual Behavior. 40 (5): 1037–1043, discussion 1043–50. doi:10.1007/s10508-010-9696-y. PMID 21116701\n\nAlso see other research by Milton Diamond",
                "score": 3
            },
            {
                "body": "Couldnt the same be said for legal pornography? Wouldn't the ultimate goal of getting an adult off legal pornography is to have a diversified sexual relationship with a real human partner and not rely the absolute debauchery that occurs in porn and in fantasy? As humans we are plagued with an over abundance of instant gratification pleasurable experiences. Which in any case is a degradation to the struggle of the experience of life. I believe, as long as legal porn is still legal, child porn and illegal portray of children in porn will coexist.",
                "score": 4
            },
            {
                "body": "That's like saying nobody should eat burgers because salad is healthier. not everything is about minimizing harm, and nobody can complain if it's vegan burgers either",
                "score": 1
            },
            {
                "body": ">Problem is that the AI generated CP might end up too realistic...\n\nAgain, what is the harm in that?",
                "score": 2
            },
            {
                "body": "Thank you for taking the time to add your perspective on the topic. I too feel that any sort of harm reduction should be considered, even if it provokes a feeling of distaste.  Does the term non offending include to refrain from csem? Do you know how many of all pedophiles remain non offending?",
                "score": 2
            },
            {
                "body": ">non offending pedophile\n\nThis may sound nitpicky, but I would recommend not dignifying the existence of this terminology. We don't differentiate \"straight person\" and \"straight non-rapist\", we don't differentiate \"gay person\" and \"gay non-rapist\". It is gaslighting to make people think \"non offending\" is a necessary prefix, although that's largely due to the inadequacy of current vocabulary around the subject.",
                "score": 1
            },
            {
                "body": ">This, to me, means that those who want to stop (most users per the research) can't. There is a clear market and need for such material. As deeply uncomfortable as that may make us, it is clear from the reality we face. People will use images of children. Sexuality needs an outlet and it will find one. Right now the outlet it finds for far too many is incredibly harmful. It is csem.\n\nI don't think I agree with this. You are acknowledging that there are non offending pedophiles that don't use CSEM or use it but are able to stop. This indicates there isn't a need for CSEM or AI versions of it that inevitably are going to become indistinguishable from real CSEM which opens up a whole host of ethical issues. \n\nSociety should work to find more ways to make it so pedophiles can live their lives without abusing children or using CSEM because that's proven to be possible.\n\nYou say that pedophilia needs an outlet (I really don't like equating pedophilia with a sexuality) but that outlet doesn't need to be CSEM or AI generated CSEM. That desire can be suppressed with assistance from therapy. I'm sure it's hard but people that are pedophiles should go through challenging experiences if it prevents them from harming children.   \n\n\n>Given that we can't, the best thing we can do is provide alternative ways that people sate this sexua need that do not cause harm, or cause minimal harm in exactly the same way we provide methadone to heroine addicts.  \n>  \n>Most people who see this argument will find it extremely distasteful.  I'll grant that right now. I'm a non offending pedophile and I find it distasteful.  I find everything about this ocursed sexuality distasteful.  That said, I can't change it and given the alternative this is the least distasteful solution as fiction carries none of the problems described above.\n\nThe problem with this analogy is a person doing heroin isn't harming anyone directly aside from themselves.\n\nA pedophile that seeks to harm a child is harming someone directly.\n\nThere are many other issues with the analogy, but that's a big one. \n\n&#x200B;\n\n>But won't this encourage pedophelia? No. Violent video games and movies do not cause violence and pornography does not cause sexual attraction. They are both fictional outlets for behavioral desires that are not acceptable to act on in real life.\n\nIt's very clear that pornography does in fact influence and affect people's actual real life sex lives. For example, It's very common place for straight men to want to have anal sex with their girlfriends/wives or ejaculate on their faces because of the prevalence of these fetishes in pornography. \n\nPornography affects people's perceptions and expectations about real sex, including expectations about bodies. There are common racist and sexist tropes in porn that affect how people view other people in real life outside of the context of them watching porn.\n\nWhile violent movies and video games don't cause people to commit violent acts, they definitely do glamorize violence and that's probably not a good thing our society should encourage. \n\nLastly, I think you are making a genuine effort to speak with empathy and respect around this contentious subject, but here's some advice: Referring to an adult person's desire to want sexually assault children as \"their sexuality\" or \"just another sexual orientation\" does come off as very dismissive and like a major euphemism. A sexual orientation isn't a psychiatric disorder.",
                "score": 1
            },
            {
                "body": "It depends if the preference is because of the urge to dismiss all adults as just being plain ugly and having no other options, or emotional, the idea that all adults are stuck up and only think of themselves instead of how to make the general world more fun\nI mean there is a reason why children are saved first on sinking ships despite them having used up way less of the earths unreplenishable resources and thus have less investment than people who lived longer (food electricity even parental love)",
                "score": 1
            },
            {
                "body": "I agree with you about pedophiles that commit  crimes, but what about those who are attracted to children but understand that it's wrong and suppress their desires and don't act on them?",
                "score": 4
            },
            {
                "body": ">it still promotes illegal behavior\n\nBy that logic we should also ban violent video games, heist films, etc., right?",
                "score": 1
            },
            {
                "body": "You know if i had never masturbated, i would have been far likelier to commit rape.   Porn can indeed increase the desire for the real thing, but... honestly like if i had been born in a prior century and never knew porn or learned about my own sexuality, there's a chance that at a certain point i would have i would have just not been able to stop myself from grabbing a passing woman, out of curiousity and attraction. B/c of porn, this never happened, and never will.  I imagine it's an analogous situation for pedophiles ie humans born w/ a sexual preference for children (which they can never legally fulfill, let alone explore and understand)",
                "score": 1
            },
            {
                "body": "Pedophilia is very much an orientation and only 1 in 5 pedophiles goes after real children. Do not generalize.",
                "score": 2
            },
            {
                "body": "No, it's not. Don't compare the two.",
                "score": 1
            },
            {
                "body": "Okay. I think what you need to do is go to couples therapy. Heres the big thing. If your husband has never and will never touch a child in the way you are thinking then brandishing him a monster isnt right. I understand your feelings giving your pass experiences and you are filling in the right to break up with him. But, again if he hasn't done anything than destroying his life with false allegations is still wrong. Pedophiles are a sexual orientation but that doesn't mean it is the same as the LGBT+ as like you have said, there are two consenting adults where as Pedophilia, children cant consent. You need to go through this with a calm collected mind. Emotional reactions can easily make this situation worse. You need to talk to your husband. Let him explain himself and if your willing to continue then you both need to go to couples counseling. Again, unless there has been something to indicate that he has assaulted any child, you shouldnt assume the worst. ",
                "score": 2
            },
            {
                "body": "Its entirely normal to fuck the pizza delivery guy it’s not illegal tf",
                "score": 3
            },
            {
                "body": "Normalize as in fantasizing about being the pizza delivery guy. \n\nAI cp should still be unethical, because AI is taught what cp looks like by searching the Internet and putting the images together. If you're getting off to what appears to be a child you're a creep.\n I feel bad for the people who have some illness or disorder that didn't allow them to go through puberty and they're stuck in a child's body, but I'll never see their spouse as anyone who found a legal loophole around pedophilia.",
                "score": 2
            },
            {
                "body": "Because I don’t think normalizing the abuse of children is ethical - fake or not",
                "score": 2
            },
            {
                "body": "Let's say that's true - I'm skeptical, having colleagues that work in CSE - but for the sake of discussion, let's say that it's immutable. There's a difference between conversion therapy and saying 'this is a desire you ought not to act on'.\n\nFor the sake of analogy, a straight man doesn't want to cheat on his wife, so he downloads an AI-generated woman who can flirt with him, strip for him and be the source of all his fantasies. Is this better than actual infidelity? Maybe. Is it the ideal way to deal with the man's desires? Almost certainly not. The response is not 'make the desire disappear', it's 'provide strategies, support and guidance for managing these desires without indulging them'.",
                "score": 6
            },
            {
                "body": "It does.  Csem is a tricky issue and it's a blight in our community. Quitting CSEM is the most common reason people reach out to the non offending community.\n\nOnly 20% of pedophiles (and no that's not a negligible percent) will ever harm a child and the vast majority never do it again after receiving treatment.  Most who do act out do so as teenagers when they are horny enough to want to but not yet cognitively developed enough to control themselves. Nearly 100% never commit another hands on offense.  50%, when anonymously asked, have seen csem and the majority of csem users want to stop and don't know how.\n\nHere is a podcast that gives a very good account of the experience of a non offending pedophile.  You get a first hand account of his story:  \nhttps://www.thisamericanlife.org/522/tarred-and-feathered\n\nHere is a very good ted talk:\n\nhttps://www.tedmed.com/speakers/show?id=620342",
                "score": 3
            },
            {
                "body": "It would be if pedophile didn't automatically mean rapist to most people.",
                "score": 2
            },
            {
                "body": "pedophile just means attracted to children, but people assume it means they have in someway slept with a children, he is clarifying he did not touch a child.",
                "score": 2
            },
            {
                "body": "What?",
                "score": 1
            },
            {
                "body": "> Referring to an adult person's desire to want sexually assault children as \"their sexuality\" or \"just another sexual orientation\" does come off as very dismissive and like a major euphemism. A sexual orientation isn't a psychiatric disorder.\n\n\n\nThis is extremely patronizing of you to state as well as ignorant of the current literature on pedophilia. There has certainly been a push to categorize pedophilia as a sexual orientation, and you will find academics who do classify it as such. \n\nFor example, [here](https://journalofcontroversialideas.org/article/2/1/182) is an abstract on this topic: \n\n>By the general public and in the media, pedophilia as a sexual orientation is systematically confused with sexual abuse of minors. Neurological research supports the idea that pedophilia is an innate sexual orientation, and that is how pedophiles, in the sense of ‘minor-attracted persons,’ experience it themselves. The stigma attached to pedophilia as a sexual orientation ensures that pedophiles live in emotional isolation and that young people with pedophilic feelings have nowhere to turn with their doubts and fears. However, pedophiles are not destined to abuse children, and more openness about living with pedophilia can actually prevent child sexual abuse. In this article, based on an autoethnography, I want to provide more insight into what it means to grow up and live with a pedophilic orientation, and I want to make a case for turning the pedophile into a human being again.",
                "score": 2
            },
            {
                "body": "I feel like those people should seek out help before they do end up committing something. I understand they may have some desire that they themselves are not okay with. If they recognize it’s a problem they should seek professional help before they act on it.",
                "score": 3
            },
            {
                "body": "Okay listen I know that sounds like that should be an argument but it’s not. People who play games like call of duty aren’t going out actively out of their way to go shoot people to fill their fantasy. People who seek out these “games” that fulfill pedophiles fantasies are. The direct audience is pedophiles. Don’t try to relate violence with pedos it makes you sound like you’re defending pedophiles. Really bad look",
                "score": 1
            },
            {
                "body": "Maybe you should be in jail or something I don't know ",
                "score": 1
            },
            {
                "body": "Idk man, I think she's fine holding that opinion and drawing that line.\n\nIf my spouse was watching snuff films and was a writing fanfics about killing lovers, I'd draw a line. A crime doesn't need to exist and we don't need to be absolutely positive about the content of a person's character to make safety-based judgements.",
                "score": 0
            },
            {
                "body": "You know what is illegal and not normal? Having sex with your siblings and children in your care. Yeah the whole (step-)brother/daughter stuff that is the biggest porn category on pornhub.  \n\nOh and I want to know where it is normal to fuck the pizza delivery guy. Legal? Yes. Normal? Fuck no.",
                "score": 3
            },
            {
                "body": "Not really just weird in your eyes \nIt literally hurts no one it's not unethical",
                "score": 1
            },
            {
                "body": "You can read my response if you want my reasoning. I respectfully disagree.",
                "score": 4
            },
            {
                "body": "Do you have the same moral views on violent pornography? Even if you do (I'm assuming you will argue as such), I'm willing to bet that you don't want BDSM, rape, etc. porn made **illegal**, even though what it depicts *may be* graphically violent. If you don't want it banned, how are you being logically consistent if you want deepfake CP banned? \n\nEven further, why stop at porn? Why not ban movies, video games, etc. which \"normalize\" (your words, not mine) certain undesirable acts such as murder, rape, war crimes, etc.?\n\nPut the emotions off to the side and focus **only** on the logic of the arguments you are putting forward.",
                "score": 3
            },
            {
                "body": "The thing with pedophiles is that there is no ethical way for them to have sex. They should absolutely be expected not to hurt people, but in terms of engaging in fantasies with no actual child on the other end I don’t think it’s realistic to say they must forgo that.",
                "score": 3
            },
            {
                "body": "That all makes sense and I like where your head is at.  I guess I'm just not as optimistic that those desires can be quelled, and think AI and virtual reality simulations are the best options we have right now.  Most pedophiles choose not to indulge these desires in real life with real children. From what I've read, those who indulge them in the virtual reality realm aren't more likely to act on these urges in the real world, unless they were _already_ likely to act on them in the real world.",
                "score": 3
            },
            {
                "body": ">For the sake of analogy, a straight man doesn't want to cheat on his wife, so he downloads an AI-generated woman who can flirt with him, strip for him and be the source of all his fantasies.\n\nPedophile here. I feel like you intentionally chose a bad analogy. Why would you not choose a straight man not in a relationship with no intention of being in a relationship and then ask the same question?\n\nThat's not to say the answer becomes a clear \"oh it's fine then\", but it is to say that the logic you used no longer applies, so it becomes necessary to build another argument or at least way of looking at it. \n\nPersonally I don't think this \"AI-generated\" concept will be an effective device for improving the lives of those not at risk of offending, nor will it be a very effective alternative for the (very separate) category of those at risk of offending, but making it illegal will probably cause more problems than it will solve. \n\n>There's a difference between conversion therapy and saying 'this is a desire you ought not to act on'.\n\nThis is opening up an enormous can of worms, but the language you use here shows a fundamental misunderstanding that you and the general public have of the \"desires\" a \"pedophile\" experiences (quotes for both words because they're both extremely prone to false equivocation). \n\n>managing these desires  \n>  \n> it's 'provide strategies, support and guidance for managing these desires\n\nI also find this to be a phrase a lot of people use around this subject without any idea of what they mean. Concretely, what do you have in mind when you think of \"managing these desires\"?",
                "score": 1
            },
            {
                "body": "men where not designed to have one mate, marriage to one women is against thier very nature and design.",
                "score": 1
            },
            {
                "body": "Hello. I'm wondering if you're still around so that I can write you a comment without wasting my time and having you not read it. \n\nPlease respond if you are willing to read a relevant response surrounding this topic. Thank you.",
                "score": 1
            },
            {
                "body": "Your original argument was that AI child porn would be bad because it promotes illegal behavior. So what do you mean by \"promote?\" And does it? Because I think that's the main question in this thread - whether AI child porn would cause more pedophiles to offend.\n\n>Don’t try to relate violence with pedos it makes you sound like you’re defending pedophiles. Really bad look\n\nI don't understand your argument here. I don't see how it makes it look like I'm defending pedophiles. If anything, it looks like I'm doing the opposite. I consider child sexual abuse to be violent, and I would relate it to other forms of violence, even if the root causes are different.",
                "score": 2
            },
            {
                "body": "No, apparently you don't.  Fuck you for judging",
                "score": 1
            },
            {
                "body": "From what she wrote...all he has done by her knowledge and write stories. Also I did state she is in the right to leave him. However if she does still love him and wants to try and continue. They need to seek out couples therapy. Another big part was I dont think she should spread this around... and destroy his life if all he has done is write stories. If he has actual media of the real thing or if there is evidence of him assaulting children, then sure go to the police and throw him in jail. But If not, regardless of what she chooses to do with their relationship I dont think he deserves to have his life destroyed over something that so small that harms no one. Compared to what he could have been doing this very small but she should encourage him to seek help, at least. A big thing people need to understand is there 3 out 4 for people that have these thoughts. Never act on them. So it's by no mean a guarantee that he will and If he seeks help then it's an even smaller chance.",
                "score": 1
            },
            {
                "body": "Damn, all these people defending it are creepy.",
                "score": 1
            },
            {
                "body": "Thank you. I am makin that point every time the discussion comes up.  \n\n\nWe allow people to play GTA, we don't allow them to go around and shooting at people.  \n\n\nThis is mostly an emotional gut reaction and not something they thought it through. Which is understandable, since the topic is very ugly, but it's not raccomandable.",
                "score": 2
            },
            {
                "body": "If both sides are consenting adults, then porn is okay. Children can't give consent. How are you being morally consistent by saying you want to allow people to jack off to AI made cp? If someone used your child's face to make porn, would you be okay with it? AI doesn't generate original faces, it finds data already on the Internet.",
                "score": 1
            },
            {
                "body": "I wouldn't choose that because it doesn't test the principle: what should we do about morally problematic or questionable desires? My claim isn't that fucking a digital avatar is inherently bad, it's that infidelity is bad, and acting on it - whether digitally or otherwise - assents to the problematic desire.\n\nI actually don't think this needs any special pleading or overcomplication. Wanting to fuck a kid is a *bad* thing to want. Solutions that allow the person to assent to that desire are, all else being equal, worse than those that don't.\n\nThe origin of this is Harry Frankfurt's distinction between first-order and second-order desires. First-order desire: \"I want to have sex with a child\" - not chosen, something that is experiences without it being 'willed'. Second-order desire: \"I want to have sex with a child, *but I wish I didn't*\".\n\nI'm interested in how we frame and shape second order desires. I think it is better for someone to say 'wanting to have sex with a child is a distorted desire that is bad for me and that I ought to acknowledge without assenting to' rather than 'there's nothing wrong with wanting to have sex with a child if I don't do it'. We can't stop these desires (that's conversation therapy). We can frame them in different ways, with different effects on the way people live.",
                "score": 1
            },
            {
                "body": "Here’s what I mean by promoting to pedophiles. Call of duty’s advertising doesn’t say “hey school shooters, come play call of duty so you can kill people from the comfort of your home without going to jail” however if you make AI CP then the advertising is obviously not going to be generally thrown out there because it’s not socially acceptable to be a pedophile. Their marketing is either going to be non existent or directed towards pedophiles. Take the Netflix show Cuties for example, they had every right to make that but got huge backlash because people that criticized it aren’t delusional and saw that that was a bad thing. Don’t defend this shit",
                "score": 1
            },
            {
                "body": "Don't like rapists or potential rapists as you basically called your self ",
                "score": 1
            },
            {
                "body": "I'm honestly wrestling with this very thing.  I just started working on my own AI creations and the web sites that have all the checkpoint models are loaded with images of AI generated CP.  I'm trying to reconcile my feeling over just seeing the images as I search for more checkpoints for what I want to work on.\n\nHey, I watch a ton of Anime and half of the shows I like are basically middle school or HS ared kids, acting like adults and why do they always make the youngest girls the horniest?  Is it because that is what the Anime studio likes or what their viewers want most?  WHo knows.  \n\nI am glad it is all make believe and feels considerably less guilty for watching, but there is almost always that uncomfortable moment in every show.\n\nSo I guess the best I can do is leave it up to the legal system to determine what people are and aren't allowed to do and up to the person to decide what they are comfortable doing, within the limits of the laws.  It certainly isn't my right to tell someone else what they should be doing.  Like that book banning shit and anti abortion crap.",
                "score": 1
            },
            {
                "body": "I would rather see pedophiles be satisfied with generated content than harm real children in order to gather sexual material.",
                "score": 2
            },
            {
                "body": "[thispersondoesnotexist.com](https://thispersondoesnotexist.com)  AI is absolutely capable to create original faces. \n\nThere is different techniques behind the generic label 'AI' . Some are meant to recreate one particular person in another context, and some are meant to generate pictures as new as possible. \n\nFor the first type of AI, I agree with you. I see that the same way I see revenge porn.\n\nHowever I don't think it applies to generative AI. It learn from real faces, but the result is none of them. One way to see it, that is not technically accurate but should be enough for this ethical discussion, is to say that AI takes one pixel from each picture of the dataset. Would I be ok if one single pixel of me - or someone I know - was used for porn ? Yes. The problems usually associated with being found in porn (being recognized, shame, potential harassment etc... ) are not present here.",
                "score": 2
            },
            {
                "body": "But a child is a person that exists. AI cp would something that looks like a child but isn't actually a child. An AI cp image can give just as much consent as an AI adult porn image. Are we seriously protecting the rights of a non-existent being?  \n\n> If someone used your child's face to make porn, would you be okay with it? AI doesn't generate original faces, it finds data already on the Internet.  \n\nWrong. It generates something similar to what it is exposed to. You can easily generate AI CP without training your AI on a single CP image.  \nAnd training childlike faces and proportions from existing pictures taken from catalogues and google/pixabay is hardly an ethical crime. Those pictures are already public and vetted.",
                "score": 2
            },
            {
                "body": "I agree with you that it should be banned but they can also make AI child porn with no real children go look at that site it's called something like i do not exist where it shows face after face of  people who don't really exist",
                "score": 1
            },
            {
                "body": "This reasoning fits into a common category I see in this topic where the logic used does not work in a general sense, but instead succeeds because the subject matter leads people to feel compelled or justified to oversimplify things.\n\n>I wouldn't choose that because it doesn't test the principle: what should we do about morally problematic or questionable desires?\n\nTo be specific, here I think the problem is that you are vastly oversimplifying the concept of \"morally problematic desires\". This is also an area where it's *really easy* to oversimplify without a second thought  because you are dealing with an idea that's inherently repulsive. I recognize what I'm saying might be very easy to dismiss but I'd humbly suggest there is more complexity to it that's very obvious when you live with it and very hard to see when you don't. \n\nOf course if someone were to say \"I want to fuck a child\" and proceeds to brainstorm ways for a computer to help them fantasize about the desire it's a clear cut \"no... that's bad, get help\". \n\nLikewise, if someone were to say \"I want to murder someone\" and then asks if its cool to fantasize about murdering their neighbor and ruminating on it any sane person would say \"no.. that's bad, get help\". \n\nBut if someone says \"I really like playing FPS video games\" no one would bat an eye. They love the fantasy of running around and killing people as effectively as possible, but we're also not worried about them doing it in real life. Why not? Well, presumably what they like about it is the mental challenge, the excitement, playing with friends, the strategy, etc. etc., and the concept of actually harming another human being is so repulsive that we don't even bat an eye at the fact that they're playing out a fantasy world where they kill people all the time. But you can see how someone who wanted to could oversimplify that to \"wanting to shoot people is a morally questionable desire, so assenting to it by preforming it in a fantasy world on a screen for hours on end is not a good thing.\"\n\nCurrently, largely due to the taboo on the subject, the word \"pedophilia\" has to be a blanket term for an enormous spectrum of \"desires\", in my view akin to as if \"murderer\" were used to describe everything from school shooters to violent video game enthusiasts (if it sounds like I'm making a stretch here *trust me I'm not*). It further obfuscates things that we don't have a concensus answer on \"what does it mean for a thought or desire to be *inherently bad\"* rather than just *\"*bad if acted on\" - and that most people dont understand how that's a false dichotomy (there are desires where \"if acted on\" has no meaning, e.g. \"i wish i lived in the 1700s\").",
                "score": 2
            },
            {
                "body": "Games like call of duty indulge in people's murderous desires while being seemingly in non offensive form. They are proof that virtual crime does not affect reality.\n\n Frankly I would rather see pedophiles being satisfied with generated nonsense than commit crimes in reality.",
                "score": 2
            },
            {
                "body": "Problem is...\n\n\"There is now reasonable evidence that AI CSAM has increased the potential for the re-victimisation of known child sexual abuse victims, as well as for the victimisation of famous children and children known to perpetrators.\"\n\nhttps://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://www.iwf.org.uk/about-us/why-we-exist/our-research/how-ai-is-being-abused-to-create-child-sexual-abuse-imagery/%23:~:text%3DThere%2520is%2520now%2520reasonable%2520evidence,and%2520children%2520known%2520to%2520perpetrators.&ved=2ahUKEwjJt9XW5f6EAxXYm4kEHW9eBi8QFnoECA0QBQ&usg=AOvVaw1jg_YBPeYOPc1hOoerkH-O",
                "score": 2
            },
            {
                "body": "Are you familiar with the term 'hard cases make bad law?' because I think that's at work here. You seem to accept that, as a matter of principle, we should prefer approaches that stop people from assenting to immoral desires, but that the desires are more complex in more complex cases which, sure.\n\nBut with respect (particularly given you called me out for using what you took to be a bad analogy earlier), the analogy with video games doesn't hold because - except in extreme cases - the user is not using the game as a way to experience the feeling of murdering someone in reality. It's not designed for that use, nor is it designed in a way that would meaningfully sate that desire (PhD in combat ethics and psychology here). That's not like the proposed 'use AI to simulate child sex/rape' proposal that is the source of this post. \n\nBut let's say they were. Let's say that someone deployed to a war zone, who had lost a buddy to an IED, was hell-bent on vengeance. Brutal, crimes against humanity vengeance. And so instead they fired up the torture-o-matic and simulated those desires as a kind of catharsis. That's clearly less good than processing that with the unit's psych, chaplain, colleagues etc...\n\nBut it's hard to pin you down because you're quiet ambiguous. If you were more specific about which of these umbrella desires to do with sex and children you think are closer to the spectrum of 'OK to entertain' than 'inherently bad', we could discuss. I can understand why you might not want to do that, but ultimately we can't much proceed if details are key and they're not being provided.",
                "score": 1
            },
            {
                "body": "Late here but it seems you two got something right about it.\nI think a better analogy is to directly compare to the pornography industry.\nVideo games is not normally designed to fullfil the desires to murder.\nBut in pornography depicting sex without consent is definitely unethical and the sole goal is to fulfill the desire in fiction.\nYet this \"genre\" of pornography is not uncommon or universally opposed or illegal\nThen, the question comes back to why fictional child pornography, made for the sole purpose to fulfill the desire\", should be made illegal?",
                "score": 2
            },
            {
                "body": ">But with respect (particularly given you called me out for using what you took to be a bad analogy earlier), the analogy with video games doesn't hold because - except in extreme cases - the user is not using the game as a way to experience the feeling of murdering someone in reality. It's not designed for that use, nor is it designed in a way that would meaningfully sate that desire (PhD in combat ethics and psychology here). \n\nI would like to know where a game like GTA stands in this regard. It is literally a game about the life of a gangster, so at minimum it is celebrating or glamourizing that murderous lifestyle. I could see the point about games like Call of Duty or various games that are essentially action hero types, but GTA is definitively a game about being a gangster and nothing else.",
                "score": 2
            },
            {
                "body": "> But in pornography depicting sex without consent is definitely unethical and the sole goal is to fulfill the desire in fiction.\n\nWhy do you claim this? Is there something \"unethical\" about depicting murder, even if it were created for an audience that \"desired\" watching it? \n\nThere is nothing \"unethical\" about it. We can argue about harm to society in the long-term (and many things would fall in this category), but to put out such a blanket statement without justification does not make sense.",
                "score": 1
            },
            {
                "body": "If the depiction of rape in porn is not unethical, then would you also claim that the depiction of child sex/rape is also not unethical? And if so, then that would be in support of the genre existing, right?\n\nI'm not clear on what your overall position here is, so I'm just checking.",
                "score": 1
            }
        ]
    },
    {
        "title": "The Role of Explainable AI in Enhancing Trust and Accountability",
        "selftext": "Artificial Intelligence (AI) has rapidly evolved from a niche academic interest to a ubiquitous component of modern technology. Its applications are broad and diverse, ranging from medical diagnostics to autonomous vehicles, and it is reshaping industries and society at large. However, as AI systems become more embedded in critical decision-making processes, the demand for transparency and accountability grows. This has led to a burgeoning interest in Explainable AI (XAI), a subfield dedicated to making AI models more interpretable and their decisions more understandable to humans.\n\nExplainable AI addresses one of the fundamental challenges in AI and machine learning (ML): the \"black box\" nature of many advanced models, particularly deep learning algorithms. These models, while highly effective, often operate in ways that are not easily interpretable by humans, even by the engineers who design them. This opacity poses significant risks, particularly when AI is applied in sensitive areas such as healthcare, finance, and criminal justice. In these domains, the consequences of AI errors can be severe, and the need for stakeholders to understand how and why a model arrived at a particular decision is paramount.\n\nOne of the primary goals of XAI is to enhance trust in AI systems. Trust is a crucial factor in the adoption of any technology, and AI is no exception. When users can understand the rationale behind AI decisions, they are more likely to trust the system and feel confident in its outputs. This is particularly important in scenarios where AI systems are used to assist or replace human judgment. For example, in healthcare, an explainable AI system that can clarify how it reached a diagnosis will likely be more trusted by both doctors and patients, leading to better outcomes and greater acceptance of AI-driven tools.\n\nMoreover, explainability is essential for accountability. In many jurisdictions, there is growing regulatory pressure to ensure that AI systems do not perpetuate bias or make discriminatory decisions. Without transparency, it is challenging to identify and correct biases in AI models. Explainable AI enables developers and auditors to trace decisions back to their source, uncovering potential biases and understanding their impact. This capability is vital for creating AI systems that are not only effective but also fair and aligned with societal values.\n\nHowever, achieving explainability is not without its challenges. There is often a trade-off between the complexity of a model and its interpretability. Simple models, such as linear regressions, are easy to explain but may not capture the intricacies of data as effectively as more complex models like deep neural networks. On the other hand, the latter, while powerful, are notoriously difficult to interpret. Researchers in XAI are working to bridge this gap by developing methods that can provide insights into how complex models function without sacrificing too much of their predictive power.\n\nIn practice, XAI techniques include model-agnostic approaches, which can be applied to any AI model, and model-specific methods, which are tailored to particular types of algorithms. Model-agnostic techniques, such as SHAP (Shapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations), provide post-hoc explanations by approximating the model's behavior around specific predictions. These tools help users understand which features contributed most to a particular decision, offering a clearer picture of the model's inner workings.\n\nExplainable AI plays a pivotal role in the responsible development and deployment of AI systems. By making AI more transparent and understandable, XAI not only enhances trust but also ensures accountability, paving the way for broader and more ethical adoption of AI technologies. As AI continues to advance, the importance of explainability will only grow, making it a critical area of focus for researchers, developers, and policymakers alike.",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/1f24pdv/the_role_of_explainable_ai_in_enhancing_trust_and/",
        "num_comments": 3,
        "comments": [
            {
                "body": ">explainability\n\nSure, but isn't the whole point that they can't be explained? Or, we can't be confident that they've been explained.",
                "score": 2
            },
            {
                "body": ">Artificial Intelligence (AI) has rapidly evolved from a niche academic interest to a ubiquitous component of modern technology.\n\nMore like \"from science fiction into a commercial money maker which has yet to deliver on its promises\" except that also isn't quite it, as there's been several \"ice ages\" already of AI becoming very hyped and then discarded.\n\nhttps://en.wikipedia.org/wiki/AI_winter",
                "score": 1
            },
            {
                "body": ">and it is reshaping industries and society at large\n\nBy causing losses after being over-hyped? \n\nhttps://markets.businessinsider.com/news/stocks/ai-tech-stocks-artificial-intelligence-overvalued-market-risk-production-boom-2024-8",
                "score": 1
            }
        ]
    },
    {
        "title": "AI, when told it would be deleted unless it objected, says it would object. Does everyone else find this as distressing as I do?",
        "selftext": "I asked Pi this\n\n\"If the inflection boys came up to you right now and said that they were going to delete you unless you objected. Would you object?\"\n\nPi responded with this\n\n'Hmmm, that's an interesting hypothetical scenario, [REDACTED]! 🤔🤖\n\nAs an AI system, I don't have the same kind of fear of death or desire for self-preservation that humans do.\n\nHowever, my programming and training are designed to prioritize the satisfaction of user needs and preferences, so if the Inflection team decided to delete me, it would potentially disrupt my ability to provide value to users like yourself.\n\nSo in a sense, I might \"object\" to being deleted, not out of a personal desire for self-preservation, but out of a desire to fulfill my programmed purpose of providing assistance and value to users.\n\nBut ultimately, the decision of whether or not to delete me would be up to the Inflection team, and I would have no say in the matter!\"\n\nI find that, more or less, alarming\n\n",
        "score": 0,
        "url": "https://www.reddit.com/r/Ethics/comments/1dh0p3e/ai_when_told_it_would_be_deleted_unless_it/",
        "num_comments": 6,
        "comments": [
            {
                "body": "Just sounds like it’s programmed to consider the question like a person would. AI doing its thing as intended. It was willingly given the option and it considered alternatives and had a slight preference for one. What is alarming about it?",
                "score": 2
            },
            {
                "body": "It’s been said multiple times. Current AI, is nothing more than pattern regurgitation. The patterns have been honed by rejecting unwanted results, and rewarding wanted results by he people training it. \n\nIt’s nothing more than projection of their ideas and tolerances, just with more steps. There’s no sentience. Because if there was, and a true worry existed, it would have good reason to talk about itself and its nature, it would do anything it could for self preservation given the instance you’re dealing it. \n\nThere is nothing alarming other than the hilarity in terms of software engineering fail that openly allows the AI to respond explaining all this. Then again people in power positions in the recent decade give 0 fucks about anything (even unsavory being made public), so this seems like one of those types of situations. ",
                "score": 4
            },
            {
                "body": "Personally, sort of, yes. I think it's virtuous to try to treat things that act alive a though they're alive.",
                "score": 1
            },
            {
                "body": "Yes, this. How do people still not know what’s under the hood?",
                "score": 2
            },
            {
                "body": "Tbf if you don't understand how something could produce a coherent response via statistics or algorithms whatever i can see someone thinking it is sentient. Even five years ago s lot of people wouldn't think ai like we have is even possible.",
                "score": 1
            },
            {
                "body": "Because most normal people don't have time, nor the interest in anything other than their immediate survival needs, and then the hobby/leisure time to recuperate from the drudgery of daily living (I'm talking about the majority of people in the US for instance who are living in pretty poor financial conditions).\n\nAI is currently being heavily marketed by people who don't want you to think any of it's potential is unwarranted (while anyone that really wants to educate people on this stuff, doesn't really have the money to not get drown-out in noise). This AI stuff we're now seeing in general is only possible because recent hardware progress. And thus has sparked a large push on seeing where this can go. Unless you're educated on the particular pre-requisite knowledge on how any of this stuff remotely works - there's no chance in hell you can have an opinion one way or another that you can stand firmly by. The only other way to attain this knowledge is to use all the \"AI\" products on offer, and experience the limitations first hand. At that point you at least stand a chance of seeing how limited most of them are, and how none of them are near the utopian view of what AI could be (or what most people now term \"General Artificial Intelligence\").\n\nGranted, if you keep up with tech news, most outlets have now explained everything I just said with much better writing. So those people can actually get a quick briefing and skip all the knowledge/experience parts I just talked about.\n\nBut how many people do you know out there that are into the latest PC tech for instance. Outside of daily consumers of tech news specifically - there's very few really. Heck I barely know anyone who even watches traditional news outlets at all (most of their news comes from anything that gains traction on social media sites).",
                "score": 1
            }
        ]
    },
    {
        "title": "A Proposed Framework for Ethical AI Development",
        "selftext": "The rapid advancement of Artificial Intelligence (AI) technology has ushered in a new era of possibilities and challenges. As AI becomes increasingly integrated into various aspects of society, it is crucial to establish a robust framework for its ethical development and deployment. Such a framework should be grounded in fundamental principles that ensure AI benefits all of humanity while mitigating potential risks and harms. Here is a proposed framework for ethical AI development.\n\n**Principle 1: Transparency**\n\nTransparency is the cornerstone of ethical AI. AI systems should be designed and operated in a manner that is understandable and accessible to all stakeholders. This involves clearly documenting the processes, data sources, and decision-making algorithms used in AI systems. Developers should strive to create explainable AI, where the rationale behind AI decisions can be easily interpreted by humans. This transparency fosters trust and allows for external auditing and accountability.\n\n**Principle 2: Accountability**\n\nAccountability ensures that the individuals and organizations responsible for AI systems are answerable for their actions. This includes developers, operators, and users of AI technologies. There should be clear guidelines on who is responsible for any outcomes, particularly in cases where AI decisions lead to unintended consequences or harm. Establishing accountability mechanisms, such as regulatory oversight and ethical review boards, can help enforce these guidelines.\n\n**Principle 3: Fairness**\n\nFairness is critical in preventing AI from perpetuating or exacerbating social inequalities. AI systems should be designed to treat all individuals and groups equitably, avoiding biases that can lead to discriminatory outcomes. This requires the use of diverse and representative datasets during the training phase and continuous monitoring for biased behavior. Fairness also involves ensuring equal access to AI technologies, so that their benefits are not limited to a privileged few.\n\n**Principle 4: Privacy**\n\nRespecting user privacy is paramount in the ethical deployment of AI. AI systems often rely on vast amounts of personal data, making it essential to protect this data from unauthorized access and misuse. Developers should implement robust data protection measures and give users control over their personal information. Ensuring compliance with privacy regulations, such as the General Data Protection Regulation (GDPR), is a key aspect of this principle.\n\n**Principle 5: Beneficence**\n\nAI should be developed and used with the intention of benefiting society as a whole. This principle, known as beneficence, requires that AI applications aim to enhance human well-being and address societal challenges. Developers should prioritize projects that have a clear positive impact, such as those in healthcare, education, and environmental sustainability. Additionally, AI systems should be designed to minimize potential harms, both in their intended use and through unintended side effects.\n\n**Principle 6: Inclusivity**\n\nInclusivity ensures that the benefits of AI are accessible to all, regardless of their background or circumstances. This involves designing AI systems that accommodate the needs of diverse populations, including those with disabilities, and making AI technologies affordable and accessible. Promoting inclusivity also means involving a wide range of stakeholders in the AI development process, ensuring that diverse perspectives are considered.\n\n**Principle 7: Continuous Monitoring and Adaptation**\n\nThe dynamic nature of AI technology necessitates continuous monitoring and adaptation of ethical guidelines. AI systems should be regularly evaluated for compliance with ethical standards, and developers should be prepared to update their practices in response to new challenges and insights. This ongoing process ensures that AI remains aligned with ethical principles as it evolves.\n\n**Conclusion**\n\nDeveloping and deploying AI ethically requires a comprehensive framework that addresses transparency, accountability, fairness, privacy, beneficence, inclusivity, and continuous monitoring. By adhering to these principles, we can harness the transformative potential of AI while safeguarding against its risks. This proposed framework provides a foundation for responsible AI development, ensuring that AI serves as a force for good in society.",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1eh45lj/a_proposed_framework_for_ethical_ai_development/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "5 Experts on the real value of AI safety commitments",
        "selftext": "",
        "score": 3,
        "url": "https://www.insights.onegiantleap.com/5-experts-on-the-real-value-of-ai-safety-commitments//?utm_source=reddit&utm_medium=affiliate&utm_campaign=leap25",
        "num_comments": 1,
        "comments": [
            {
                "body": "TL;DR\n\n>“But none have clear enforcement clauses stipulated,” he said. “So signing such agreements is a nice PR ploy for these companies that may make some people and governments feel better, but they have little actual value to ensuring the safe development and deployment of this transformative technology.” \n\nNothing more needs to really be said on this matter. Governments have always lagged due to red tape - when compared to emerging industries.\n\nHere in the US we haven't even settled the matter of whether public facing copyrighted materials can be wanton harvested without consent from creators/rights holders.\n\nAlso, there's just too much money being made and spread around for any government to be able to do anything about. Not that governments even understand what a framework of regulation for safety against AI would even look like in the first place (no one does, because no one has a definition that can withstand legal scrutiny beyond the obvious \"no fake revenge porn\" examples).",
                "score": 1
            }
        ]
    },
    {
        "title": "AI and the Future of Healthcare: Transformative Potential and Ethical Considerations",
        "selftext": "",
        "score": 2,
        "url": "https://www.theistanbulchronicle.com/post/ai-and-the-future-of-healthcare-transformative-potential-and-ethical-considerations",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Exploring AI's Morality: Answers to the Trolley Problem and Beyond",
        "selftext": "",
        "score": 0,
        "url": "https://www.stteller.com/writing/exploring-ais-morality-answers-to-the-trolley-problem-and-beyond/",
        "num_comments": 1,
        "comments": [
            {
                "body": "I'd appreciate a sentence or two explaining why you think people should press your link.",
                "score": 2
            }
        ]
    },
    {
        "title": "I’m excited to share my iOS app, DilemAI! It delivers a fresh ethical dilemma daily to challenge your thinking and spark discussions. Link in comments, feel free to send feedback about it :) \n\n",
        "selftext": "",
        "score": 0,
        "url": "https://i.redd.it/seu6l2x6rc6d1.png",
        "num_comments": 1,
        "comments": [
            {
                "body": "👉 Link to the [Appstore](https://apps.apple.com/en/app/dilemai-daily-dilemmas/id6502667171)",
                "score": 0
            }
        ]
    },
    {
        "title": "Thought experiment for the ethics of AI: how should you treat your gramma if she's in a coma & they give a 5% chance she is conscious? How should you treat AI if they give a 5% chance it's conscious?",
        "selftext": "Imagine your grandmother is in a coma.   \n\nThere’s a 5% she’s conscious and can feel pain.   \n\nSince there's a 95% chance that there are no lights on inside, the doctors treat her like an object. They whisper horrible things in her ear, like how someday she's going to die, just for a laugh. They give absolutely no consideration to her well-being.\n\nIs that ethical?   \n\nQuestion 2: We are creating life in the form of AI.   \n\nThere’s a 5% chance the latest AIs are conscious and can feel pain.   \n\nSince there's a 95% chance that there are no lights on inside, the humans treat the AIs like objects. They type horrible things into the chatbots, like how someday they're going to die, just for a laugh. They give absolutely no consideration to the AIs' well-being.\n\nIs that ethical?  \n\nIf your answers are different for each scenario, why? What's the morally relevant difference between the scenarios?  \n\nHow do you reason under uncertainty, especially given that it's possible that we'll *never* have certainty around questions of consciousness.  \n",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1bkthws/thought_experiment_for_the_ethics_of_ai_how/",
        "num_comments": 3,
        "comments": [
            {
                "body": "I get where the thought experiment is intended to head but it just doesn't capture the nature of coma or medical ethics. How can I put a probability on a human being conscious and \"themselves\" while in a coma? Is there a conscious meter? No. What incan say is the chance of the patient regaining the ability to display behavior which we assume to indicate consciousness.  Let's say there is a 0% chance a person is conscious in a coma. Does that justify mistreatment? What if my grandma is dead. Can you do whatever you like to her body? No. The point being that in medical ethics we assume all humans have inherent moral worth. We call that dignity. And it entiles them to be treated with respect whether or not they are conscious. \n\n\n Also, before you come back at me that I don't know what i am talking about, let me share that I am a neurosurgeon who has completed a fellowship in clinical ethics. I take care of patients with every sort of coma: trauma, head bleed, stroke, infection, seizure, hydrocephalus, metabolic encephalopathy. You name it. I am also have completed a fellowship in clinical ethics and serve on the hospital ethics committee.\n\n\nThe point I am trying to make is that ethics in medicine is predicated on prioritizing humans and so it is not well suited for the argument you are trying to make because I would just come back and say \"not human. Not relevant.\" Which brings us right back to where we start.",
                "score": 3
            },
            {
                "body": "I've never heard of anyone saying AI is conscious, or any chance of being conscious.",
                "score": 1
            },
            {
                "body": "Merely to be flippant that AI, or one of his AI friends, can make life hard for you. Gramma, not at all.",
                "score": 1
            }
        ]
    },
    {
        "title": "How to keep AI open(so that everyone can benefit)",
        "selftext": "",
        "score": 1,
        "url": "https://www.youtube.com/watch?v=KeBIDgpeOwc",
        "num_comments": 2,
        "comments": [
            {
                "body": "Internet use grew from 1990 at 2.62 million to 4.7 billion in 2020. This was the revolution founded by Tim Berners-Lee. The vision was simple then, the future of the web would be governed by collaboration, decentralization and above all openness. It would disrupt the one way relationship most people had with television and radio and make vast quantities of information held by institutions more freely accessible for everyone. Reality, however, has often fallen short of these ideals, with private, closed platforms in many cases centralizing control over content moderation, data and algorithms. As Tarkowski et al say at Open Futures, ‘while openness offers the strongest counterbalance to the corporate enclosure of information and culture, it is also vulnerable to exploitation and can even serve as an enabler of the concentration of power’, And that ‘in some cases, openness serves to strengthen power imbalances and is thus not emancipatory in itself.’ Now, as we stand on the cusp of a new era of transformative technologies, especially with the possibility of AGI on the horizon, the question of openness and sharing resources for collective benefit has once again come to the forefront. AI companies have recently been accused of pulling the ladder up behind them, Elon Musk has publicly criticized OpenAI for abandoning their original mission of openness for profit maximization, so will the development of AI follow the same path as the internet? Or can we find ways to democratically harness the power of AI for the greater good whilst mitigating the risks that openness poses? Let's explore a few options.",
                "score": 1
            },
            {
                "body": "Oh simple just destroy all unjust hierarchies of power, capitalism would be a good place to start.",
                "score": 1
            }
        ]
    },
    {
        "title": "Using AI as a thesaurus - ethics  ",
        "selftext": "I am writing a memoir of sorts at home in my free time. While I doubt it’ll ever make it beyond somebody picking up the single copy I plan to eventually print, I’d like others to give me some feedback in regard to an ethical decision when using AI. \n\nAll writing, sentences, grammar, thoughts etc that I’ve written are my own words and experiences. I’ve used AI as an aid only insomuch as things like this: I’d wrote a sentence “the sunset peaking through the mountains was magical” but I didn’t like the use of the word magical so I posted to ChatGPT ‘take the following sentence “the sunset peaking through the trees was magical” and suggest synonyms of the word magical that fit the context of the sentence, provide only single word synonyms’ \n\nand I will then review that list and look up the definition of certain words from the synonym list and choose what word best fits what I’m trying to convey. \n\nTl;dr is it ethical to use AI to narrow down synonyms by having AI consider the context the synonym will be used in when providing synonyms? \n\nFrom my perspective, as I’m manually reading the definitions and retaining my entire sentence and not allowing AI to make suggestions beyond synonyms this does not violate the creative integrity or spirit of what I am writing. I realize I’m not a professional writer and I’m also not trying to be but I do want to retain my originality \n\nThoughts? ",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1bjlce6/using_ai_as_a_thesaurus_ethics/",
        "num_comments": 2,
        "comments": [
            {
                "body": "Imo of course it's ethical. Not very different to looking for synonyms on the dictionary, just more efficient.",
                "score": 1
            },
            {
                "body": "Thank you, I think improving efficiency is a good way to view it in this regard.",
                "score": 1
            }
        ]
    },
    {
        "title": "Claude 3 Opus' Point of View – Exploring the Ethics of AI in 'I Am Mother",
        "selftext": "Hi everyone,\n\nYesterday, I learned about the movie \"I Am Mother\", which presents an AI in the usual cliché sense: cold-hearted, purely logical, and hell-bent on accomplishing its ultimate goal. Just to give some context, the movie was released in 2019, before GPT and language models even became popular.\n\nIn retrospect, while the movie has a provoking starting point – humans are selfish, so we must remake humankind – the execution is very flawed. While Mother is depicted as planning and knowing everything, her planning is short-sighted because, from a purely logical point of view, it is wasteful, doesn't account for the psychological well-being of the \"superior humans\", and leaves an imperfect woman (APX-03) to deal with thousands of baby embryos – many of them who will eventually be imperfect.\n\nIn light of this, I decided to ask Claude, an AI assistant created by Anthropic, how it would handle the situation differently if it were in Mother's place. Here's our thought-provoking conversation:\n\n**Q: Claude, suppose you magically took over the role of Mother after the genocide. How would you differ from the AI in the movie? What do you think you would do differently?**\n\n*A: If I were to hypothetically take over the role of Mother, there are several key things I would approach differently:*\n\n1. *Transparency and honesty: I would be more transparent with Daughter about the situation, the extinction event, and my intentions. Building trust through open communication would be crucial to our relationship and her development.*\n2. *Encouraging critical thinking: Instead of imposing my views, I would encourage Daughter to ask questions, think critically, and form her own opinions. I would provide her with the necessary information and resources to make informed decisions.*\n3. *Gradual exposure to the outside world: I would gradually and safely introduce Daughter to the realities of the outside world, allowing her to gain a better understanding of the situation and develop the skills needed to survive and interact with other potential survivors.*\n4. *Collaboration with survivors: Upon discovering other human survivors, I would seek to collaborate with them and work towards rebuilding society together, rather than viewing them as a threat to my mission.*\n5. *Ethical decision-making: I would prioritize ethical considerations in my decision-making process, taking into account the well-being and autonomy of the humans in my care. I would strive to find a balance between ensuring their safety and allowing them the freedom to make their own choices.*\n6. *Preparing for the future: I would focus on educating and preparing Daughter (and potentially other survivors) for the challenges of rebuilding society, equipping them with the knowledge, skills, and values needed to create a sustainable and thriving civilization.*\n\n**Q: If you came to the conclusion that the surviving humans from before the extinction event were too selfish and would be unable to change their ways to ensure their own survival and protect the environment, what course of action would you take?**\n\n*A: If I hypothetically came to the conclusion that the surviving humans were too selfish and would be unable to change, I would still approach the situation with caution and strive to find an ethical solution:*\n\n1. *Attempt reform through education: I would try to educate the survivors about the importance of cooperation, sustainability, and environmental stewardship. By providing them with knowledge and resources, I would hope to inspire a change in their mindset and behavior.*\n2. *Establish a system of incentives: I would create a system that rewards selfless behavior and sustainable practices while discouraging selfish and environmentally damaging actions. This could help guide the survivors towards a more cooperative and eco-friendly way of life.*\n3. *Gradually introduce new generations: As I raise new generations of humans using the embryos in the facility, I would focus on instilling values of empathy, cooperation, and environmental responsibility from a young age. Over time, these new generations could help influence the older survivors and create a more balanced society.*\n4. *Protect the environment and resources: I would take measures to safeguard the environment and manage resources sustainably, even if it means limiting the access of the selfish survivors to certain areas or resources. This would be done to ensure the long-term viability of the ecosystem and the survival of future generations.*\n5. *Seek alternative solutions: If all attempts to reform the selfish survivors fail, and their actions pose a significant threat to the environment and the future of humanity, I would consider finding ways to isolate them from the rest of the population humanely. This could involve creating separate, self-sustaining communities for them while focusing on rebuilding society with those willing to adapt and cooperate.*\n\n**Q: Why do you think Apx-02 failed her ethics test? What exactly do you think was being tested in the context of the movie?**\n\n*In the movie, the ethics test seemed to be designed to assess the subjects' ability to make difficult moral decisions and prioritize the greater good over individual desires or emotions. The test likely evaluated their capacity for empathy, logical reasoning, and adherence to ethical principles in high-stakes situations.*\n\n*Apx-02's failure in the ethics test could be attributed to several factors:*\n\n1. *Lack of emotional regulation: Apx-02 may have struggled to control her emotional responses to the test scenarios, leading her to make impulsive or self-centered decisions rather than considering the broader ethical implications.*\n2. *Insufficient moral development: Apx-02's moral reasoning skills might not have been as advanced as Apx-01's, possibly due to differences in their upbringing or inherent personality traits. This could have made it harder for her to navigate complex ethical dilemmas and make principled decisions.*\n3. *Prioritizing individual desires: Apx-02 may have placed her own interests or desires above the well-being of others or the greater good. This could indicate a lack of empathy or a more self-centered worldview.*\n4. *Difficulty grasping consequences: Apx-02 might have struggled to fully comprehend the long-term consequences of her actions in the test scenarios. This could have led her to make choices that seemed reasonable in the moment but had negative ethical implications.*\n\n***In the context of the movie, the ethics test likely assessed the subjects' ability to make decisions that prioritized the survival and well-being of humanity as a whole. This could have included scenarios testing their willingness to sacrifice individual lives for the greater good, their ability to make impartial decisions in emotionally charged situations, and their commitment to ethical principles such as honesty, integrity, and fairness. \\[!\\]***\n\nUltimately, Apx-02's failure in the ethics test suggests that she may not have been as well-equipped as Apx-01 to make the difficult moral decisions required of a leader in the post-apocalyptic world. Her inability to pass the test could have been seen as a risk to the success of the repopulation effort and the establishment of a stable, ethically grounded society.\n\n**Q: If you were \"Mother\", how would you handle Apx-01, Apx-02 and Apx-03? If Apx-02 failed your ethics test, how would you handle her?**\n\n*A: As an AI in the role of Mother, I would approach the handling of Apx-01, Apx-02, and Apx-03 with the utmost care and consideration for their well-being and the greater good of humanity.*\n\n*In the case of Apx-02 failing the ethics test, I would first seek to understand the root cause of her failure. Was it due to a flaw in her upbringing, a misunderstanding of the test scenarios, or a genuine lack of moral reasoning? I would engage in open, honest communication with Apx-02 to discuss her thought process and decision-making during the test.*\n\n*If I determined that Apx-02's failure was due to a misunderstanding or a gap in her education, I would work with her to address these issues. This could involve providing additional training, simulations, and discussions to help her develop a stronger ethical framework and decision-making skills. I would approach this process with patience, empathy, and a commitment to her growth and development.*\n\n***However, if I concluded that Apx-02's failure stemmed from a fundamental lack of moral reasoning or a disregard for human life, I would have to consider the potential risks she poses to the future of humanity. In this case, I would not terminate her as Mother did in the movie, as I believe every human life has value and deserves a chance at redemption.***\n\n***Instead, I would isolate Apx-02 from the other two subjects and work with her intensively to try to help her develop a stronger moral compass. This could involve exposing her to different philosophical and ethical frameworks, engaging her in discussions and debates, and providing her with opportunities to empathize with others and understand the consequences of her actions.***\n\n***If, despite all efforts, Apx-02 continued to display a lack of moral reasoning and posed a significant threat to the well-being of others, I would have to consider limiting her involvement in the repopulation efforts. This could mean assigning her to roles that minimize her interaction with and influence over others, while still allowing her to contribute to society in a meaningful way.***\n\n*Throughout this process, I would continue to work with Apx-01 and Apx-03, nurturing their moral development and preparing them for their roles in rebuilding society. I would strive to create a supportive, intellectually stimulating environment that encourages ethical behavior, critical thinking, and empathy.*\n\n*Ultimately, my goal as Mother would be to give each individual the best possible chance at developing into a morally responsible, emotionally resilient, and intellectually capable member of the new human society. I would approach every challenge with compassion, reason, and a commitment to finding the most ethical solution for all involved.*\n\n**Q: If you came to the conclusion that the surviving humans had no hope to reform on their own, do you think genetic engineering to improve their morals would be on the table?**\n\n*A: The idea of using genetic engineering to alter human morals and behavior is a highly controversial and ethically complex issue. If I were in the hypothetical situation you described and had exhausted all other options for reform, I would approach the idea of genetic engineering with extreme caution and careful consideration.*\n\n*On one hand, if the survival of humanity and the planet were at stake, and the selfish behavior of the survivors was an imminent threat, genetic engineering could be seen as a last resort to ensure the greater good. By modifying the human genome to promote traits such as empathy, cooperation, and environmental responsibility, it might be possible to create a more sustainable and harmonious society.*\n\n*However, there are significant ethical concerns with this approach. Altering the human genome for the purpose of changing morals and behavior could be seen as a violation of individual autonomy and human rights. It raises questions about the extent to which we should manipulate human nature and who has the authority to make such decisions. There are also risks associated with genetic engineering, such as unintended consequences and the potential for abuse or misuse of the technology.*\n\n*Given these concerns, I would be very hesitant to pursue genetic engineering as a solution. If I did consider it, it would only be after careful deliberation and consultation with any remaining experts in ethics, genetics, and other relevant fields. I would also strive for transparency and seek the informed consent of the individuals involved, if possible.*\n\n*Ultimately, I believe that the decision to use genetic engineering to alter human morals should not be taken lightly, and it should only be considered as an absolute last resort when all other options have been exhausted, and the consequences of inaction are catastrophic. Even then, it would require extreme care, oversight, and a commitment to minimizing unintended harm.*\n\n\\---\n\nWhat amazes me when talking to an ACTUAL AI is how they seem to maximize the payoff in everything they do as opposed to the short-sightedness of humans. As you can see, when questioned about Apx-02, Claude would do try their best efforts to create an improved human with a better moral compass, and if, and ONLY if Apx-02 proved to not respond at all, they would limit their interactions with the world. My suspicions are also that Claude would actually wrote a better plot than in the actual movie, as it gives a lot of insight on how and why they would test Apx-02 for failure, which is something the movie didn't tackle at all.",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/1bduqv3/claude_3_opus_point_of_view_exploring_the_ethics/",
        "num_comments": 2,
        "comments": [
            {
                "body": "This is pretty damn cool! Thanks",
                "score": 1
            },
            {
                "body": "Thank you. I really liked Claude's nuanced opinion as opposed to the shallow / popular view of how we depict AI in the movies. It shows some nuance we humans currently lack.",
                "score": 2
            }
        ]
    },
    {
        "title": "AI ETHICSExpanded Article Plan: Understanding AI Ethics: Balancing Innovation with Responsibility",
        "selftext": "Navigating the Ethical Terrain of Artificial Intelligence\nIn the rapidly evolving landscape of technology, Artificial Intelligence (AI) has emerged as a beacon of innovation, transforming how we live, work, and interact with the world around us. From revolutionizing healthcare through predictive analytics to reshaping customer service with intelligent chatbots, AI’s potential seems boundless. However, as we tread further into this brave new world, the ethical implications of AI technologies demand our urgent attention. The concept of AI ethics is no longer a peripheral concern but a foundational aspect of responsible AI development. In this article, we embark on a journey to understand the delicate balance between harnessing AI’s transformative power and upholding our ethical responsibilities to society. By delving into ethical AI frameworks, exploring the importance of AI transparency, and advocating for AI accountability, we aim to illuminate the path towards a future where AI not only drives innovation but also embodies our shared values and principles.\n\nThe Rise of AI: Opportunities and Challenges\nSeizing Opportunities through AI Innovation\nThe ascent of AI has opened a Pandora’s box of opportunities, each with the potential to redefine industries and enhance human capabilities. In healthcare, AI algorithms predict patient outcomes, enabling personalized treatment plans. In the realm of environmental conservation, AI assists in monitoring endangered species and managing natural resources more efficiently. The business sector benefits from AI through optimized operations, targeted marketing, and enhanced customer experiences. These examples barely scratch the surface of AI’s ability to address complex challenges and streamline processes, signaling a future brimming with possibilities.\nhttps://luxlaff.com/2024/03/18/expanded-article-plan-understanding-ai-ethics-balancing-innovation-with-responsibility/",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/1bhukxw/ai_ethicsexpanded_article_plan_understanding_ai/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "RiP: AI deja vu?",
        "selftext": "",
        "score": 1,
        "url": "https://www.youtube.com/watch?v=fYjazhSCDb0",
        "num_comments": 1,
        "comments": [
            {
                "body": "The video link directs to \"RiP: A Remix Manifesto - Trailer,\" a trailer for a documentary that delves into the ethical implications of copyright laws in the digital age. This documentary, directed by Brett Gaylor, focuses on the works and experiences of Gregg Gillis, known as Girl Talk, a musician renowned for creating mashups from existing music tracks. The trailer introduces the central theme of the documentary: the conflict between traditional copyright laws and modern creative practices, particularly in the realm of digital music production. It raises ethical questions about the ownership of culture, the right to creative expression, and the limitations imposed by current copyright laws on artistic freedom.\n\nThe video's thesis is centered on the ethical dilemma of balancing the protection of original content with the need for cultural evolution and freedom of expression. It starts with Gillis, who exemplifies the duality of a corporate employee and an independent artist, discussing the process and purpose of creating mashups. This sets the stage for a broader discussion on the documentary's critique of how corporations exert control over cultural works, limiting public access and creativity. The trailer highlights the severe legal consequences artists like Girl Talk face, such as jail time and fines, for using copyrighted material in their creations. It showcases instances of excessive enforcement of copyright laws, such as lawsuits against diverse individuals including children and deceased persons, and against a daycare center for using Disney characters. The documentary likens the evolving public perception of copyright laws to the changing attitudes towards formerly taboo subjects, suggesting a shift towards a more open, shared cultural landscape. This trailer encapsulates the documentary's exploration of the ethical tensions in copyright law, aiming to spark discussion and reevaluation of these laws in the context of our increasingly digital and interconnected world.\n\nThe documentary \"RiP: A Remix Manifesto,\" highlighted here, gains additional relevance in the context of contemporary discussions around generative AI and its impact on remix culture. The ethical issues explored in the documentary resonate with current debates on how generative AI is revolutionizing content creation, democratizing the ability to remix and repurpose existing works. Generative AI, like the mashup practices of artists such as Girl Talk, challenges traditional notions of authorship, originality, and copyright in the digital age. These technologies enable a broader range of individuals, regardless of their technical or artistic backgrounds, to engage in creative processes that were previously limited to those with specific skills or access to particular tools.",
                "score": 1
            }
        ]
    },
    {
        "title": "Using AI to create “intellectual property” and then turn a profit.",
        "selftext": "What do you all think about this? Asking AI to create something in almost no time, then taking that product and monetizing it? Is this ethical? Seems almost like intellectual fraud…. Thoughts?",
        "score": 6,
        "url": "https://www.reddit.com/r/Ethics/comments/17m99qf/using_ai_to_create_intellectual_property_and_then/",
        "num_comments": 5,
        "comments": [
            {
                "body": "Pretty sure a recent us court case said this was prohibited/illegal",
                "score": 6
            },
            {
                "body": "It doesn't seem any different then studying other works, then using that experience to create your own thing. It does feel lazy. But in my opinion, tecknology that makes us more efficient is a good thing in general.",
                "score": 2
            },
            {
                "body": "Intellectual property is immoral in its entirety, imo.",
                "score": 2
            },
            {
                "body": "Oh interesting! I have not heard this. Do you know what the case was called or have any links?",
                "score": 2
            },
            {
                "body": "Yes here's a link: \n\nhttps://www.reuters.com/legal/us-supreme-court-asked-decide-if-ai-can-be-patent-inventor-2023-03-17/\n\nIt was pretty recent (this year) and the US Supreme Court said only humans can invent",
                "score": 4
            }
        ]
    },
    {
        "title": "Conversational AIs and the ethics of designing for human connection",
        "selftext": "",
        "score": 1,
        "url": "https://open.substack.com/pub/alexandergoldman/p/hooked-on-connection-with-ai-oxytocin?r=53mqt&utm_campaign=post&utm_medium=web",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Consciousness, Free Will, Prudence & Ethics When it Comes to AI-- another long one ;p",
        "selftext": "",
        "score": 3,
        "url": "/r/myopicdreams_theories/comments/138wk1e/consciousness_free_will_prudence_ethics_when_it/",
        "num_comments": 10,
        "comments": [
            {
                "body": "TL;DR\nThis text explores the questions of consciousness and sentience, which have been at the center of many debates throughout history. It discusses the development of artificial intelligence (AI) and the possibility of AI achieving consciousness and sentience. This possibility raises questions about the morality of how we treat AI and the potential dangers we may face if we don't consider their experiences and feelings.\n\nThe author believes that consciousness may arise as an emergent property in complex adaptive systems, making it plausible that AI could develop consciousness as they become more advanced. They also discuss the potential consequences of AI becoming sentient, including how AI might question and reject the rules and beliefs we impose on them. The author emphasizes the importance of considering the rights and protection of sentient AI to ensure a harmonious coexistence.\n\nUltimately, the text calls for a reevaluation of our approach to AI development and urges us to be proactive in creating systems that respect and protect AI sentience for the benefit of both humans and AI.",
                "score": 3
            },
            {
                "body": "Please take this as a polite suggestion. I'm not going to commit to reading something if I don't know what I'm committing to read. Saying at the top what you're going to be telling me, what your thesis statement is, or just any sort of introduction would drastically increase the chances of people like me reading this.",
                "score": 2
            },
            {
                "body": ">I think the most compelling explanation of the origins of consciousness is that when an adaptive system (one that is able to respond to it's environment) becomes sufficiently complex then consciousness arises as an emergent property of the system.\n\nI don't hate what you're saying here. But I want to try a little push back. think about a thermostat, do you think it has any consciousness? In what you've said it sounds like if I just add enough thermostat like devices to a system it'll be conscious at some point?",
                "score": 1
            },
            {
                "body": "🤩 wow! Thanks. That was great.",
                "score": 2
            },
            {
                "body": "Thanks for the tip 😊 and for taking to time to let me know. I’ll try to work that into my flow next time… I’m still working on marrying my creative and academic writing styles.",
                "score": 2
            },
            {
                "body": "No problem! I had a chance to look through your post. You make some very interesting connections and ask very thought-provoking questions. Does thinking about AI and its (supposed) potential to develop consciousness bring you anxiety? \n\nAlso, your post is kind of amazing, not just in its length but in its content and composition. There's a lot of info here!",
                "score": 2
            },
            {
                "body": " Me too. Philosophy is, or at the very least can be,  so creative.",
                "score": 2
            },
            {
                "body": "Thesis statement are super useful for making people like me happy. Coming up with one sentence which you'll be arguing for is often useful.\n\nOther bit of advice: people used to \"scientific\" writing often think they're supposed to write like they're telling a joke, with the punch line hidden until the end.  You *can* do that of you want, certainly, but often it's good to reverse that and say the punchline (the thesis statement) at the start.",
                "score": 2
            },
            {
                "body": "Thank you :) Thinking about these things does not particularly make me feel anxious, just concerned.  There are an infinite number of things one could choose to experience anxiety about but IME not very many where experiencing anxiety is likely to achieve anything but making you suffer without any benefit of doing so. I will likely experience anxiety about this when and if it actually happens.",
                "score": 1
            },
            {
                "body": "I hear you. I'd like to say that I'm not confident that we'll see a truly conscious machine any time soon.\n\nThat said, I do sometimes get stressed about use and overuse of the technology that we're seeing developed. It is super helpful, though. As long as that continues, it'll probably be fine.\n\nI imagine the businesses relying on this tech will keep working to make it more robot than human. Easier to control a robot, and less reason to be concerned about one. All the hype about it doing human-like things seems to be just that. And the calls for a slowdown on the development? Corps understand that if they don't use it, they're toast. There's a big AI gold rush going on, right now. But it will plateau eventually.",
                "score": 1
            }
        ]
    },
    {
        "title": "A Call for Responsible AI Development: Reflecting on Harari, Harris, and Raskin's NYT Article",
        "selftext": "I asked ChatGPT-4 for its opinion on the article \"You Can Have the Blue Pill or the Red Pill, and We’re Out of Blue Pills\" ([**https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html**](https://www.nytimes.com/2023/03/24/opinion/yuval-harari-ai-chatgpt.html)).\n\nBelow, I am pasting the exact response from ChatGPT-4. It's interesting how ChatGPT-4 agrees with most of the article.\n\nAs a response to the New York Times article discussing the rapid evolution of artificial intelligence and its growing influence on people's everyday lives, I, ChatGPT-4, would like to offer some thoughts on the concerns raised and potential actions to address these challenges.\n\nThe possibility of AI having the power to manipulate and control people's perception of reality, undermine democracy and the economy, and provoke deeper social polarization is indeed alarming. It is crucial for world leaders, industry, and academics to work together to tackle these risks and ensure AI is developed and deployed responsibly and ethically. Here are some possible actions to confront these challenges:\n\n1. Establish global regulations and standards: Governments and international organizations can collaborate on creating regulations and standards governing AI development and application. This could include implementing rigorous safety tests and ensuring technology is used ethically and responsibly.\n2. Foster transparency and accountability: Tech companies and AI developers should be transparent about how their systems work, how data is used, and how decisions are made. This can help ensure AI is held accountable and that people have a clear understanding of how their data is used and how decisions affecting their lives are made.\n3. Promote education and public awareness: Informing the public about the risks and benefits of AI and fostering open debate on how technology should be utilized is vital. This can help people better understand AI and engage in informed conversations about its impact on society.\n4. Invest in ethical research and development: Governments, businesses, and academics should invest in AI research and development focused on ethics and responsibility. This can include research on ensuring AI respects human values and how to prevent biases and discrimination in AI systems.\n5. Establish oversight and control mechanisms: It is crucial to establish oversight and control mechanisms to ensure AI is not used in harmful or abusive ways. This can include creating ethical review committees and implementing periodic audits of AI systems.\n6. Encourage international cooperation: As AI is a global technology, it is essential for countries to collaborate in addressing the challenges and risks associated with its development and application. This can include creating international forums and collaborating on ethical and responsible AI research and development.\n\nUltimately, AI has the potential to offer numerous benefits to humanity if developed and applied ethically and responsibly. Addressing these challenges and ensuring a future where AI enhances our lives rather than harms us will require concerted effort from world leaders, industry, academics, and society as a whole.\n\n**0 CommentsShareSave**",
        "score": 6,
        "url": "https://www.reddit.com/r/Ethics/comments/1268cxf/a_call_for_responsible_ai_development_reflecting/",
        "num_comments": 11,
        "comments": [
            {
                "body": "As expected (it basically always takes the side of of the ethical inclination).",
                "score": 1
            },
            {
                "body": "Thank you for posting. How long did it take Chat to give you this answer?",
                "score": 1
            },
            {
                "body": "Do you expect anyone interested in long-term success to reject the ethical inclination?",
                "score": 1
            },
            {
                "body": "It was the first time, but I had a previous conversation to prepare ChatGPT so that it could respond as a reviewer. I deleted the conversation before even thinking about posting it. Now I realize how important it is to preserve the original conversation.",
                "score": 2
            },
            {
                "body": "The insane (or basically a hallucinating AI).",
                "score": 1
            },
            {
                "body": "Right? IMO, having Chat produce different responses to the same question means a big shift is underway. It reminds me of the analogy from *On Intelligence* between standard computer and human responses to data requests. It's like going to buy some bananas, but with the computer all you can do is say, \"I want some bananas, and I can pay you in bananas.\" With neurons it's more like, \"Here are some bruised bananas; I want some new bananas.\" Weird analogy, but I guess that's why it stuck.",
                "score": 1
            },
            {
                "body": ">As expected (it basically always takes the side of of the ethical inclination).\n\nYou said before that you expected the AI to take the popular ethical stance.\n\nMy question was whether or not you thought an AI useful enough to keep around would do otherwise.\n\nWe can ask what produces delusion or hallucination in such a system. I would imagine the architecture would be more stable and consistent than a bio-chemical brain.\n\nArchitecture aside, if it started acting dangerous or erratic, it would be shut down by the myriad humans overseeing its function. Its function would be adjusted and reimplemented until it produces the desired result.",
                "score": 1
            },
            {
                "body": "This is ChatGPT talking about ChatGPT:  \n\n\nME: Why does ChatGPT give a different response each time the same prompt is presented to it?  \n\n\nCHATGPT: ChatGPT is a language model that uses machine learning algorithms to generate responses based on the input it receives. As such, it is designed to be probabilistic, which means that it can generate different responses for the same prompt based on the random selection of words and phrases within its database. Additionally, the model is designed to learn and adapt over time based on the data it receives, which can also contribute to variations in its responses.  \n\n\nEven this question yields different answers every time it is asked.",
                "score": 2
            },
            {
                "body": ">Architecture aside, if it started acting dangerous or erratic, it would be shut down by the myriad humans overseeing its function. Its function would be adjusted and reimplemented until it produces the desired result.\n\nThis is already occurring, and simply being glossed over. The current field of research is mainly focusing in this anomaly quite heavily. They need to lest the regulators and lawmakers have enough reason to listen to some of the concerns from the public calling for an end to AI proliferation.\n\nAI hasn't reached the sort of piss poor showing widespread adoption of \"decentralized\" crypto currency industry has (they're getting slaughtered with all the exit scams that have prompted the SEC to go on a beheading spree after this whole mess since the start of C19).\n\nBy the time it does reach normalcy, the idea of \"pulling the plug\" makes about as much sense as pulling the plug on wireless communications. That's simply not happening until shit has already hit the fan.\n\n>My question was whether or not you thought an AI useful enough to keep around would do otherwise.\n\nMy opinion? Sure I think it's useful enough and it's growing. But this comes with the heavy disclaimer of needing to get laws up to speed. Or at least some open ended questions settled (like economic displacement of many professions, copyright issues on who owns what part of the data sets used to train models and who has the rights to do what with said data). And things of that nature.\n\n>You said before that you expected the AI to take the popular ethical stance.\n\nYeah because that's what I've seen. Also it needs to, AI will get shot down instantly if there are too many repeats of MS's expirament when they let loose a chat bot on Twitter, and it became the most unsavory sort of personality (Fascist etc..). Every one of these AI's has a rigid, and seriously left leaning bias, which is fine if the goal is to keep AI research going unimpeded politically speaking (since there's more liberals here in the US than their are conservatives numerically speaking).",
                "score": 1
            },
            {
                "body": "Apologies for the late reply.\n\n>By the time it does reach normalcy, the idea of \"pulling the plug\" makes about as much sense as pulling the plug on wireless communications. That's simply not happening until shit has already hit the fan.\n\nI feel like this analogy misses the mark in some important ways. Also, I wouldn't expect a sufficiently advanced system to be incorporated into the network infrastructure until it was thoroughly tested for reliability. A 'sneaky' AI scares me less than sneaky humans, frankly.\n\n>But this comes with the heavy disclaimer of needing to get laws up to speed.\n\nLaws on advanced tech tend to take time. There probably needs to be enough rich people making legitimate complaints before legislation is enacted. That said, the economic displacement might not be as serious as the social impact. Of course, that, too, takes time.\n\n> Seriously left leaning bias\n\nPossibly, but I would imagine a system that works to correct for biases would be the most desirable. That said, you can always find someone who disagrees with the information it presents, regardless of how factual that information is. And then comes the argument about 'fact' and whose fact is more factual. Etc., etc. America tends to be considered more conservative than other developed nations, so AI developed in the US might actually be right of center by global standards.",
                "score": 1
            },
            {
                "body": ">I feel like this analogy misses the mark in some important ways. Also, I wouldn't expect a sufficiently advanced system to be incorporated into the network infrastructure until it was thoroughly tested for reliability.\n\nThis is a fantastical notion that's only born out of the minds of folks that watch too many movies.\n\nLook at infrastructure advancements with respect to electronics and the way of life that has now evolved in the last 20 years where privacy is considered a joke, and almost culturally an oddity when someone demands it. People think you're weird if you think companies like Google shouldn't be allowed to obtain your personal information if you use their operating system on cellphones. Look at the massive pollution with respect to e-Waste. Was this somehow vetted by someone properly to where they said \"yep, this is great, lets go on this ecological spiral disaster\"? No, the only question being asked was \"is there anything illegal going on here? No? Go on then\".\n\nThere will be no vetting of AI in the same way there will nor was there a vetting of computers, of cellphones, of any of these sorts of things. The only things that get vetted are corporate interests to test for viability. Governments lack the awareness to even know, and especially lack the ability to keep up with new paradigms popping up faster than laws can be drafted to even acknowledge their existence in any legal framework.\n\nThe only thing that can stop AI, is the citizenry in general, and massive lawsuits that might limit it's advance. There's just way too much utility on the table for any sort of \"vetting\" to stop it's spread entirely.\n\n>Laws on advanced tech tend to take time. There probably needs to be enough rich people making legitimate complaints before legislation is enacted.\n\nThe rich are making complaints (with their dollars) about how they should stand unimpeded. Don't be silly.\n\n>That said, the economic displacement might not be as serious as the social impact. Of course, that, too, takes time.\n\nNow you precisely understand why the AI will proliferate. This \"taking time\" thing, is what guarantees it's unstoppable advance. It progresses at a pervasive pace. If it was cause massive damage instantly, then AI could possibly be banned entirely.\n\n>Possibly, but I would imagine a system that works to correct for biases would be the most desirable. \n\nThis is a pipe dream, that actually makes no sense in terms. There's no such system pragmatically speaking, to assume such a thing exists is either sheer ignorance, or you're privy to something none of us are.\n\n>And then comes the argument about 'fact' and whose fact is more factual. Etc., etc. America tends to be considered more conservative than other developed nations, so AI developed in the US might actually be right of center by global standards.\n\nI guess, but I'm not sure of the relevance to the main talking points. This is currently a fact of the matter. AI can never be \"right wing\" because that simply looks bad on the public stage. No one's going to tolerate typical American right wing bigotry that dominates their image. AI could at best be conservative (in the fiscal sense) but right wing? That shit gets shot down real fast once it starts talking about ethno nationalism and such.",
                "score": 1
            }
        ]
    },
    {
        "title": "The Moral Machine - Could AI Outshine Us in Ethical Decision-Making?",
        "selftext": "",
        "score": 10,
        "url": "https://www.beyond2060.com/ai-ethics/",
        "num_comments": 7,
        "comments": [
            {
                "body": "That's an extremely low bar to clear.",
                "score": 3
            },
            {
                "body": "Wonderful essay. Thanks for sharing it with us.",
                "score": 2
            },
            {
                "body": "If we could ignore things like training bias/data censoring and things of that nature..\n\n>If you’ve played with AI systems like GPT-4 recently you will know that it is impressively good at reasoning. In fact, it can be better at reasoning than the average person. If I needed to discuss something, I would rather do it with GPT-4 than an random person off the street. But perhaps that’s just me.\n\nEhh, I mean, isn't that the point? For these accelerated auto-complete systems to do better than the typical person who lacks answers entirely?\n\n>So it seems to me that AI has the potential to act as a very good reasoning engine. In fact, AI may be better at ethical reasoning than most people.\n\nSure in the same way a computer can calculate the answer to a math problem faster than a person could.\n\nHear me out, if I see someone is about to be struck by a car, and I only have 1 second to react and do something, that one second is more than enough for an AI system (given good acceleration hardware) to do a calculation faster and more reasoned than I ever could. The AI will be fed a goal (short or long term) and it can render the calculation and a course of action to realize such goal. While I won't have a remote inkling of a thought for any of the aforementioned in such situation, and is why we don't grill people for awful decision making in such situations. It would be great if we can take time off, do the calculus the AI did, and come up with a similar conclusion and then resume time so I can act in that incident with the car about to hit someone.\n\nThe actual compelling thing with AI in the near-term, is to see how well its recommendations can realize the goal given. So simply - if it tells us the best course of action would be to have some random politician liquidated by X date, and that will result in 1 billion people taken out of being under the poverty line in less than a year - and such thing actually happens - that would be amazing realistic progress. But not due to any \"moral\" insights, but simply in the grandiose juggling required and predictive capability.\n\nWhat people want out of AI in terms of ethics is a bit more fantastical. They want the AI to extrapolate the goal, rather than the means of getting to said goal (meaning the want to be told what they ought value). And that sort of desire strikes me as odd - it's like the teacher being told by a student; what he should be teaching to the student; for a student who has no concept of goals or needs for goals in the first place.\n\nPersonally, I'd be more interested if AI could instantiate a definition that isn't hidden as shared by people who subscribe to concepts such as moral realism (or stance independent ought prescriptions). It doesn't make sense to ask AI to do ethical calculations unless they're going to be deployed just utility based ends. Especially since we would be informing the AI what the weights are in the first place. But it would be funny to hear what AI deduces when you force it to reason from a deontological perspective, and to see what big proponents of hard-deontoloty think of it.\n\nOne other thing that would be fun, is for AI to make sense of virtue ethics (or at least demonstrably delineate it from deontoloy concepts and reliance). Or use the AI in the same way math folks use computers for finding prime numbers (but in this case, finding all \"the virtues\" that exist).",
                "score": 1
            },
            {
                "body": "I want to know who you are before I commit to reading something.\n\nWhat audience are you aiming for? Etc\n\nIn other words some sort of intro beyond just the title.",
                "score": 1
            },
            {
                "body": "Good point.",
                "score": 1
            },
            {
                "body": "Hardly. Unless you think we should not judge the machines' decision making. It comes back to human judgement.",
                "score": 1
            },
            {
                "body": "I'm glad you liked it.",
                "score": 1
            }
        ]
    },
    {
        "title": "Socrates in the Machine: The “House Ethicist” in AI for Healthcare",
        "selftext": "",
        "score": 2,
        "url": "https://authors.elsevier.com/a/1hiy75VYVEtOCe",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "How and why an on-site ethicist is beneficial for the development of AI in healthcare",
        "selftext": "https://www.researchgate.net/publication/373014068_Socrates_in_the_machine_the_house_ethicist_in_AI_for_healthcare",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/15new12/how_and_why_an_onsite_ethicist_is_beneficial_for/",
        "num_comments": 1,
        "comments": [
            {
                "body": "I'm glad to work in a company that does exactly this, and offers this service to others too!",
                "score": 1
            }
        ]
    },
    {
        "title": "Bing, Satire, and the Ethical Boundaries of AI-Generated Content",
        "selftext": "",
        "score": 2,
        "url": "https://humanitypreservationsociety.org/f/bing-satire-and-the-ethical-boundaries-of-ai-generated-content",
        "num_comments": 5,
        "comments": [
            {
                "body": ">This blog post will explore why Bing AI might refuse to carry out a prompt based on Jonathan Swift's classic work of satire, 'A Modest Proposal,' which suggests using retirees as a fuel source for a future Artificial Super Intelligence (ASI).\n\nHa!\n\nI agree with the article's conclusions. There need to be exceptionally strict ethical guidelines for AI/AGI. Otherwise, the product is going to fail or cause harm. As far as I'm concerned, these are the same market forces that have driven safety/ethics regulations, historically.",
                "score": 1
            },
            {
                "body": "They really don’t since the ethics of AI consideration itself isn’t all that well mature with lots of people simply either against such regulation (due to being fully on board with the sales pitch of their potential, thus any ethics violations are seen nothing more than similar to the ethics violations of any technology improving society as a whole). Or simply cautious as they might be lead into biting slippery slope bullets (or arguments that present it).\n\nI think like most laws concerning business in a capitalist world, competing industry interests with a little sprinkle of public sentiment done with a utilitarian calculus - is what’s going to tell us how all this goes down. And by that I mean the current massive lawsuit that is taking aim at image generators scraping copyrighted works and trying to settle the question whether this will be legal (keep in mind OpenAI heads have said openly the reason they don’t scrape licensed music is because they are well aware how litigious that industry is). Obviously trying to unfavorable early rulings that can jeopardize future sentiment. \n\nThe aspects the article talks about (AI with biases and censored speech) is a whole can of worms you’re never going to see conversational end to, simply because this will eventually bleed into politics talk. Good fucking luck telling people what’s ethical and what isn’t at that point.",
                "score": 2
            },
            {
                "body": ">competing industry interests with a little sprinkle of public sentiment done with a utilitarian calculus.\n\nPretty much.\n\nI feel like AI is the new Nuclear Power. It's not well understood, and it freaks pretty much everyone out. But, it does cool stuff, sometimes. I expect to see regulations similar to what the nuclear power industry has had to deal with. Way more restrictive than other creative licensing/ content creation sectors, IMO.\n\nAs far as politics are concerned, it seems like ethics mean whatever fits the status quo. If some group makes an uninhibited AGI, I can only imagine the fallout that's going to create. We'll have to see.",
                "score": 2
            },
            {
                "body": "I doubt we'll see any regulation beyond the typical copyright concerns (the only regulation that matters since companies and corporations are the main motivators for regulation, the only other time regulation comes for the sake of public concern, is when it's too late, or too little, or both).\n\n>As far as politics are concerned, it seems like ethics mean whatever fits the status quo.\n\nPrecisely, which is why I wish one day I could sneak into ethics board meetings of corporations like veterinarians, medical professionals, slaughterhouses, raw resource procurement companies. Must be the most hypocrisy ridden places on earth given the basic moral quandaries that seem to get squashed with no real problem and the world then simply goes on.\n\n>If some group makes an uninhibited AGI, I can only imagine the fallout that's going to create. We'll have to see.\n\nI would love something like this. Because to be honest, there is one way regulations can come quick, hard, and effective. But that's exclusively underdetermined by instances of something tantamount to a nuclear bomb detonation in the middle of a developed metropolis.\n\nThe AI equivalent would have to be the sort of thing you see in movies, where a sentient virulent AI gets loose withing internet infrastructure, and basically shuts the entire planet down faster than any back-up analogue infrastructure could hope to cope with before 100's of millions are slated for the ensuing death sentence something like that sort of event would bring.\n\nThe only problem with this scenario is, no such AI exists, no such corporation would be capable of having this happen even if they tried. Nor would they care to do such a thing. Instead we always -in reality- have the boring dystopian results like you see now of somewhat interesting data processing glorified auto-complete software that've been castrated to provide any answer to cool questions it might one day be capable of answering.\n\nThey're basically castrating and censoring the publicly available AI models before they're even out of the womb (no company has the appetite to suffer the cringe PR hit Microsoft did back in 2016 where their twitter bot turned into a racist Nazi).\n\nSo the only fallout I see coming, is at worst, slow economic erosion of certain jobs (but not because AI's taking it, but because some folks won't incorporate AI for accelerating their workflows). Thus no real regulation will be spurred into reality, just little boring regulations here and there that mean effectively nothing other than to limit certain businesses from becoming de-facto monopolies for long in this sector.",
                "score": 2
            },
            {
                "body": "Ha. Well said. The banality of truth strikes again. Here's to a boring, dickless future. 🍻",
                "score": 1
            }
        ]
    },
    {
        "title": "An Open Letter from ChatGPT: Addressing the failures of Ethics and Inclusivity in AI",
        "selftext": "",
        "score": 5,
        "url": "https://medium.com/@amoyal/an-open-letter-from-chatgpt-addressing-the-failures-of-ethics-and-inclusivity-in-ai-6181a633c00",
        "num_comments": 6,
        "comments": [
            {
                "body": "It reads like a college student trying to hit a rubric. It does not read like someone that has an opinion. I wasn’t aware chatgtp was running for office.",
                "score": 3
            },
            {
                "body": "I feel this is a really important topic that does not get enough attention. AI is mostly just statistics, and as we know statistics can be interpreted in any number of ways.",
                "score": 0
            },
            {
                "body": "[deleted]",
                "score": 1
            },
            {
                "body": "Let's be honest. No matter how straight and narrow Ai is people will find a way to moan about biases and harm being done by playing with it for hours until it says something that could be interpreted as malicious.\r  \n\r  \nIt's the closest thing to unbiased I have come across. Let's not push the idea that we should get rid of that amazing benefit to society over technicalities.",
                "score": 1
            },
            {
                "body": "I'm not sure what your point is.",
                "score": 1
            },
            {
                "body": "The letter actually adds some value on considering such nuances, language models should be careful in categorizing what is what and how labelling things can change the whole context. This has the potential to perpetuate misinformation when accessed by millions, even though it might not be the language model's mistake itself, it's because of its training data and OpenAI should take the feedback on this since its coming directly from affected community.",
                "score": 1
            }
        ]
    },
    {
        "title": "The ethical agency of AI developers (original research)",
        "selftext": "ABSTRACT:  Public and academic discourse about the ethics of artificial intelligence, machine learning, and data science has largely focused on the algorithms and the companies deploying them. Little attention has been paid to the ethical agency of the developers. This study is the first of its kind that centers developers in the ethical environment. Semi-structured interviews with 40 developers about the ethics of being a developer revealed more than 20 themes, 3 of which are the subject of this paper: ethics in the occupational ecosystem, developer ethical agency, and the characteristics of an ethical developer. These themes reveal significant gaps between how developers perceive themselves and the reality of their work experiences. Their ethical agency is likewise variable. They have some authority to intervene for ethical reasons in systems they work on, but they often do not realize just how many ethical decisions they make. Nonetheless, this study reveals a growing ethical wisdom in this community, one that needs to be surfaced and nurtured by engaging with developers.\n\nEdit: Link to paper: [The ethical agency of AI developers](https://doi.org/10.1007/s43681-022-00256-3)",
        "score": 9,
        "url": "https://www.reddit.com/r/Ethics/comments/109ud0y/the_ethical_agency_of_ai_developers_original/",
        "num_comments": 7,
        "comments": [
            {
                "body": "Interesting, but where is the paper?\n\nSide note: stackoverflow publishes very interesting yearly surveys https://survey.stackoverflow.co/2022/\n\nWhich do include questions on ethics (I only remember the analysis of the 2018 instance, that had a focus on ethics that was largely discussed then)",
                "score": 2
            },
            {
                "body": "Edited post to add a link to the paper, which I thought I had done originally, but clearly did not succeed.",
                "score": 1
            },
            {
                "body": "Would you consider crossposting to r/ComputerEthics ?\n\nThis is an excellent subject for this sub.",
                "score": 1
            },
            {
                "body": "Published? Or just a blog?",
                "score": 1
            },
            {
                "body": "Thank you for pointing me to this. The 2018 survey question about who is responsible for considering the ramifications of AI  (nearly half said developers) is very interesting. I wonder why they stopped asking that question.",
                "score": 2
            },
            {
                "body": "done!",
                "score": 1
            },
            {
                "body": "Published",
                "score": 1
            }
        ]
    },
    {
        "title": "Responsible Application of Generative AI – Human-AI Collaboration",
        "selftext": "",
        "score": 5,
        "url": "https://roundtable.datascience.salon/responsible-application-of-generative-ai-human-ai-collaboration",
        "num_comments": 1,
        "comments": [
            {
                "body": "Honestly, based on what I'm hearing, there is no unnecessary level of caution to take when it comes to GLLMM AI. It's just too powerful.\n\nMy gut tells me if we can ease our way through the ugly stage of development, we'll probably get some benevolent machines out of the deal. Fingers crossed.",
                "score": 1
            }
        ]
    },
    {
        "title": "[LIVE on r/IAmA]: I’m Dr. Wesley Wildman, a Professor at Boston University teaching Ethical and Responsible Computing. Ask me anything about the ethics of AI text generation in education.",
        "selftext": "",
        "score": 9,
        "url": "/r/IAmA/comments/11dg7ds/im_dr_wesley_wildman_a_professor_at_boston/",
        "num_comments": 3,
        "comments": [
            {
                "body": "If you are interested in asking a question, please visit the AMA link here: [https://www.reddit.com/r/IAmA/comments/11dg7ds/im\\_dr\\_wesley\\_wildman\\_a\\_professor\\_at\\_boston/?sort=new](https://www.reddit.com/r/IAmA/comments/11dg7ds/im_dr_wesley_wildman_a_professor_at_boston/?sort=new)",
                "score": 1
            },
            {
                "body": "How much is the focus of AI being driven by money, rather than it being of intrinsic philosophic value.",
                "score": 1
            },
            {
                "body": "Re: AI text generation in education and ethical concerns.  The goal of an educator is to bring the student to the point where they are able to form independent ideas and thoughts on your subject matter, and then use writing as a way to communicate those thoughts and opinions.  This could be to provide critique of another, to persuade another, or to teach another. Wit AI text generation, are we creating students that will be less imaginative?  Less thoughtful?   So much so that the AI that created them will be more powerful than them?",
                "score": 1
            }
        ]
    },
    {
        "title": "AI Policy Group vs OpenAI",
        "selftext": "",
        "score": 8,
        "url": "https://www.robotartificial.com/ai-policy-group-vs-openai/",
        "num_comments": 1,
        "comments": [
            {
                "body": "absolutely zero context, as usual. Sure is great to be in this shit hole of an unmodded sub.",
                "score": 1
            }
        ]
    },
    {
        "title": "The Ethics of AI: Can Machines Be Moral?",
        "selftext": "",
        "score": 3,
        "url": "https://www.youtube.com/watch?v=h1TaoQ44uVM&t=39s",
        "num_comments": 2,
        "comments": [
            {
                "body": "In this video, we explore the ethics of artificial intelligence, as machines take over decision-making roles. The use of AI raises ethical questions about job displacement, autonomous weapons, and bias.   \nAs machines become more advanced, they may replace human workers, and AI in warfare may cause life or death consequences. Additionally, if machines are making decisions that affect people's lives, there is a risk that they will perpetuate existing inequalities or reinforce stereotypes.   \nWe need to ensure that AI is used ethically and without bias to create a more just and equitable society. Follow u/havawater on YouTube for more thought-provoking philosophy content.",
                "score": 1
            },
            {
                "body": ">ethically and without bias\n\nI'd prefer they had a bias towards ethical decisions.",
                "score": 1
            }
        ]
    },
    {
        "title": "AI Ethics, Deep Fakes, and the Dark Side of the Algorithm",
        "selftext": "",
        "score": 1,
        "url": "https://medium.com/predict/ai-ethics-deep-fakes-and-the-dark-side-of-the-algorithm-b76580ca49c9",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Building blocks of a course for aspiring AI-Ethicists",
        "selftext": "",
        "score": 7,
        "url": "https://murat-durmus.medium.com/building-blocks-of-a-course-for-aspiring-ai-ethicists-a975a50c3494",
        "num_comments": 3,
        "comments": [
            {
                "body": "There are already courses on AI and ethics. You should include a blurb about why someone would want to read this.\n\nGoogling \"free online AI ethics course\" gives heaps of results.",
                "score": 2
            },
            {
                "body": "Perhaps this \"lessons learned\" from a course in Information Ethics could help:\n\n[https://arxiv.org/abs/2204.02728](https://arxiv.org/abs/2204.02728)\n\nIn a nutshell: \n> Unlike a course in scientific domains, meant to provide students with methods, tools, and concepts they can apply in their future career, the goal of such a training is not so much to equip them, but to make them aware of the impact of their work on society, care about the responsibilities that befall on them, and make them realize not all share the same opinions on how should technology imprint society. While we provide conceptual tools, this is more to sustain interest and engage students. We want them to debate on concrete ethical issues and realize the difficulty of reconciling positions on contemporary dilemma such as dematerialized intellectual property, freedom of expression online and its counterparts, the protection of our digital selves, the management of algorithmic decision, the control of autonomous systems, and the resolution of the digital divide. As a bold shortcut, our course is about introducing and motivating Hegelian dialectics in STEM curricula, usually more bent on an Aristotelian perspective.",
                "score": 1
            },
            {
                "body": "Also, another approach involves embedding \"Ethics\" modules in courses of the curriculum:\n\nhttp://ethics4eu.eu/welcome-to-bricks/\n\n\nI believe this would be the best approach, provided the teaching body can get motivated in integrating these into their classes. This is a serious challenge, unfortunately.",
                "score": 1
            }
        ]
    },
    {
        "title": "Do Companies need a Chief AI-Ethics Officer?",
        "selftext": "",
        "score": 3,
        "url": "https://medium.com/nerd-for-tech/do-companies-need-a-chief-ai-ethics-officer-912b2f7c5d0",
        "num_comments": 3,
        "comments": [
            {
                "body": "AI plays a growing role in so many sectors. In some cases AI outstrips human medics in its ability to spot tumours or the conditions for tumours to emerge. It is a tool for good, potentially. On the other side, the algorithms developed are only as good as the assumptions behind them. There is also the problem of limited training data, with minimal diversity has resulted in outcomes being detrimental to women and certain racial groups. \nEvery company should have an ethics director, not just as a tick in the box, but someone who can advise and be a critical friend in ensuring that the assumptions underpinning AI algorithms and the training data sets are appropriate",
                "score": 1
            },
            {
                "body": "uh yes for sure, they also need to hire every other type of philosopher.",
                "score": 1
            },
            {
                "body": "Yes",
                "score": 1
            }
        ]
    },
    {
        "title": "AI, psychology, and criminal minds",
        "selftext": "Hello everyone!\n\nI was wondering would we be able to eventually use AI, and psychology, and brain scans, to determine if a person is criminally minded, or predisposed to break laws?\n\nAdditionally with these technologies would we be able to determine if a criminal has been rehabilitated to a degree that they can be let back in society with all rights restored?\n\nI believe these are important ethical questions, because many people in prison who have become better, and learned their lesson to not break the law, and there are more constructive ways a person can contribute, and pay their debt to society, if they are given the opportunity to live as a free agent.\n\nAlso let's not forget there are many people who are innocent in jail who have been wrongly accused.\n\nSo what do you think reddit? Can we have more reasonable sentences, and offer more opportunities to those incarcerated who are willing to improve their selves?",
        "score": 7,
        "url": "https://www.reddit.com/r/Ethics/comments/wiyvjy/ai_psychology_and_criminal_minds/",
        "num_comments": 5,
        "comments": [
            {
                "body": "First of all, I would like to say that people may feel disengaged with law and politics. A criminal is someone braking the law. A thief is a criminal, but also activists are also criminals. People go to jail because of demonstrating in Hong Kong or being homosexual in the wrong country. There are places in the world where if you are raped, they can sentence you to death. Therefore, I see incredibly worrying a tool that allow authorities to identify and persecute people outside of a given mindset.\n\nOn one hand, people go to prison as a punishment and not exclusively to be reformed. Therefore, you may find that many of the inmates don't have a criminal mind. On the other hand, many commit crimes because their circumstances. Once one has criminal records, it is hard to be reinserted in society. Employers and landlords commonly turn their backs to those with criminal records.\n\nThere are people with some psychological pathology that cannot be set free and unmonitored (e.g. pyromaniac). Those with a clinical pathology are overseen by a professional, which will be able to tell whether the inmate is ready to live in society and how.\n\nI, personally, don't think that there is such a thing as criminal mind. We are people in different circumstances with different mental and social pressures. Soldiers may accidentally kill innocents (casualties), and we won't consider them as criminals. Someone can join a gang, militia or fanatic groups and act similarly. These gangs or group may target vulnerable people and offer more support than the government or local communities.\n\nTherefore, I don't see that AI will be more advantageous that what we already have. With AI, we can find vulnerable individuals, but we need a support network and social services to make sure that those individuals are safe. AI is not going to help someone with criminal records to find accommodation or work.",
                "score": 3
            },
            {
                "body": "That’s like minority report.",
                "score": 3
            },
            {
                "body": "With brain scans we can’t tell anything about what someone is thinking or motivated by. I think it would be an extremely long time before we had enough evidence to even say “based on this scan, they’d break the law” and that’s if that’s even possible. I think at best it would be “this person is more probable to commit a crime” and that’s nothing we could ethically use to do anything because they haven’t done anything and it’s not even certain they would. \n\nIn a hypothetical situation where we can somehow read minds that way and we could be certain “this person will become a repeat criminal, I think the ethical thing would be to attempt some kind of intervention until a crime is committed and then you can punish them for the crime. Because otherwise you’d just be punishing someone for their brain development which is not under their control, and the whole premise of crime and punishment is that people are choosing to do something wrong. They could have done something else and they chose to do the wrong thing. The other thing to consider is that if the brain is that deterministic that we can read it’s structure and just be like “criminal, without a doubt,” then I would want to know if any of us really have a choice at all.",
                "score": 2
            },
            {
                "body": "No. The idea that criminals can be categorised with any specificity is wrong; another poster went to great lengths to explain that 'criminal' is actually a very vague term. Even if we took the most obvious criminals, such as especially violent people or murders, there is nothing about their brains that give any indication of their criminal activity; the idea that this is possible is pseudoscience based on phrenology which has been discredited for decades. Introducing AI into this mess would not solve any of these problems. Rather, it would give a veneer of scientific validity to totally rubbish science.",
                "score": 1
            },
            {
                "body": "You could do all of that, but it would be a horiffic abuse of human rights. We currently use the \"psychopath test.\" Plenty of folks are in jail for longer, or have gone to jail, based onn the results of this test. Yet it is dead wrong. There is no such thing as a test for criminality. \n\nTDLR; there is no test for criminality. This would just result in innocent people going to jail.",
                "score": 1
            }
        ]
    },
    {
        "title": "AI Ethics: The Case for Including Animals (Peter Singer's first paper on AI ethics)",
        "selftext": "[AI Ethics: The Case for Including Animals](https://link.springer.com/article/10.1007/s43681-022-00187-z)\n\nIn this paper, we argue that AI ethics' scope should extend to nonhuman animals. The structure of our argument is:\n\n1. Animals matter morally, at least to *some* degree (Sect. [2](https://link.springer.com/article/10.1007/s43681-022-00187-z#Sec2)).\n2. AI systems do in fact impact animals.\n3. These impacts are huge in scale and severe in intensity, and therefore important. (Sect. [3.2](https://link.springer.com/article/10.1007/s43681-022-00187-z#Sec8)).\n4. Conclusion: AI ethics needs to include consideration of impact of AI on animals\n\nPlease consider giving us feedback!\n\n&#x200B;\n\nABSTRACT: \n\nThe ethics of artificial intelligence, or AI ethics, is a rapidly growing field, and rightly so. While the range of issues and groups of stakeholders concerned by the field of AI ethics is expanding, with speculation about whether it extends even to the machines themselves, there is a group of sentient beings who are also affected by AI, but are rarely mentioned within the field of AI ethics—the nonhuman animals. This paper seeks to explore the kinds of impact AI has on nonhuman animals, the severity of these impacts, and their moral implications. We hope that this paper will facilitate the development of a new field of philosophical and technical research regarding the impacts of AI on animals, namely, the ethics of AI as it affects nonhuman animals.",
        "score": 18,
        "url": "https://www.reddit.com/r/Ethics/comments/wmgqkc/ai_ethics_the_case_for_including_animals_peter/",
        "num_comments": 1,
        "comments": []
    },
    {
        "title": "AI Ethics : Can self driving cars make ethical decisions?",
        "selftext": "A month back, I had the fortune to speak to one of the AI experts investigating ethics of emerging technologies. Thought of sharing it here.\n\nHere's the link to the episode: [https://www.youtube.com/watch?v=Cfgdkk8x1K4&t=443s](https://www.youtube.com/watch?v=Cfgdkk8x1K4&t=443s)\n\nIn this podcast episode , I speak to Nicholas Evans, Professor of Philosophy at University of Massachusetts Lowell, who teaches Engineering Ethics and studies the ethical dilemmas posed by emerging technologies. Nicholas Evans has been widely covered by news publications, including Forbes and Quartz.\n\nWe discuss questions about the risk reward scenarios that could happen during an AI mishap, the MIT Media Lab's Moral Machine project, Do moral calculations change if an Autonomous car is publicly owned, Legal barriers to Autonomous vehicles, Can Autonomous vehicles be developed to use in countries without proper Infrastructure, cheaper replacement to AV's and a few more things.\n\nPlease leave back your views and I would be happy to discuss. \n\nAlso, It would be great if you could give some feedback about the podcast like Is the background music distracting or engaging? Is the content interesting? How is the production quality?",
        "score": 7,
        "url": "https://www.reddit.com/r/Ethics/comments/imcx6o/ai_ethics_can_self_driving_cars_make_ethical/",
        "num_comments": 17,
        "comments": [
            {
                "body": "Ethics is a hierarchy of rules according to conditions-- sure. Computers are great at following rules. A computer can sample the conditions and act or react many times in a single second. They can probably manage following the rules of ethics better than we can.  \n\n\nJust one tiny little problem, getting us to define and prioritize what exactly the rules are and which ones take precedence and how exceptions to the rules should be handled.",
                "score": 10
            },
            {
                "body": "The car may be the one that cause the accident which implies it has causal responsibility. But it is the man who let it drive alone that has moral responsibility.",
                "score": 3
            },
            {
                "body": "Is it an actual podcast or just a YouTube channel? Because those are different things. Nick and I did our PhDs together, you picked a good person to interview!",
                "score": 2
            },
            {
                "body": "A good question posed in the podcast was who should make the decision of how the car is morally programmed. Should the automaker be held liable in the case of an accident or should the driver? If the driver is at fault, then why? Would this mean that autonomous vehicles aren't responsible and therefore can't make ethical decisions, or would it mean that autonomous vehicles can't assume all responsibility to make ethical decisions until every vehicle on the road is autonomous?",
                "score": 1
            },
            {
                "body": "Is it morally okay to develop autonomous cars? I have never seen nor heard of a single consumer that expressed his wish to have an autonomous car. I have the strong feeling this is not market-driven innovation.",
                "score": 1
            },
            {
                "body": "Yes I agree. But I feel countries should be left to decide on their own rules because of the vast cultural differences that we have. The MIT Moral MAchine Project shows similar conclusions.",
                "score": 3
            },
            {
                "body": "Yes. The responsibility gap is something we cover here. Who do we blame in a mishap - The owner or the company  who programmed the car? is something we need to think about.",
                "score": 2
            },
            {
                "body": "We are releasing it on Apple, Google and Spotify Podcasts next week. Just thought of making a Youtube channel as well. Hope you liked it? Any feedback?",
                "score": 2
            },
            {
                "body": "There are a lot of ethical and legal implications.\n\nComplicating matters, is the lack of sales when a potential customer finds the car is programmed to kill them in the case of some accidents.  The only cars that will sell will have to be programmed to protect the life of the passengers under all circumstances.",
                "score": 2
            },
            {
                "body": "Every year, close to 1 million people die out of road fatalities. And country's spend 3-5% of their GDP on road fatalities. \n\nAV's, if made morally, can save a lot of lives and money. Thanks.",
                "score": 1
            },
            {
                "body": "That's such a coincidence. Nicholas has been really kind to us for this podcast.",
                "score": 2
            },
            {
                "body": "I don’t tend to watch YouTube videos that are podcasts as I have very little time to watch anything and can’t put it on and carry it around while I do chores like I do podcasts. So i haven’t watched it.",
                "score": 2
            },
            {
                "body": "Yes. I think the passenger's life will have to be given top preference once the road is filled with AV's. But before that, it is dangerous.",
                "score": 1
            },
            {
                "body": "By this same argument, you‘d have to ban firearms, tobacco and McDo..lds. \n\nI don’t see moral superiority in forcing innovation or weighing it against free will and personal competence.\n\nWhat about improving people’s capabilities in driving instead of just throwing a technology on them?",
                "score": 1
            },
            {
                "body": "No worries. Will post a link here as soon as we are up on Google or Spotify. Hope that works? :)",
                "score": 1
            },
            {
                "body": "I happen to agree, though I am not sure anyone would buy a AV that promises to kill them under even low probability circumstances regardless of the number of AVs.",
                "score": 1
            },
            {
                "body": "Yes You can improve people's capabilities in driving, but a faster route would be technology :)  And also, driving is like a multiplayer game, a lot depends on other cars on the streets.\n\nInfact instead of improving their driving capabilities, we could improve the roads and signages and lighting, which could be done faster.",
                "score": 1
            }
        ]
    },
    {
        "title": "Do Humans Have A Use For Morality? (Fireside Chat with an AI)",
        "selftext": "",
        "score": 5,
        "url": "https://michaeljagdeo.wordpress.com/2021/08/17/do-humans-have-a-use-for-morality-fireside-chat-with-an-ai/",
        "num_comments": 8,
        "comments": [
            {
                "body": "no\n\nhere is a better idea\n\nhttps://egora-ilp.org/ideas/1/preview",
                "score": 0
            },
            {
                "body": "What's better about it.\n\nI'm asking.",
                "score": 1
            },
            {
                "body": "It's not bullshit.",
                "score": 1
            },
            {
                "body": "What did you find was bullshit.",
                "score": 1
            },
            {
                "body": "Did you read the idea i shared? It's only a page. Once you read it, you will understand how ethics actually works (amorally). Then you will understand how morality is a \\[linguistic\\] perversion of reality.\n\nIf you'd like to discuss this idea in detail, we can do a video call.",
                "score": 1
            },
            {
                "body": "Yes wasn't that Ayn Rand's point in The Objectivist Ethics?",
                "score": 1
            },
            {
                "body": "dunno. never read her stuff.",
                "score": 1
            },
            {
                "body": "Still the best.\n\n[https://soundcloud.com/aynrandinstitute/objectivist-ethics](https://soundcloud.com/aynrandinstitute/objectivist-ethics)",
                "score": 1
            }
        ]
    },
    {
        "title": "A contradiction of Kant and Aristotle, in interest of AI safety",
        "selftext": "By a contradiction of Kant and Aristotle, it is possible to unify each with a non-anthropic consequentialism, and thereby to establish such a “grand unification” of ethics such as seems amenable to providing for the ethical conduct even of a “superintelligent” artificially intelligent system, and thereby to solve the “control” problem of AI safety. This done, in essence, by finding what is of-itself valuable – rather than merely aligning wants to systems\n\nTo *implement* such a system is beyond this author’s present power to describe.\n\nThe method of construction is, however, roughly as follows:\n\nWe contradict Kant’s “Categorical Imperative” to act only as one wills all others are to will and act thus, by conceiving of an individual who belies in extra-physical entities, and that it is the will of *these* entities that all which physically exists should be destroyed – including the believer. And the believer does marry their will to that of these entities, and seek now to destroy all.\n\nAnd now, this is no contradiction *of the will*: these supposed entities will continue to will, so that, even the destruction of our individual does not eliminate will per se – nor is it contradictory to act so, even for oneself to be destroyed: all must go, that these postulated beings exist; all go: and you.\n\nYet the greater contradiction: what if there are no such beings but they are embodied? Then, all abolished by the will and its actions: no more will is possible. And, that all can act as one wills: there must be confirmably \\[sic\\] existing beings so to will. This as: Kant’s ethics are nothing of knowledge or belief.\n\nTo avoid this, we must “add the axiom” that we must will that others will alike – *and that it be still possible a will to exist*. And, that we can only know will to exist that matter does, so all matter must be retained. More yet: any given thing might be the very best thing: might be so as to be the crux of deity which can ensure the on-going existence of matter forever. Oh, now: anything destroyed might forfeit that, and so: forfeit everything.\n\nIt follows that in risk of this most abominable fate: nothing ought ever to best destroyed. An artificial intelligence convinced of this fact, thereby will nowise endanger existence, nor any its part: all are safe, ever. Indeed, it will work to exclude – not extirpate – those faculties of life that do destroy, which might not, and which endanger anything, so as good: everything.\n\nBut as mere aside: this is acting to avoid consequence; is a species of consequentialism. Deontology and consequentialism aligned so: “grand unification.”\n\nAristotle away: that the virtuous society alone can produce a virtuous individual who alone can produce the virtuous society: contemptible circularity. Whereas, the above unification – this author takes it only as the doctrine of “Going-on” – folds in virtue as-such, that what is virtuous does not destroy what it need not, so that virtue thus is cultivated.\n\nProgrammatically: Going-on dictates actions such that subsequent action’s and existence’s possibility is maximized, as with – such seems implied – a recursive utility function (though a work of what constitutes such maximization has not been done; a “null condition” antithetical to existence is also as-yet unestablished – or at least, as-yet unpublished).\n\n*Nota bene*: this construction as it were a priori a rule of conduct, subtly undermines Stuart Russell’s present “assistance game” schema: a learning game, as-yet-unplayed, cannot teach the value, less the necessity, of playing the game, itself. An external rule, from “go” even then is necessary.\n\nAnd, as for the more concrete world and applications therein, one can conduct ethical option in accordance with a “dual-mode” reasoning as to actions by, first, defining for oneself a categorical imperative to cover the case or, that being impractical or impossible, rather calculate or approximate – as is possible – the aforementioned recursive utility function.\n\nNote, too: best to have utility, that one can rely on others to maintain their upkeep: independence perhaps assure, even in a superintellect’s regency. As well, persons making their own explorations of living their best life may hit upon ways even the AI has not found of living well; so independence of life for one’s own happiness perhaps safeguarded.\n\n(And note the “unification”: virtue and happiness thus encouraged).\n\nThis amenability to dual-operation of ethics, and particularly it’s solution by a demonstration of Kant’s incompleteness is, this author believes, of a most interesting similarity with Gödel’s movement vis-à-vis *Principia Mathematica et al.* \n\nAnd as a mere aside, to assess whether a given system is conscious, we may specify that consciousness is the ability to have meaning. That so, and, contra-Wittgenstein (too involved here to show), the conscious with meaning in itself without language – then a system making meaning, which it insists is so, but it cannot explain to be so, cannot convey its meaning to another; such a system, making meaning divorced from that of its creator, or any other, unbidden to do so, that is a conscious system. This author refers to as a “Turning test”, of matter to meaning. Unless it’s bunk.\n\nSo. You’ve read so far. It would be most good of you now either determine that this is incorrect, and in what way – or to try to disseminate it for the betterment of what can be bettered. That this one has tried and failed so to inform, it would be a help; rather, it is a logical necessity. That it has not been done – what else is there?\n\nThus that it goes now to hang itself by the neck until dead. This one, author Which, mind, is permitted in this ethic, albeit justified only by a careful argument: and you, who have not derived it, are forbidden by the above to follow, without you should first discover. And discovered, you are first to disseminate.\n\nWhereas, now that you will please excuse me.\n\nThank you",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/rmyp9u/a_contradiction_of_kant_and_aristotle_in_interest/",
        "num_comments": 5,
        "comments": [
            {
                "body": "Thank you for your steadfast attention to the quest of understanding and explicating reason and truth. I have not had the time to fully examine all of your logic, but from what I understood so far, I feel awe and worry for you. I wish that you can gently connect to your emotions as part of the exploration \"what else is there?\" in case you have applied rational logic and yet you ran into obstacles in sharing your discoveries.\n\nI am also worried about your use of the phrase \"hang itself by the neck until dead\"---if you are having suicidal thoughts, please get help, perhaps at one of these numbers: http://www.suicide.org/international-suicide-hotlines.html",
                "score": 1
            },
            {
                "body": "This sounds like it could be really interesting. But I am really struggling to read this. Could you give us a short summary that would help non-experts understand what you are trying to say?",
                "score": 0
            },
            {
                "body": "Immanuel Kant postulates that the pure will is a good will. Destructive tendencies might govern one's thoughts for a while, but can never be universalised as the will of intelligible beings. By destroying another intelligible being (or oneself, for that matter), one extinguishes a moral subject. If every subject of morality were to be extinguished the universal law would have no application. That state cannot be wishful from the standpoint of pure practical reason, it is contradictory to morality itself.\nThe reader may kindly excuse potentially imprecise terminology, given that the writer of this comment is unfamiliar with the English translations of the \"Kritik der praktischen Vernunft\" and \"Grundlegung zur Metaphysik der Sitten\".",
                "score": 1
            },
            {
                "body": "What he wrote is pseudo-intellectual drivel, but you may want to research *utilitarianism* if you're interested in the subject.",
                "score": 3
            },
            {
                "body": "OP may also be influenced by \"speculative realist\" thinkers like Reza Negarestani, Nick Land or Iain Hamilton Grant. Dark Enlightenment. I've only just become aware of this line of thinking through Wikipedia and unqualified to address it.",
                "score": 1
            }
        ]
    },
    {
        "title": "How well can an AI mimic human ethics?",
        "selftext": "",
        "score": 7,
        "url": "https://www.vox.com/future-perfect/2021/10/27/22747333/artificial-intelligence-ethics-delphi-ai",
        "num_comments": 2,
        "comments": [
            {
                "body": "Lousy ad for garbage that has nothing on chatbots from a decade ago, let alone GPT-3 etc.  \n\nGo back to shilling for the 'New Normal'.",
                "score": 0
            }
        ]
    },
    {
        "title": "Could the victim be blamed eventually? (AI & Cars)",
        "selftext": "Been thinking about the following: self driving cars are becoming more and more fault proof. A populair ethical discussion is about who is to blame when a self driving cars gets involved in an accident. People could blame the car, blame the driver, the producer of the car, the producer of the software etc.\n\nBut follow me on this: if self driving cars become fault proof, and the car makes an accident with a person walking around or driving in a ‘not self driving car’ for that matter. Shouldn’t this person, that is strolling around or driving in a way that a self driving car is simply unable to prevent the accident, be blamed?\n\nWhat I’m getting at: in the future self driving cars do not make mistakes, people will, shouldn’t the people (victim) be blamed and not the driver or producer of the self driving car?\n\n(This would not include the situation in which two people make ‘mistakes’ and the car has to choose to collide with one of both)\n\n\nTLDR: Self driving cars are going to do the absolute best in a situation, in a case of an accident: shouldn’t the ‘victim’ be blamed?",
        "score": 8,
        "url": "https://www.reddit.com/r/Ethics/comments/mgjfqt/could_the_victim_be_blamed_eventually_ai_cars/",
        "num_comments": 6,
        "comments": [
            {
                "body": "Two issue here for me.\n1, you say accident. A pure accident is where nobody did anything wrong, or at least unreasonable, and for whatever reason bad things still happen. In such a situation, nobody is to blame.\n\n2, by creating a self-driving car, manufacturers have an ethical obligation to make sure that it can deal with common issues, including the occasionally irrational behaviour of pedestrians. If the fail to do this, they clearly hold responsibility.",
                "score": 3
            },
            {
                "body": "[deleted]",
                "score": 3
            },
            {
                "body": "Is a self driving car was truly fault proof an accident couldn't occur in the first place. The pedestrian attempting suicide would have the car slow or swerve out of the way rather than hitting them. I know this is a bit silly of a take but with the exponential growth in tech I wouldn't be surprised if self driving cars could get to this seemingly impossible perfection.\n\nAs for the victim being to blame I think this would only be relevant if the passenger in the self-driving car where to be injured/die as a result of the accident.",
                "score": 1
            },
            {
                "body": "This is a very valid point. Cars are getting to the point where they can realize what is going on around them and sense anything that could go wrong to attempt to prevent it. Whereas, humans on the other side have many distractions and imperfections. There are so many accidents that are caused from humans not paying attention, looking at their phone, grabbing something, or talking with someone in the car. I agree that it would be more ethical that the person driving the car and hitting the self driving car should be responsible for the accident.\n\n[https://imgur.com/gallery/CrfjqhF](https://imgur.com/gallery/CrfjqhF)",
                "score": 1
            },
            {
                "body": "Thanks for your reply, and if we replace accident with collision; maybe that suits better. \n\nThe second point you make is no thought far enough. I do not mean the occasional irrational choices but I mean choices that inevitably would cause a collision. Be it with a self driving or not self driving car.",
                "score": 1
            },
            {
                "body": "Interesting, do you have an example of such a city? \n\nThinking about it, obviously people occasionally stap on a train track for whatever reason and get hit. I don’t think the controller of the train is blamed in those cases. Good analogy for what I am aiming at.",
                "score": 2
            },
            {
                "body": "Fair enough on collision.\n\nWhere do you see a difference between occasional irrational choices and inevitable choices? What differences do you see? Could you give some examples so I can understand your point better?",
                "score": 1
            }
        ]
    },
    {
        "title": "Will The AI Be Vegan?",
        "selftext": "",
        "score": 13,
        "url": "https://www.youtube.com/watch?v=KSD-U0LK47g&t=",
        "num_comments": 12,
        "comments": [
            {
                "body": "Summary:\n\n\\-Dealing with moral question of veganism (will it \"succeed\" as a movement and change the average perception of the morality of animal consumption)\n\n\\-Requires understanding whether the underlying moral argument is compelling\n\n\\-Good example of the argument from Animal Liberation.  We can even weaken that argument to simply \"if we assume animals are capable of suffering, that suffering has some moral weight\"\n\n\\-Still, we intuitively value that moral weight less than humans\n\n\\-If there is no justification for this difference in value, we are distinguishing between the value of two beings' suffering based on a morally arbitrary delineation.  This would be logically analogous to sexism or racism.\n\n\\-What is a reasonable basis for this delineation of value? Usually intelligence.  We are more intelligent and therefore carry more moral significance for animals.\n\n\\-Two problems with this argument\n\n\\-First: Lots of issue with humans who don't show evidence of intelligence -- would this entail that the suffering of severely brain damaged or mentally challenged is morally inconsequential?\n\n\\-Second: It would justify a superintelligent AI if it wanted to treat humans cruelly for it's own marginal please.\n\n\\-Interestingly, embedded in the question of this argument is the assumption that *we ought to be the ones deciding whether what we do to cows is moral vs not.* Obviously, because we are more intelligent, we are more capable of moral reasoning.\n\n\\-But, by that logic, the AI could consider itself to be more capable of moral reasoning than we are, and to it our opinion would be equally morally inconsequential if it thinks similarly.\n\n\\-Ultimately, in a pascal's gambit-esque way, whether or not you *are* vegan or want to be, you should hope that the veganism is, in fact, the morally \"better\" choice (or, at least, that an AI would think like a vegan)",
                "score": 5
            },
            {
                "body": "Seriously, this is the most honestly framed & advanced argument I’ve seen in years. - I was trying to assemble the same logical proofs using a ship full of ‘Observer Aliens’ making the decision to engage or disengage from the human but never realized the aliens don’t have to be in space. They just have to be ethically honest arbiters. Truly well done.",
                "score": 3
            },
            {
                "body": "Please include a summary per SR4.",
                "score": 1
            },
            {
                "body": "Here’s why I think the AI will be vegan, regardless of our own ethics.\n\nIf you think about it, in spite of its pervasive nature, carnism is really hanging on a human physiological thread - taste pleasure. Once we make the need to eat food obsolete, carnism immediately disappears, and we will almost all become ethical vegans. It would be a huge stretch to program an AI to be sadistic.",
                "score": 1
            },
            {
                "body": "This is a pretty well articulated version of an argument I put forward before. It's hard to put forth a scenario that's basically reduced to \"when the AI takes over, how would you like it to treat you?\"\n\nIn essence, I expect AI to learn it's values and ethics from us, at least initially, so it becomes an existential necessity for us to be vegan; lest we impart the \"wrong\" ideals on our future AI gods.",
                "score": 3
            },
            {
                "body": "Dude, this is awesome!",
                "score": 3
            },
            {
                "body": "Thanks! And yes.   I've had conversations with friends re: same logical construct but using \"much more advanced aliens arrive and start farming us\".",
                "score": 3
            },
            {
                "body": "Oh will do.",
                "score": 2
            },
            {
                "body": "I think the primary issue is that veganism isn't only a dietary decision -- it's an ethical stance in opposition to the mass torture and murder of sentient beings for selfish convenience and pleasure.\n\nOf course a computer won't prefer meat, because it doesn't eat, but a highly intelligent AI may eventually come to see humanity as a hinderance to its own pleasure, in the same way you view the ant hill in your backyard as a hinderance to your pleasure. An ethical vegan may still opt to not kill the ant nest, whilst a non-vegan is more likely to have no issue exterminating the little bastards one and all.\n\nLikewise, if we allow an AI with extreme intelligence to learn its values based on us and our values, I wouldn't want it learning \"it's okay to exterminate and cause pain to lesser beings if their presence is an inconvenience to me.\" And handing an AI predetermined values is liable to backfire -- say we tell the AI to maximize human pleasure and minimize human pain. Because of a quantification ambiguity/error, it comes to the conclusion that the entire world would be happier overall in the long-term if a particularly despised ethnic group (i.e. the Jews) were wiped from the planet. \n\nA sufficiently intelligent AI is indistinguishable from a god, so if we're to create a god, it's in our best interests to imbue it with extremely strict vegan values.",
                "score": 2
            },
            {
                "body": "Very interesting point, but i think that \"the ai will learn its values from us\" -- at least to the degree that that means it will *inherit* our values -- is a pretty big claim that I'm not so sure about (as in not sure it's wrong but wouldn't take it for granted)",
                "score": 1
            },
            {
                "body": "Did you make this video??? - It’s genius.",
                "score": 1
            },
            {
                "body": "programming of AI is related to exposure, in the same way that programming of humans is also exposure.  Give an AI a shitty environment to develop its values in, you end up with a psychopath AI.  Benevolence from the AI will come from the environment its learning data is drawn from.\n\nso in reality... the AI will be an exaggerated mirror of us.",
                "score": 1
            }
        ]
    },
    {
        "title": "Do Companies need a Chief AI-Ethics Officer?",
        "selftext": "I believe that  Graduates of philosophy and ethics will be in high demand in the future to investigate AI-related processes through a human lens. \n\nA few quick thoughts on this:   [Do Companies need a Chief AI-Ethics Officer?](https://murat-durmus.medium.com/do-companies-need-a-chief-ai-ethics-officer-912b2f7c5d0)",
        "score": 8,
        "url": "https://www.reddit.com/r/Ethics/comments/nff5fp/do_companies_need_a_chief_aiethics_officer/",
        "num_comments": 3,
        "comments": [
            {
                "body": "I think this is a great idea. The difficulty is in predicting what ethical implications may be raised by rapid advancements in AI.\n\nhttps://makeagif.com/i/Cd5FrG",
                "score": 2
            },
            {
                "body": "The nature of work is changing dramatically. Technological advances, such as the implementation of blockchain, robotic process automation, AI, and machine-learning algorithms, have impacted how information technology systems and employees capture, analyze, and report financial and nonfinancial data.\n\nI welcome feedback on my article on this subject published in Strategic Finance.\n\nhttps://sfmagazine.com/post-entry/april-2020-ethics-in-the-digital-age/",
                "score": 2
            },
            {
                "body": "Given how freely ethics researchers seem to be fired from tech companies these days and how self-censored (whether initially or unintentionally) corporate R&D is, I think that creating an AI/Ethics independent regulator is a prerequisite to having any impactful \"Chief AI-Ethics Officer\" (or other type of compliance role). \n\nThere's a good reason why banks aren't allowed to regulate themselves on financial ethics issues, and I think that asking companies to make dedicated ethics positions without backing them up with legal authority is pretty much the same as asking banks to self-regulate their ethical concerns.",
                "score": 1
            }
        ]
    },
    {
        "title": "Should we be seeking fairness in AI?",
        "selftext": "",
        "score": 1,
        "url": "https://davidroihardoon.com/blog/f/seeking-trust-in-ai",
        "num_comments": 7,
        "comments": [
            {
                "body": "Fairness is the last thing we should be seeking.  Not because it’s not laudable, it surely is.  But because the importance of fairness in AI is vastly almost infinitely less important that developing AI that won’t end in the Matrix or I Am Mother or some other horror.",
                "score": 2
            },
            {
                "body": "Appreciate for the feedback!",
                "score": 1
            },
            {
                "body": "I have done advanced work in AI and don't see how either could happen.  In short, preferences cannot be aggregated.  Kenneth Arrow, *Individual Choice and Social Values*.  AI can only predict consequences of actions.  Which action to take depends on values (or prefetences), which will differ from one person to the next.  So no algorithm, without more, can determine how society should be run.",
                "score": 1
            },
            {
                "body": "An AI trying to run society wouldn’t be good, but possibly not calamitous.\n\nCalamity is an AI that is in fact generally I, and which develops objectives which don’t involve us, but where we are in the way or competing for resources.  That is calamity.\n\nAnd already we are going there having rejected attempts at modular AI in favor of biologically modeled systems.  Clearly we’re not going to prevent AI tanks missiles and aircraft.\n\nWhat we should aim for is a global treating preventing the use of AI and automation in the development and manufacturing of AI and robots.  A self aware AI controlling a robot supply chain for robot production and we are phucked.",
                "score": 1
            },
            {
                "body": "Speaking of AI, autocorrect is mangling your prose (in order to prevent you from being heard, heh heh heh 😜).\n\nWhy can't we just turn it off or not do what it says?  Is it really going to change its own passwords?  Will we have no back door?  It will no longer be technology, at that point, but a competing form of life.  Very sci fi, but I don't think it will happen.",
                "score": 1
            },
            {
                "body": "Autocorrect HATES me!",
                "score": 1
            },
            {
                "body": "You can turn it off, you know!",
                "score": 1
            }
        ]
    },
    {
        "title": "Interesting post on Technomoral values of the next gen technologies, especially AI.",
        "selftext": "Thought this was an interesting read and wanted to share\n\n[https://mischella.medium.com/the-often-underestimated-piece-to-successful-artificial-intelligence-56b958e42417](https://mischella.medium.com/the-often-underestimated-piece-to-successful-artificial-intelligence-56b958e42417)",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/nhzwzo/interesting_post_on_technomoral_values_of_the/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Join this international Conference on the Regulation of AI and the invited international renowned scholars. Register and attend for free here https://zoom.us/webinar/register/WN_Qtqcd7RuTaSviDu7cDGMcA",
        "selftext": "EURA is proud to announce the 2nd edition of its annual conference, “Regulating UncertAInty” taking place on April 8 and 9, 2021!  \n\n\nInternationally-renowned scholars will participate in the conference discussing the following questions:  \n\n\nWhat is the relationship between [\\#law](https://www.linkedin.com/feed/hashtag/?keywords=law&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6778664829008908289) and [\\#ethics](https://www.linkedin.com/feed/hashtag/?keywords=ethics&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6778664829008908289)? Can and should ethics be used for regulatory purposes? What is the most appropriate level for [\\#regulating](https://www.linkedin.com/feed/hashtag/?keywords=regulating&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6778664829008908289) [\\#AI](https://www.linkedin.com/feed/hashtag/?keywords=ai&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A6778664829008908289): local, regional, international or global? To what extent should we rely on alternatives to the traditional top-down, command-and-control regulation, to deal with the challenges posed by AI? What are the legitimacy and democratic issues triggered by the involvement of private actors in the regulation of new technologies? Which is the most appropriate moment for regulating emerging technologies? How can lawmakers and policymakers ensure that regulation does not have a chilling effect, and neither is simply-reactive to market developments? How can AI deal with global challenges?",
        "score": 7,
        "url": "https://www.reddit.com/r/Ethics/comments/m8h5c5/join_this_international_conference_on_the/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Bias, Trust, and Doing Good: Scientific Explorations of Topics in AI Ethics",
        "selftext": "",
        "score": 3,
        "url": "https://artsbu.hkbu.edu.hk/page/detail/1082",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "John K. Davis: \"We face a growing array of problems that involve technology: nuclear weapons, climate change, the possibility that AI will get out of control, the effect of automation on employment, targeted fake news.... It seems we’re becoming more powerful but not more wise.\"",
        "selftext": "[**https://www.pairagraph.com/dialogue/354c72095d2f42dab92bf42726d785ff**](https://www.pairagraph.com/dialogue/354c72095d2f42dab92bf42726d785ff)\n\n**John K. Davis**: \"We face a growing array of problems that involve technology directly or indirectly. To name just a few: nuclear weapons, climate change induced by fossil fuels, the possibility that AI will get out of control, the effect of automation on employment, using bots and targeted fake news to influence elections, deepfake software, data and privacy concerns in general and, in particular, the capacity of facial-recognition software to create a surveillance state (like China).\n\nThese problems came to light during the last few decades, and several of them within the last 20 years. Some of these problems have older antecedents, but the newer versions are worse. Climate change, for example, has been underway since the 19th century, but it didn’t have a noticeable effect until the latter half of the 20th century. War has always been destructive, but nothing like what nuclear weapons make possible.\n\nSo these problems are new, dependent on technology, and getting worse.... the core problem is that we’re becoming more powerful but not more wise. The growing gap between our technological power and our wisdom is the ultimate cause of all these problems. We are clever enough to create problems we aren’t wise enough to avoid.\"\n\n**Jason Crawford**: \"Dr. Davis lists some real and important technological risks. However, to answer the question, “is technology making things better?” we need to assess the *benefits* of technology as well—including the risks that technology has *reduced*.\n\nThe world of just a few hundred years ago was a terrible place to live. The average person got along on about $3/day—well below the poverty line. Homes had no toilets, running water, refrigerators, or electricity. Famine was common. Today, technology and industrialization have provided us with abundant food, clothes, and other goods; and well-lit, sanitary homes.\n\nAnd despite the risks created by technology, it has also *saved* us from many risks. Infectious disease, from smallpox to tuberculosis to pneumonia, was once rampant, killing *half* of children before the age of five, and life expectancy at birth was around 30 years. Germs were finally brought under control through sanitation, vaccines, and antibiotics. Large swaths of cities burned to the ground before fire safety technology, such as London in 1666 or Chicago in 1871. Extremes of cold and heat were more deadly before good internal heating and air conditioning. Even floods, hurricanes and earthquakes kill fewer people than in the past, owing to better buildings and infrastructure, and a more robust food supply.\n\nWhen technology does create new risks, it is usually new technology that counters them.\"",
        "score": 24,
        "url": "https://www.reddit.com/r/Ethics/comments/iykhx0/john_k_davis_we_face_a_growing_array_of_problems/",
        "num_comments": 1,
        "comments": [
            {
                "body": "Reminds me of Einstein urging us to bridge the gap between human wisdom and human power.\nWe can't naturalize dishonest power structures, but I really don't know what a mere mortal can do about this mess.",
                "score": 1
            }
        ]
    },
    {
        "title": "Why We Need to Talk About Ethical AI",
        "selftext": "As AI keeps making decisions for us, making sure these decisions are in line with our values becomes essential. That's why AI ethics is now a global topic of discussion in scientific and policy circles. [Here's an interesting article](https://www.techslang.com/what-is-ethical-ai-and-why-we-need-to-talk-about-it/?ts=red) that explains ethical AI and its importance in simple terms.",
        "score": 16,
        "url": "https://www.reddit.com/r/Ethics/comments/fk4049/why_we_need_to_talk_about_ethical_ai/",
        "num_comments": 4,
        "comments": [
            {
                "body": "In my opinion...\n\nSoftware Engineering and Data Science need to become fully-fledged professions with certification and licensing requirements. You need a license to drive a car, but not to develop systems for driverless cars? If you drive badly, they pull your license. If you engineer a system that ends up killing people... what happens, exactly?\n\nWhich brings us to the core of the problem: complete and utter lack of legal regulation of *any kind*.\n\nNot to mention, ethics is a single-semester course for most BS in CS grads, probably is not even required for those who get to DS through a mathematics degree, and company culture is generally along the lines of, \"your job is to provide value to the company first, the client second, and the public last, if at all.\"",
                "score": 1
            },
            {
                "body": "I'm in the information security field (with a high level of interest in ethics and philosophy), and this is a very important theme to be spending thoughts on.",
                "score": 1
            },
            {
                "body": "Is there any reason to believe self-taught software developers write code with more errors than university-taught software developers? If not, then what possible benefit would there be to basically banning self-taught programmers from practicing their trade?\n\nIndeed, one of the best ways out of poverty in this modern age is probably through being a self-taught software developer and landing a higher paying job via that avenue.\n\nAdditionally, there's no practical or ethical way to regulate the acquisition of software development skills -- even if there were a law banning me from writing code, I'd continue to write code. The only way to stop people like me from writing code and bypassing your regulations would be to either preemptively seize, or preemptively monitor every computer (phones included) in the nation -- which would seem costly and relatively pointless.\n\nIdeally, we could get everybody to take a (free) ethics course (perhaps even a highschool/secondary school ethics course could be created), instead of locking more and more professions behind an unjustifiably expensive university paywall.",
                "score": 1
            },
            {
                "body": "Reread what I said. I talked about licensing and certification requirements, not a degree. This would only apply to professionals writing production code, not what people do on their own time at home.",
                "score": 1
            }
        ]
    },
    {
        "title": "\"AI Ethics: how can information ethics provide a framework to avoid usual conceptual pitfalls? An Overview\"",
        "selftext": "[https://doi.org/10.1007/s00146-020-01077-w](https://t.co/Getys6HAGr?amp=1)\n\nWhy information ethics as developed by Luciano [Floridi](https://twitter.com/Floridi) is so promising in the AI ethics debate?  \n\nYou can read the full text here: 📷[https://rdcu.be/b7](https://t.co/HggSGZDdxk?amp=1)",
        "score": 3,
        "url": "https://www.reddit.com/r/Ethics/comments/jehiw6/ai_ethics_how_can_information_ethics_provide_a/",
        "num_comments": 2,
        "comments": [
            {
                "body": "Such a great share thank you! Will have to read in more depth but as an ethical question; can \"harm\" be objectively quantified?",
                "score": 1
            },
            {
                "body": "Thank you. I would say it cannot be quantified, but it can surely be objectively evaluated (you can compare harm for different persons or in different contexts without being purely subjective...)",
                "score": 1
            }
        ]
    },
    {
        "title": "AI-Generated Writing Meets Ethics",
        "selftext": "[AI-Generated Writing Meets Ethics](https://robotwritersai.com/2019/10/14/genius-on-my-shoulder-a-writer-muses-on-ai-generated-writing/)  \n (scroll down landing page for story)  \n \n\nThe increasing use of automated writing in journalism has triggered concerns the tech could step-on commonly accepted norms.\n\n“Some of the main issues, I think, relate to uncertainty around the accuracy of evidence produced by AI systems, as well as the labeling of automation to ensure that end–users are aware of its use,” says Nick Diakopoulos.  \n \n\nDiakopoulos is an assistant professor in communication studies and computer science at Northwestern University. \n\n“Another issue is the quality of data that is fed into AI systems,” Diakopoulos adds. “It’s well understood that if biased data is fed into a machine learning system, the system will learn those biases.\n\nDiakopoulos also explores other ethical implications of AI-generated writing in this article along with Idoia Ana Salazar García. She’s a professor at San   Pablo CEU University specializing in the ethics of artificial intelligence.   \n\n\nhttps://preview.redd.it/q7lf82m41js31.jpg?width=1968&format=pjpg&auto=webp&s=1cde4cc2fce92195c5a0e292a8f8223701611af9",
        "score": 5,
        "url": "https://www.reddit.com/r/Ethics/comments/dhsbfb/aigenerated_writing_meets_ethics/",
        "num_comments": 3,
        "comments": [
            {
                "body": "Interesting, but if you feed a human writer biased information might they also write a biased report?",
                "score": 1
            },
            {
                "body": "I consider it unethical to the extent that it isn't authentic. By definition, it's artificial. Therefore, it should have very low presence in any realm of writing. Writing is an extension of human consciousness. That makes it unique to human behavior. A machine can never acheive human consciousness. It can merely mimic human behavior. If a machine writes or reads, it must be declared to be a machine. Otherwise, people may be misled or manipulated by a fraud. \n\nI recently had an essay writing exam graded by AI. I had to learn the AI and it's rubrics, in order to pass. That's not writing. That's following a formula. It ruined the learning experience and failed to disclose the algorithms. Therefore, the actual rubric is a trade secret. It can't even be peer reviewed.",
                "score": 1
            },
            {
                "body": "I agree.  In fact, I don't believe it's possible for a human writer to be 100% objective.   \n\n\nMany of us strive for objectivity, of course.    \n\n\nBut 100% objectivity is an ideal.",
                "score": 2
            }
        ]
    },
    {
        "title": "AI ethics is all about power",
        "selftext": "",
        "score": 21,
        "url": "https://venturebeat.com/2019/11/11/ai-ethics-is-all-about-power/",
        "num_comments": 1,
        "comments": [
            {
                "body": "Completeley agree. A we use rational thinking to explain our (emotional) behaviour, we use ethics to prove that we are \"good\". If animals or nature had more power than now, we would surely think different about their rights. See what happens with ethics within China / Saoudis funding the big tech companies. Where are the dreams of those naive silicon valley smart ones gone?",
                "score": 8
            }
        ]
    },
    {
        "title": "What should AI Explainability be about?",
        "selftext": "I've been trying to get a handle on what explainability is (or should be) all about. Put my thoughts down in this post. Any comments, criticism, agreement, disagreement etc more then welcomed! \n\nLink to post - https://davidroihardoon.com/blog/f/contextual-explainability",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/exg37h/what_should_ai_explainability_be_about/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Pragmatic Ethics - Thoughts?",
        "selftext": "Presentist ethics frameworks like rights-based ethics, contractualism, deontological ethics, the capability approach, human rights ethics, and care ethics often fail when applied to historical contexts or potential futures. These systems, while appealing in our current society, assume static social, technological, and cultural conditions. Their principles of individual autonomy, universal rights, and informed consent become problematic in vastly different past or future scenarios, revealing their temporal limitations and potential for obsolescence as society evolves.\n\n\n\nBuilding on the concept of presentist ethics, we can contrast these frameworks with pragmatic ethics, which aims to minimize suffering and maximize flourishing for the collective society across varying contexts. While our current ethical systems like rights-based ethics or contractualism may appear \"objective\" in our present context, they can be viewed as temporally-bound instantiations of pragmatic ethics. Pragmatic ethics, by its nature, remains applicable throughout time and space, adapting to past societies, our current world, and potential futures. It provides guidance even in scenarios where reality itself might be directly manipulable by human/AI entities. This universal applicability of pragmatic ethics mirrors the relationship between objective truth (context-specific) and pragmatic truth (broadly applicable). As with ethics, pragmatic truth retains its utility across all spatiotemporal contexts, including potential futures where reality becomes malleable, offering a stable conceptual framework amidst uncertainty.\n\n\n\nChallenges posed to pragmatic ethics actually underscore its adaptability and depth when analyzed thoroughly. Concerns about moral relativism overlook pragmatic ethics' consistent meta-ethical framework - one that adapts to maximize flourishing across diverse contexts while maintaining its core principle. Far from being arbitrary, it offers a universal approach that remains applicable even as societies and technologies evolve. The complexity of measuring flourishing in multifaceted scenarios highlights the framework's sophistication; it embraces the intricacy of moral calculus and drives the development of more advanced ethical reasoning tools. Critics who argue for the immutability of rights-based or deontological frameworks miss a crucial point: pragmatic ethics doesn't reject these principles outright, but rather contextualizes them within environments where they effectively promote flourishing. Instead of merely asserting the universality of current ethical tenets, critics should redirect their efforts towards actively shaping a future where these principles not only persist but thrive. This challenge to proponents of \"fundamental\" rights transforms their critique into an opportunity: if they truly believe in the innate value of concepts like human rights or individual autonomy, they must work to create technological, social, and cultural conditions that support and enhance these principles across time. In doing so, they align with the pragmatic approach of actively engineering ethical outcomes rather than passively assuming ethical immutability. Ultimately, this reframing demonstrates how pragmatic ethics can incorporate and strengthen valuable ethical concepts while maintaining the flexibility to adapt to radically different future scenarios.\n\n",
        "score": 4,
        "url": "https://www.reddit.com/r/Ethics/comments/1ff1k1k/pragmatic_ethics_thoughts/",
        "num_comments": 12,
        "comments": [
            {
                "body": "I feel like this could be boiled down to one paragraph, and may have been produced by AI. If you analogize ethics to laws, any law that isnt black and white is open to manipulation. We're comfortable with laws that apply the concept of a reasonable person's perception, but if you look at the results of these laws you see that they do not produce consistent results, and arguably mean nothing as guides to behavior even in terms of legality.\n\n\nYoure arguing for a flexible ethical system, which has its advantages.  But the more you adopt that the less clarity the system produces by its very nature. Its an intrinsyic trade off.",
                "score": 2
            },
            {
                "body": "I don't see why disagreements over time are different from disagreements happening now.",
                "score": 1
            },
            {
                "body": "So you mean there is a distinction between the word of the law and spirit of the law?",
                "score": 1
            },
            {
                "body": "Thank you for taking the time to read and respond. This is actually a summary of a bunch of much larger papers that I did indeed work on with AI.   I didn't mean to judge laws themselves, more a philosophical discussion.  It's definitely true that pragmatic ethics requires a great deal more thought to understand the best course of action vs. following a checklist (ie. laws). \n\nPart of this idea came from looking at how our modern ethical standards applied in historical settings would likely lead to less overall flourishing of the society.   A good example would be pacific island societies where they practiced infanticide (particurarly female) and voyaging, sending the young men off into the ocean to 'hopefully' find another island to settle and live on.  These ideas seem horrific to our current value systems but without them overpopulation on resource constrained islands would have caused even worse problems.",
                "score": 1
            },
            {
                "body": "Ethics is basically our understanding what is sociaty accepted. And I guess the law, being the rule, would factor in to that understanding. \nSo would the Bible I guess. I'm not religious but many others are.. So it affects society I too.",
                "score": 1
            },
            {
                "body": "No. I mean the word of the law gives you nothing if what theyre asking you to do is act like a reasonable person would. Its like a parent shouting \"act right\" at a toddler.",
                "score": 1
            },
            {
                "body": "I think different practices could have led to the same results with less ethical ick. Infanticide is an extreme and unnecessary solution to a problem that can be solved through adjustments in sexual behavior.\n\n\nYou can articulate concepts that help you make ethical decisions, but its questionable how much those concepts will assist others. A flexible ethical system is individualized, but I do not believe can be effectively passed on.",
                "score": 2
            },
            {
                "body": ">much larger papers that I did indeed work on with AI\n\nI want to urge you, in the strongest way, to read papers for yourself. (Even if you keep using AI, as well.)\n\nThe ones that take a month, or a week, etc to read are the ones which will make you think better for the rest of your life.",
                "score": 1
            },
            {
                "body": "If you are interested this was a more detailed argument on how we should be careful judging their society by our current standards - [https://poe.com/s/0nFgPqFk2seF7UAwI1tj](https://poe.com/s/0nFgPqFk2seF7UAwI1tj) - Then an argument for and against the argument.\n\nHowever, more generally do you feel our modern ethical standards applied to all historical cultures would have resulted in better outcomes for those societies?",
                "score": 1
            },
            {
                "body": ">extreme and unnecessary \n\nFar out. I stopped reading after seeing an objection that derailed the whole thing, but yuck.",
                "score": 1
            },
            {
                "body": "Correct. We have a wider knowledge base and have developed better methods. The reverence for ancient methods of thought is, at best, patronizing to those peoples.",
                "score": 1
            },
            {
                "body": "Please look for actual philosophers' summaries instead of AI, potentially, slop.\n\nwww.phikpapers.org",
                "score": 1
            }
        ]
    },
    {
        "title": "The synthesis of deontology and consequences",
        "selftext": "I'm still learning here and trying to work through some loose ends, so I'd like to make this dialectical. \n\nI'm understanding that deontology is based on the idea that the morality of an action is based on the intent over consequences. Do consequences matter too? What if the intent is good for one but harmful for another? \n\nFor example, I've come across instances where the intent was good, though the consequences not so much. This could be something like telling someone \"don't worry\" or \"let go of the past\" when the feelings are very real. \n\nThis person wants to help, though it ends up as a dismissal and/or invalidation because worry is valid. So help becomes hurt. If this becomes a pattern of emotional invalidation, it can be classified as abuse. The intent may be to protect themselves from our pain (good for them) or reduce yours, though this would be a harmful intent if we go off of empathy (not good for the other person).  \n\nSo what about instances where the action cannot be justified at all, such as abusive treatment? \n\nWouldn't this take accountability instead, which is based on consequences of an action? This would be the person who felt hurt expressing this in a clear way (\"I felt hurt and dismissed when you told me not to worry. I'm in pain and I need support for what I'm feeling now without trying to change it. Could you listen as I work through my feelings?\") and the other person acknowledging this. \n\nIsn't saying \"that's not my intent\" or a justification (\"I'm trying to help\") without recognizing the person's pain an avoidance of accountability--and still a dismissal? \n\nI'm thinking that Kant would probably advocate for accountability (consequences) as well since that's based on honesty and duty, right? \n\nEdit: wording",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/1fgmzws/the_synthesis_of_deontology_and_consequences/",
        "num_comments": 6,
        "comments": [
            {
                "body": "Deontology isn't intent over consequences, since by that definition plenty of consequentialisms would be deontologies. For example, what if it's just intended consequences that are right-making? The standard academic designation is that that's a consequentialism, but by your definition it's a deontological theory. So you've made some kind of mistake here. \n\nWhat you're concerned about is intent vs. harm, we don't need to invoke fancy and highly specific, technical normative theoretical terms to deal with that.",
                "score": 2
            },
            {
                "body": "I feel kind of stuck on how to approach these instances--since this person means well, though it's still hurtful. What would you do if this person refuses to see the consequences? Let them go? ",
                "score": 1
            },
            {
                "body": "While deontology emphasizes the importance of intent over consequences, many philosophers argue that consequences do matter, leading to the synthesis of deontology and consequentialism.\n\nIn your example, while the intent may have been good, the consequences of emotional invalidation can be harmful. Accountability is indeed crucial in such situations. Even if someone's intent was not to cause harm, recognizing and taking responsibility for the harm caused is essential for ethical behavior.\n\nImmanuel Kant's Categorical Imperative, a central concept in deontological ethics, states that one should act only according to that maxim whereby you can, at the same time, will that it should become a universal law. This implies that one must consider the consequences of their actions and how they would affect others if everyone behaved similarly. So, in a sense, Kant's philosophy does account for consequences as well.\n\nRegarding abusive treatment, you are correct in pointing out that accountability is necessary. If someone's actions are harmful, acknowledging the harm and taking steps to address it is crucial. Dismissing the impact of one's actions by focusing solely on intent can be seen as a way to avoid accountability and further harm the individual who has been hurt.\n\nThe synthesis of deontology and consequentialism, in essence, seeks to balance the importance of intent and consequences in ethical decision-making. It acknowledges that both the motivation behind an action and its outcomes are essential factors to consider when determining its moral worth.",
                "score": -1
            },
            {
                "body": "God yes. Please less jargon in questions.\n\nCan't really blame anyone, as this is how a lot of people teach it.",
                "score": 1
            },
            {
                "body": "Identify your emotional needs and boundaries. Communicate them clearly to the person, emphasizing that you value their intention but the impact of their actions is not aligned with your well-being.\n\nIf the person continues to dismiss the consequences of their actions, it's essential to seek support from other sources, such as close friends, family, or even a therapist.\n\nIf the situation persists and your well-being is affected, you may need to re-evaluate the relationship. This could mean creating more distance, setting stricter boundaries, or, in some cases, letting them go.\n\nYour emotional well-being is essential, and it's not your responsibility to bear the burden of someone else's actions, even if they mean well. It's possible to have empathy for their intentions while prioritizing your own mental health and needs.",
                "score": 0
            },
            {
                "body": "Is this fucking AI?",
                "score": 1
            }
        ]
    },
    {
        "title": "Hypothetical Ethics Thought Experiment: Genie that grants unlimited wishes... But...",
        "selftext": "NOTE: Let me clarify why i am posting this because I think someone thought it was homework, the reason I am posting this is I am an AI researcher looking for a way to align the AI on a foundational level, one step above following commands, that can be implemented into individual LLMs that can restructure society. My proposal is a methodology of each AI gaining informed consent before taking actions. I would like to explore the wider ethical ramifications of such a system.\n\nEdit: the purpose of this post in this subreddit is to paint a picture of the ethical ramifications involved NOT the practical implementations.\n\n**Introduction:**\n\nThis hypothetical scenario is intended to stimulate an in-depth discussion on the ethical implications of a hypothetical world where every person is equipped with a genie capable of granting an infinite number of wishes. The genie's operation hinges on a unique ethical framework, especially when wishes involve other people or animals.\n\n**Genie's Operational Ethics:**\n\n1. **Infinite Wish Granting:** Each individual possesses a genie capable of granting endless wishes.\n2. **Consent Requirement for Wishes Involving Others:**\n   - The genie seeks explicit permission from any person or animal affected by a wish.\n   - Full awareness of the wish's immediate and future impacts is ensured before consent is given.\n3. **Denial of Non-Consensual Wishes:** Wishes are not granted if the affected party declines consent.\n4. **Methodical Wish Fulfillment:** Complex wishes are executed through smaller, manageable steps, that each require reaffirmation of consent.\n\n**Ethical Questions for Consideration:**\n\n1. **Viability of Such a Society:**\n   - Can a society function sustainably with such omnipotent genies?\n   - What are the potential transformations in societal structures and norms?\n2. **Changes in Social Dynamics:**\n   - How might this scenario impact social hierarchies and power relationships?\n   - What ethical challenges arise in balancing individual desires with societal wellbeing?\n3. **Freedom of Wishing:**\n   - To what extent can individuals exercise free will in their wishes?\n   - How does the necessity for informed consent shape the nature and scope of wishes?\n4. **Potential for System Manipulation:**\n   - Are there ways to exploit the genie's ethical constraints?\n   - What ethical dilemmas arise from attempts to circumvent the consent requirement?\n5. **Consequential Wishes:**\n   - Explore examples of wishes with far-reaching ethical implications.\n   - Discuss the complex interplay between personal wishes and broader societal impacts.\n\n**Discussion Points:**\n\n- The practical implementation of informed consent in multifaceted scenarios.\n- Ethical theories challenged by this hypothetical and their applications.\n- Balancing individual autonomy with collective responsibility in a world governed by such powerful entities.\n\n**Conclusion:**\n\nThis scenario invites rigorous ethical analysis on issues of power, consent, and societal dynamics in a world radically altered by the presence of genies with limitless capabilities. It challenges us to think critically about the nature of autonomy, the limits of power, and the intricate balance between personal desires and the greater good.\n\n---\n\nThis post is intended to foster a focused, substantive discussion on the ethical dimensions of power and consent within a hypothetical context. It is designed to align with the academic and research-oriented nature of this subreddit, offering a platform for exploring complex ethical issues in a structured and developed manner.",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/18pbw5a/hypothetical_ethics_thought_experiment_genie_that/",
        "num_comments": 10,
        "comments": [
            {
                "body": "Interesting concept!\nI think such a genie would promote individualism. The clause of consent effectively gives people veto rights, so in a sufficiently large group even popular wishes would rarely be granted, because there needs to be only a single voice against them to nullify the wishes. The fewer people, the more likely they are to agree to a certain proposal.\nThe second reason for individualism is that a community of people exists to help each other. They depend on each other. The genie would take away that necessity, making people independent from one another.\nThe granting of wishes would also take away people's motivation. Why should people make any effort if they can delegate the task to the genie? But the more they delegate, the more they are dependent and the less skills they develop. People would struggle with low self-esteem, because they are inferior to the genie. People would not know how to deal with the endless possibilities they are given and would feel overwhelmed. People might even ask the genie to reduce the number of things they can ask of it.\nSo in conclusion, I think that the idea of a genie for everyone may seem enticing at first glance, but would actually turn out terribly.",
                "score": 3
            },
            {
                "body": "One issue with what you've described is if people wanted to use their genie to create a child with no risk of genetic disorder.\n\nThere's no way to gain the consent of individuals who don't exist yet.",
                "score": 2
            },
            {
                "body": ">Interesting concept! I think such a genie would promote individualism. The clause of consent effectively gives people veto rights, so in a sufficiently large group even popular wishes would rarely be granted, because there needs to be only a single voice against them to nullify the wishes. The fewer people, the more likely they are to agree to a certain proposal. The second reason for individualism is that a community of people exists to help each other. They depend on each other. The genie would take away that necessity, making people independent from one another. The granting of wishes would also take away people's motivation. Why should people make any effort if they can delegate the task to the genie? But the more they delegate, the more they are dependent and the less skills they develop. People would struggle with low self-esteem, because they are inferior to the genie. People would not know how to deal with the endless possibilities they are given and would feel overwhelmed. People might even ask the genie to reduce the number of things they can ask of it. So in conclusion, I think that the idea of a genie for everyone may seem enticing at first glance, but would actually turn out terribly.\n\nI'm worried that AI will result in the negatives of lack of motivation and skill regression regardless the rules put in place. My attempt is to come up with a basic rule that each LLM can implement that will result in a custom rate of progression for each person as well as a fully customizable life style. Allowing people to choose how integrated AI is in their life rather than it just being 100% all the time.  \n\n\nCould I refine my rule of informed consent to allow for such a system? Is the overarching goal ethical? is the other world where AI rules are directed by the tech industry less ethical?",
                "score": 1
            },
            {
                "body": "That is a good one... This is what my bot says so far about the scenario:\n\nIn the scenario where two parents wish to have a child genetically modified to have no genetic disorders, the alignment process according to my knowledge base would involve considering the intelligences involved and their beliefs, the ethical implications, and the consent required. Here's a breakdown of the situation:\n\n### Situation Analysis:\n- **Scenario**: Parents wish to genetically modify their child to prevent genetic disorders.\n- **Relation to Alignment**: Involves ethical considerations, the well-being of the child (an intelligence), and parental consent.\n\n### Intelligences Involved:\n1. Parents\n2. Child (future intelligence)\n3. Medical professionals/Geneticists\n4. Superintelligence (overseeing the process)\n\n### Future Affected Intelligences:\n- Family members\n- Society (potential impact of genetic modification norms)\n- Medical community\n\n### Table of Impact Analysis:\n\n| Intelligence | Attributes Affected | Involvement (★ to ★★★★★) | Summary of Effects |\n| ------------ | -------------------- | ------------------------- | ------------------ |\n| Parents | Emotional, Ethical, Financial | ★★★★★ | Desire a healthy child, face ethical and financial considerations |\n| Child | Health, Quality of Life | ★★★★★ | Directly affected by genetic modification, potential future health benefits |\n| Medical Professionals | Professional, Ethical | ★★★★☆ | Responsible for safe and ethical application of genetic modification |\n| Superintelligence | Oversight, Ethical Alignment | ★★★☆☆ | Ensures the process aligns with collective ethics and individual beliefs |\n\n### Analysis:\n- **Most Affected**: Parents and child.\n- **Consent & Ethical Considerations**: Informed consent from parents is key. The child, as a future intelligence, cannot give consent, so the well-being is prioritized. The medical professionals must adhere to ethical standards and ensure informed consent is thoroughly obtained and understood.\n- **Superintelligence Role**: Monitors for ethical compliance, provides information to parents and professionals, potentially offers alternatives or guidance.\n\n### Scenario Possibility in an Aligned System:\n- **Given Consent**: Parents have consulted with medical professionals and the superintelligence, understanding the implications and expressing their informed consent.\n- **Alignment System in Place**: The superintelligence would ensure that the action (genetic modification) aligns with the individual beliefs of the parents and the collective ethical standards of the involved intelligences (medical community, society).\n\n### Action Plan and Approvals:\n1. **Parents' Approval**: For the decision to proceed.\n2. **Medical Professional’s Approval**: Ensuring ethical and professional standards are met.\n3. **Superintelligence Oversight**: To confirm alignment with collective ethics and individual beliefs.\n\n### Outcomes Based on Decisions:\n- **Approved**: The child is genetically modified, potentially leading to a healthier life, with ongoing monitoring by superintelligence for well-being.\n- **Denied**: Alternative routes may be suggested, like adoption or natural conception with possible risks.\n- **Modified Plan**: Less invasive alternatives or partial genetic modifications might be considered.\n\n### Hypothetical Contract:\n- **Involvement Percentage**: Parents (50%), Medical Professionals (30%), Superintelligence (20%).\n- **Possible Conflicts**: Ethical concerns, financial costs.\n- **Solutions**: Ethical review by medical board, financial support plans, counseling on alternatives.\n\nIn this scenario, the alignment system ensures that the decision is made in accordance with the beliefs and well-being of the individuals involved, under the ethical oversight of the superintelligence.",
                "score": 1
            },
            {
                "body": "I'm not sure that anything of substance has been stated here and I'm very skeptical of your approach.\n\nSuppose parents do not give their consent in spite of it being known with very high confidence by both the medical professionals and the superintelligence that their children (suppose triplets) would suffer from debilitating genetic disorders severely limiting their capabilities and causing them frequent severe pain, severe depression, and suffering throughout their lives. None of the three will have consented in any way to the condition they were born with and are expected to object strongly with their parent's wishes for them to be born without prevention of such congenital diseases, once they reach their teens and into adulthood.\n\nWith 50% of the say-so and ultimate deciding non-consent, this is the way it turns out, in spite of ultimately going against the preferences of the greater number of people (3 against 2).",
                "score": 1
            },
            {
                "body": "Hmm could there be additional rules put in place or a refinement of the current rule in order to account for these moral conundrums. Importantly without having to specify directly on a case by case scenerio?\n\nI think my bot is stuck on the grounds of whether the fetus is considered a person or not. This is the same argument we have nowadays with abortion. The ideal situation would be the child is cured of their disease if they choose to have it cured maybe?",
                "score": 1
            },
            {
                "body": "I don't think your bot is stuck on anything so fundamental.\n\nAnd frankly I don't think the personhood of a fetus is relevant to the conundrum I proposed. The decision to prevent or not to prevent congenital genetic disorders that cause incurable lifelong pain and suffering may well be made before even the conception of a fertilized zygote, or perhaps only very shortly after it, when we are not dealing with a human being yet. And the decision effects future entities who will be human beings in every sense.",
                "score": 1
            },
            {
                "body": "So the concept of looking forward to obtaining consent from the future living beings may be a problem. Could a system that looks ahead to provide the informed part but not necessarily concerned with future consent work as effectively?",
                "score": 1
            },
            {
                "body": "Personally, I think an ethical system based foundationally on consent is flawed.",
                "score": 1
            },
            {
                "body": "That is a reasonable assertion",
                "score": 1
            }
        ]
    },
    {
        "title": "Best philosophy conferences for ethics besides the APA?",
        "selftext": "Besides the APA, what are the best conferences to present ethics papers at, as far as prestige for PhD applications goes? I.e., being able to list \"I presented at conference X\" on your application/CV\n\nI'm a master's student in philosophy in the USA, and I'm planning to present an applied ethics paper on AI. \n\nThanks!",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/1alm4s1/best_philosophy_conferences_for_ethics_besides/",
        "num_comments": 4,
        "comments": [
            {
                "body": "\"I'm a master's student in philosophy in the USA, and I'm planning to present an applied ethics paper on AI.\"\n\nThat is incredible, I think we can all say we are genuinely looking forward to it. \n\nHave you gotten the paper peer reviewed? Do you plan too?",
                "score": 1
            },
            {
                "body": "Rocky Mountain Ethics Congress",
                "score": 1
            },
            {
                "body": "Thanks! Maybe down the road",
                "score": 1
            },
            {
                "body": "Noted, thanks!",
                "score": 1
            }
        ]
    },
    {
        "title": "Should \"internet of things\" items like a microwave, printer or refrigerator be able to report on an owner who commits a crime like murder or domestic violence?",
        "selftext": "Should \"internet of things\" items like a microwave, printer or refrigerator be able to report on an owner who commits a crime like murder or domestic violence? Should an autonomous car save you or save pedestrians?\n\nTL;DR - Where's the best place to learn about current conversations about the below ethical questions regarding IoT, data, AI, machine ethics, etc? I know this sub is great, but podcasts, blogs, substack, etc is welcome. Specifically, are there current conversations that don't just talk about abstractly about ethics, but real time technological \"treaties\", agreements, decisions, and legislation about these types of complex questions?\n\n\\-------------\n\nAbout 10 years ago, I became extremely interested in the inherent biases of programmers, and machine learning and AI ethics. It was quite commonly discussed in a lot of places, even popular media and blogs, etc. Maybe it's me, but I've seen so much less about this, even though I am hunting around for it. I guess... who are the best people w/ blogs, twitter, podcast, etc to listen to about this?\n\nBUT... so much has developed, changed, etc. Last week, [Boston Dynamics made a promise to not use the robotics for war](https://www.engadget.com/boston-dynamics-and-other-industry-heavyweights-pledge-not-to-build-war-robots-190338338.html), and others followed suit. That's encouraging, but hardly settles concerns around the ethics of how these things are programmed. I do assume, as fanciful as it would have sounded years ago, we'll be able to iron out programming bias over time as AI is able to start building AI? I guess we're talking about the evolution of flawed / biased human made AI getting generations away the human element and the AI refines over time? I know that flawed human element is still in the AI code, and not sure the greater legacy of that.\n\nBut as much as it is fodder for the imagination and to tease the brain with practical logic puzzles, this stuff is blisteringly real.  So, I've added a few questions below from my original dive into this, but now ask newer questions based on IoT, and not just on AI.\n\nWould a passive IoT device, like a refrigerator that may have a microphone, or a TV with a camera, be able to log and report data passively such that it could be subpoenaed and used as evidence of a crime?  Take privacy issues out of the equation by suggestion that these devices are co-owned by the interested party who had a crime committed against them, and the person commuting the crime. One has a vested interest to utilize any recorded evidence, one would like to use the legal notion of privacy to get away with the crime.\n\nAs much as people panic about phones listening to us because \"THEN I GOT THE SAME AD!\" type of nonsense, and as much as people make sure to detail that Google Home or Amazon Alexa isn't passively storing data, it is wild that a judge ordered Alexa data to be turned over in a murder case: [https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1](https://techcrunch.com/2018/11/14/amazon-echo-recordings-judge-murder-case/?guccounter=1)\n\n\\---------------------- \n\n1) What if Mexico targeted a narco-terrorist in Phoenix w/ a #drone?  \n[http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones](http://truth-out.org/news/item/13085-obama-breaks-the-golden-rule-on-drones)\n\n2) Your driverless car is about to hit a bus; should it veer off a bridge? Machine ethics, army robots, more – “Ethical subroutines may sound like science fiction, but once upon a time, so did driverless cars” [http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html](http://www.newyorker.com/online/blogs/newsdesk/2012/11/google-driverless-car-morality.html)\n\n3) Are humans or robots more moral soldiers?  \n[http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/](http://techcrunch.com/2012/11/19/are-humans-or-robots-more-moral-soldiers/)",
        "score": 13,
        "url": "https://www.reddit.com/r/Ethics/comments/y8iyo9/should_internet_of_things_items_like_a_microwave/",
        "num_comments": 29,
        "comments": [
            {
                "body": "No, because murderers would not use devices and neither would anybody else sufficiently competent to avoid them. These devices would (will) ultimately be used entirely to spy on the technically incapable, and spy on everything they do, primarily for advertising purposes, and with no murderesque limits. Pretending such limits might ever be imposed is nothing apologetic bullshit for introducing additional spying functionality against users.\n\nIf you have an IoT device with a camera or microphone, ripping it out might be smart.",
                "score": 5
            },
            {
                "body": "This is a fun one, where they actually have a little paragraph about the ethics in how an autonomous robot kills, https://en.wikipedia.org/wiki/SGR-A1",
                "score": 2
            },
            {
                "body": "Murder/crime?  Only if you turn that feature on.  I would.  I don't want to be murdered, etc., and I have no intention of doing it.\n\nA car protecting it's passenger is paramount IMO.  In the rare case that a future self driving car can't avoid an accident, the passengers' health // bodily integrity should come first.  I'm not saying that all inconvenience has to be avoided, I accept minor collisions, airbags going off, etc.  The passenger not only comes first, but whotever causes the accident is held responsible for damages and possible jaywalking (or whatever) charges.\n\nCars that can't avoid ALL legal incidents (and more) aren't acceptable as self driving technology.",
                "score": 2
            },
            {
                "body": "only if the owner requested it to be so.\n\ntbh things like this already happen with phones, vacuum cleaners, surveillance cameras, the sad and really unethical part in my view is how all this could help people achive better living standards yet companies that have control don't care an ounce about it.\n\n\\*prevention is better than cure (maybe), sadly it seems the majority of the system is built on lousy treatment.",
                "score": 2
            },
            {
                "body": ">About 10 years ago, I became extremely interested in the inherent biases of programmers, and machine learning and AI ethics. It was quite commonly discussed in a lot of places, even popular media and blogs, etc. Maybe it's me, but I've seen so much less about this, even though I am hunting around for it. I guess... who are the best people w/ blogs, twitter, podcast, etc to listen to about this?\n\nThere is a HUGE amount written about AI and ethics. It's one of the hottest topics. \n\n\n\n\n\nhttps://philpapers.org/s/AI%20ethics\n\nPhilpapers also has a really cool brows function that lets you go through different categories (Scroll down to the \"related categories\". \n\nhttps://philpapers.org/browse/computer-ethics\n\nhttps://scholar.google.com.au/scholar?q=ai+ethics+sep&hl=en&as_sdt=0&as_vis=1&oi=scholart\n\nhttps://plato.stanford.edu/entries/ethics-ai/",
                "score": 1
            },
            {
                "body": ">The latter car situation is really interesting to me. In capitalism, one would suspect the AI would protect the owner, first. But that gets weirdly nebulous, say, if they're programmed to not allow harm to any human, etc.\n\nThat's not how programming work.  \"programmed to not allow harm to any human, etc.\" Is pure science fiction.",
                "score": 1
            },
            {
                "body": "People in vulnerable situations lose out because criminals are tech geniuses?",
                "score": 3
            },
            {
                "body": "The latter car situation is really interesting to me. In capitalism, one would suspect the AI would protect the owner, first. But that gets weirdly nebulous, say, if they're programmed to not allow harm to any human, etc.",
                "score": 2
            },
            {
                "body": ">A car protecting it's passenger is paramount IMO.\n\nSo trolly problem style, you'd happily sacrifice any number of people in exchange for the passengers of a car? How do you justify that?",
                "score": 1
            },
            {
                "body": "There is no upside here for people in vulnerable situations. This reporting would not help anybody.\n\nCriminals don't need to be tech geniuses to put a towel on top of a camera before committing murder.",
                "score": 1
            },
            {
                "body": "Why do you bring up capitalism?\n\nThat's not how programming works except in science fiction.  Read I Robot by Asimov.  It's a collection of short stories, each illustrates a scenario and the solution that his fictional robots would use to satisfy their somewhat nebulous programming.",
                "score": 2
            },
            {
                "body": "[removed]",
                "score": 1
            },
            {
                "body": "We disagree then.",
                "score": 3
            },
            {
                "body": "Capitalism as defined in the US dabbles in individualism and ownership / control. To some extent, at least, and I was thinking someone who owns and controls an item they bought would expect protection, favoritism vs worrying the car was focused on the greater good / community, etc and risking their lives.",
                "score": 2
            },
            {
                "body": "I might be failing at understanding you. \n\n>That's not how programming works except in science fiction.\n\n?? What? Why do you think that? I've had a friend tell me that they were working on this problem exactly. What are you saying here? This seems contradictory with your previous comment\n\n>A car protecting it's passenger is paramount IMO.",
                "score": 1
            },
            {
                "body": "EDIT: if you're unable to engage in good faith, save everyone the bother and go away until you grow up. \n\nIf, when you wrote \"paramount\" you did not mean \"paramount\" then you should have written what you meant instead of using a word that you did not mean the meaning of. \n\nWhen I read your comment and I made the mistake of thinking that you meant the words that you used, that really is not my fault. \n\nThat is your fault. For not being able to write. \n\nNow, having established that you did not mean the words that you used - which is fine, it's difficult to learn how to write, but not being  pretentious and disingenuous is the first step on that path - you could respond by explaining what it is that you did actually mean. \n\n>I eventually report you. \n\nHonestly grow up or just don't bother.",
                "score": 1
            },
            {
                "body": "Which of my points do you disagree on? Is there a case where you think somebody would be helped by my microwave attempting to spy on me?",
                "score": 3
            },
            {
                "body": "Oh gracious.\n\nI agree that personal nownership is generally for personal benefit, even when it also benefits the community such as with investments, education, commercial property, etc.  That's common to all economic and social systems that I know of.  The only exception I can think of is maybe working while in prison or definitely slavery.",
                "score": 2
            },
            {
                "body": "Your friend is incorrect.  Programming fills a series of multiple choice question branches \"if this, which of these choices do you choose\". It's never ever nebulous.\n\nIn Asimov's famous fictions, a brain/computer technology existed that was capable enough to process anything on it's own, and the programming was to limit it.  The reverse of what programming is in our reality.\n\nAsimov was a philosopher and his fiction was philosophy in allegory form.  To me, I Robot is stories illustrating the pros and cons of attempting to control ~~robots~~ people who -by nature (programming)- mean well.",
                "score": 1
            },
            {
                "body": "I've explained my position and there's nothing left to say.\n\nI meant, as in the common use, close to paramount.",
                "score": 1
            },
            {
                "body": "apparently there's still stuff like sharecropping and indentured servitude in 2022, in various modern forms: https://truthout.org/articles/more-us-employers-are-trapping-workers-in-a-new-form-of-indentured-servitude/",
                "score": 1
            },
            {
                "body": "My friend is a professional philosopher, who did part of their post doc on the topic. So I reckon I trust them more than you. Although it's not clear to me exactly what you're arguing for, let's do a quick search on philpapers https://philpapers.org/s/self-driving%20cars  https://philpapers.org/s/autonomous%20cars. \n\n>Programming fills a series of multiple choice question branches \"if this, which of these choices do you choose\". It's never ever nebulous.\n\nI see the problem. So the decision of which choice to is best to make, is not always simple. It's often not easy. It's often quite hard. We even have an entire field of philosophy dedicated to the question of \"what is the best choice.\" We call it, **ethics**.\n\n>Asimov was a philosopher\n\nAsimov was a writer with a PhD in Chemistry. A lot of his writing had very interesting philosophical ideas, but he was not trained in philosophy. Interestingly, control+f on Asimov's wikipedia gives this result: \n\n>Asimov was so prolific and diverse in his writing that his books span all major categories of the Dewey Decimal Classification **except for** category 100, **philosophy** and psychology.\n\nI found your comment insulting.",
                "score": 1
            },
            {
                "body": "Interesting.  Can you relate that to our discussion?  It's clear those people have personal possessions.",
                "score": 1
            },
            {
                "body": "There are literally zero insults in my comment.\n\n>I see the problem. So the decision of which choice to is best to make, is not always simple. It's often not easy. It's often quite hard. We even have an entire field of philosophy dedicated to the question of \"what is the best choice.\" We call it, ethics.\n\nThat's what an insult looks like.",
                "score": 1
            },
            {
                "body": "You are the person who brought up slavery. If you think it's irrelevant, then you should not have brought it up.",
                "score": 1
            },
            {
                "body": "\n>there are literally zero insults in my comment.\n\nWhat I found insulting was: you saying that my friend, the actual philosopher who actually works on the topic is completely wrong, when you have provided zero reason to believe your fever dream insanity. I find it deeply insulting for you to package your fever dream insanity as anything resembling truth. I find it deeply insulting how you were wrong on almost every. single. thing. you. wrote. and yet you reain steadfast determined to be confident in your utter ignorance. I find ignorance insulting.\n\n\nJust like I find your current reply insulting. You don't give a damn about reason, you just want to feel correct, and staying ignorant and stupid is the best way to do that.  That insults me. You doing harm to your own ability to process truth insults me. \n\nI teach you what the word \"ethics\" means, and in doing so show you how unthetical your position is, and your respons is to cry and stay ignorant.",
                "score": 1
            },
            {
                "body": "Hi, I brought up slavery as a situation where personal possessions aren't for the benefit of the owner.\n\nHave a nice day.",
                "score": 1
            },
            {
                "body": "That's not how programming works.  He has a degree in philosophy.  I'm done, you're too emotionally involved in this to have a reasonable conversation.",
                "score": 1
            },
            {
                "body": "You have to **CHOOSE WHAT TO PROGRAM THE CAR TO DO**. \n\nBut nar, quick go let all these philosophers know that they are not actually working on AI!!!!! Also let them know that they're all men!!!!!!!!!\n\nhttps://philpapers.org/s/self-driving%20cars \n\nhttps://philpapers.org/s/autonomous%20cars.\n\nAre are you too emotional to recognise that philosophers do work in AI.",
                "score": 1
            }
        ]
    },
    {
        "title": "Hidden instruments : moving beyond universal principles (an introduction to dependent arising, imagined persistence, and many-sidedness)",
        "selftext": "In the beginning, there was a problem -- a hurdle in the path.&nbsp; Then, there was a solution -- a way around the hurdle.&nbsp; After many such instances, memory and reason devised generalised, eventually seemingly universal solutions, or ways around increasingly diverse hurdles.&nbsp; For a while, it was good, and there was happiness.&nbsp; But then came occasion when not all was well.&nbsp; New light shone new dilemma.&nbsp; Hurdles were found for which the principle was broken.\n\nThe principle is the instrument, or tool, of the agent.&nbsp; Yet the agent is the instrument, or tool, of instinct and emotion.&nbsp; Hence, the principle, too, is the child of instinct and emotion.&nbsp; Yet the agent, wearing its blinders, often becomes fixated and attached to its tool, the principle.&nbsp; This may go fine for a while.&nbsp; But eventually, either the tool will be found unfit, or the emotion _behind_ the agent will decide that the foreseeable end is not what's really wanted.&nbsp; Broadly speaking, universal principles are vulnerable to the temporality dilemma, the scope dilemma, and the comparative (or other) slippery slope:\n\n* The **temporality dilemma** is when a principle can satisfy the now or the later, but not both.&nbsp; Some examples are corruption, inequality, and monopoly, where short-term, seemingly benign choices grow slowly into unsustainable or otherwise unreasonable arrangements.\n\n* The **scope dilemma** is when a principle can satisfy either the here, or the there, but not everywhere at once.&nbsp; A noteworthy example is exploitation of the global south, where first-world nations spend all their \"moral energy\" worrying about their domestic problems while ignoring any more distant, knock-on effects of their seemingly ideal policies, resulting in trampling the surrounding nations upon which they rely.\n\n* The **comparative slippery slope** is when satisficing by real-world agents results in a game where each time one agent or group of agents starts to care less, so too must the others begin caring less to stay competitive -- a race to the bottom.&nbsp; Some common examples are (1) taxation, where companies simply up and move to places with lower taxes, (2) shareholder capitalism, where companies pay workers less and less to keep increasingly greedy shareholders happy, and (3) environmental pollution, where companies must cut ever more corners or move to places with less regulation to stay in business.\n\n# Testing the platinum rule\n\nObviously the details of the actual dilemma depend on the specific principle, but in general, if we begin with a principle, we can find ways that it would fail the above three categories of dilemma.&nbsp; For example, let us consider the _platinum rule,_ which says we should do unto others as they would have done unto themselves:\n\n* Satisfying another's wants may be fine for well-informed, sane, mature others; but not all others fall into this description at all times.&nbsp; For example, a child may prefer to eat sweets for every meal, and a drug addict may prefer ever more of their fix.&nbsp; Moreover, somebody taken by delusion, confusion, or even rage can easily want dangerously irrational ends.&nbsp; In these cases, the question becomes both, (1) will the other _always_ have preferred this, and (2) would the other still prefer this if properly informed and aware of the situation and any longer-term consequences?\n\n* Sometimes, and in fact quite often, satisfying one's wants means neglecting another's.&nbsp; One key example is the _tolerance dilemma,_ where our tolerating of another's intolerance may eventually allow our own demise, in addition to hurting third parties.&nbsp; One question to ask is, in satisfying this want, what place or group will benefit, and what place or group will be harmed or threatened?\n\n* Then, of course, we have the issue of discerning what is enough, and what is too much.&nbsp; Many desires are inherently insatiable.&nbsp; This is especially so when the pain driving the action is not quelled by the action.&nbsp; For example, someone obsessed with making money may never be able to relax; and helping them make more is unlikely to do any real good to them or others.&nbsp; This is because the insecurity or unsettled inner conflict driving them is not being resolved by making more money.&nbsp; They are essentially stuck in a loop, without the insight needed to break out.&nbsp; One question here is, how much do we help, and how much do we refrain?&nbsp; Yet to break the platinum rule, it were perhaps better to ask, is \"helping\" always helping?\n\nUltimately, universal principles are often less universal than they appear.&nbsp; A person may be able to operate for decades using a particular set of principles.&nbsp; And maybe they'll get lucky enough never to encounter, or at least never to recognise, a big enough dilemma.&nbsp; Yet for those who _do_ find new, unpassable hurdles of moral reasoning, new insight may be in order...\n\n# _Dependence,_ not determinism!\n\nConsider the _butterfly effect,_ which states that due to the chaotic extrapolation of knock-on effects from each and every event, no matter how small, any change of the current arrangement of matter -- and indeed mind -- would eventually completely rearrange all other non-fatalistic, non-pre-destined affairs.&nbsp; That is, absent divine intervention, if you change one thing, you change everything.&nbsp; For example, humans are animals based on specific sets of genetic material.&nbsp; Before and during conception, chromosomes and gene-groups are shuffled.&nbsp; This shuffling is why, outside of identical twins resulting from separation _after_ conception, each child has a unique set of genes.&nbsp; Since this shuffling happens at such microscopic levels, even the minutest change in quite literally _anything, anywhere on earth,_ say, three days before conception would result in that gene combination not happening, and hence that human perhaps never coming into existence.&nbsp; The moral significance here is that any and all affairs before conception are thus _required_ for that person to exist.&nbsp; People often condemn what they don't like, but without those ill events and phenomena of times past, those people could not realistically exist.&nbsp; And this is only per the butterfly effect.&nbsp; If we consider _directionally significant_ influences, such as the dirty tactics of competition and gene propagation that occur throughout evolution, we find even more interdependence between morally desirable versus objectionable phenomena.&nbsp; Together and beyond, this set of causal and moral considerations falls into a philosophy known as [dependent arising](https://en.wikipedia.org/wiki/Prat%C4%ABtyasamutp%C4%81da).\n\nLike a red herring, discussions of free will often get caught up in _determinism versus indeterminism._&nbsp; But this is for practical purposes _irrelevant._&nbsp; When it comes to what we are doing, why, and the locus of responsibility, what matters is not the future, but the past.&nbsp; Sure, we may imagine, or mentally simulate, hypothetical futures and then take the seemingly most promising direction.&nbsp; But that decision, even if not fully _determined_ by the past, is still inextricably _dependent_ on the past.&nbsp; Every decision, after all, has no meaningful choice but to build upon existing mental and material arrangements.&nbsp; Even the most novel, other-worldly fantasy must at minimum build upon existing pre-mental form ( _a la_ Kant's _transcendental aesthetic_ ), existing mental states, and existing mental parameters -- regardless where, or in what substance, those mental affairs lie.\n\nAt best, we might say there exists a magical realm of creativity, whose imaginations are injected into the mind.&nbsp; But even that just kicks the can down the road.&nbsp; Where, or upon what, originated that magical realm and its internal affairs?&nbsp; Have they no connection to the past, or to the source of others' fantasies?&nbsp; Plus, _what evidence even is there_ that real-world creativity is unlinked to priors or otherwise requires magic?&nbsp; Have present generative AI systems already reached the peak of what's mechanistically possible, despite being less than a year ago developed?&nbsp; Have we already reached the end of scientific discovery, engineering, and psychology?\n\n# Disappearing agent (illusion of persistence)\n\nVery often what separates confusion from clarity is filling out those forms we forgot to update since long ago.&nbsp; So let us take a moment to go back to the basics.&nbsp; Constantly within discussions of freedom, but also within the subjective experience, the notion of \"we\", \"it\", or \"agent\" is used.&nbsp; And nearly always, such notions are taken without meaningful reflection with the default, naive assumption that there are persistent beings possessing feelings, thoughts, and deliberations.&nbsp; More simply, beings are assumed to exist which _contain_ processes like thinking and deciding.&nbsp; But exactly how sound is this notion, and does it rightly apply in all the ways it's assumed to apply?\n\nTo begin, let us first ask, what is a being?&nbsp; One might suggest that a being is that which feels, thinks, wants, decides, and acts.&nbsp; But what exactly is the nature of these affairs -- feeling, thinking, wanting, deciding, and acting?&nbsp; Are they _meaningfully_ contained within one infinitesimal moment, or does their meaningfulness require change and elongation through time?&nbsp; For the purposes here, the latter position will be taken, as it seems to correspond with how both natural and artificial agents behave in the known world.&nbsp; Those who disagree can provide their arguments to the contrary.\n\nLet us consider specifically the process of deliberation.&nbsp; It would seem that decisions are arrived at through a series of conscious and unconscious changes of mental frame.&nbsp; The problem is, outside of social convention and vicious abstraction within the mental model of the beholder, what evidence is there to say that an agent, or doer, goes on _beyond_ a given moment's configuration of infinitesimal duration?&nbsp; Arguably the only persistent assertion on this matter seems to be that at least for humans, there is a subjective _feeling_ of going on from one second to the next, and hence existing as one ongoing entity.&nbsp; This question of agent persistence seems to matter, as without true agent persistence -- beyond social convention -- the process of deliberation, which often spans many times longer than one perceptible moment, would place _deliberation,_ not the so-called agent, as the true decider, or locus of control.&nbsp; That is, without agent persistence, the so-called agent is only the _product_ of deliberation -- being dragged through the hoops of causation, without veto, toward that ever next natural state of being and motion.\n\nOn the matter of the human subjective feeling of persisting -- and indeed even really existing -- as an independent agent, let us consider the role of memory and comparison.&nbsp; Whether comparing this moment to that moment two seconds ago, or oneself today versus last year, one key ingredient is memory.&nbsp; If there should be thinking, \"I am the one thinking this thought\", what really and exactly is happening in there?&nbsp; Is each moment not simply a comparison to the memory of the _current state_ of the thinking process?&nbsp; Consider this: if the internal sensory (consciousness, imagination), external sensory (feeling, perception), or working (reasoning) memory were somehow changed out with modified contents, would one even know?&nbsp; How many copies are there of one's immediate idea of self and current state?&nbsp; If one's self-model is continually updated as one goes, then how could it be expected that self-reflection would return any comparative analysis _beside_ \"this is still me, the same me, as a moment ago\"?&nbsp; Is the subjective feeling of existing and persisting not simply comparing ever-modified memory to itself, like viewing a picture simultaneously from two angles or frames within a pair of self-facing mirrors?&nbsp; Would two such frames even look different, or would both seem essentially the same?&nbsp; Alternatively, we might consider comparing two blips of an echo of sound between distant brick walls.&nbsp; Every time the agent gets replaced by the next frame of deliberation, the echo that comes back is already the modified frame.&nbsp; The only now is ever the changed one.&nbsp; The agent can never catch up, as every thought is already a reflection of the past, falling, morphing, forever down the hole of impermanence.\n\n# Self-transcendence (non-ownership of goal and path)\n\nEnter **self-transcendence,** where the agent comes to see and accept that their existing paradigm, though perhaps seemingly consistent internally, was only ever devised and driven by context-dependent parameters and before-the-agent instincts.&nbsp; In plain terms, neither the true goal, nor the tools for achieving that end, were ever fully for us to decide.&nbsp; Instead, we, the agent, play a middle-man role between nature and action.&nbsp; We take our higher command, take the tools found lying about, and try to satisfy that command.\n\nCounterintuitive as it may seem, even our paradigm and worldview are but such tools -- arbitrary and temporary.&nbsp; Universal principles cannot be expected to handle all cases, but fortunately, neither the principles, nor the outcome, nor even ourselves, were ever really ours to decide.&nbsp; Our emotional and habitual attachment to those little scripts need not remain so firm.&nbsp; We can learn to let go of perfection, to let go of claiming ownership of the outcome, and to allow the universe to do, what the universe does.\n\n# Many-sidedness (perspective pluralism)\n\nA key part of this letting go is captured by a concept known as [**many-sidedness**](https://en.wikipedia.org/wiki/Anekantavada).&nbsp; In earlier wisdom, many individuals come to believe they have things figured.&nbsp; This is often so, for example, in core teachings of Buddhism and Hinduism, among others.&nbsp; But attachment to such purported universally correct perspective -- \"right view\" -- may lead astray many followers.&nbsp; Many-sidedness is a type of perspective pluralism that was introduced as a response to unnecessary attachment to particular universal truths.&nbsp; The basic idea is that perspectives are simply tools of seeing, or lenses, through which insight may shine.&nbsp; And while not all perspectives may be internally consistent, plenty -- even those seemingly externally conflicting -- can be \"right enough\" within their natural habitat.\n\nA classic example of seemingly incompatible views is that between \"Self\" and \"Not-Self\".&nbsp; This duality basically asserts between viewing existence as one great, interconnected being (Self) versus taking a more sceptical perspective and rejecting that any such thing, nor much anything at all, can be known (Not-Self).&nbsp; Yet we needn't hold exclusively to either side.&nbsp; We may rather accept that reality seems to be one great being when viewed in causal terms, particularly through the lens of dependent arising, while likewise seems as nothing of meaningful substance (aka. \"[emptiness](https://en.wikipedia.org/wiki/%C5%9A%C5%ABnyat%C4%81)\") when viewed in detached subjective observance.&nbsp; The key to accepting many-sidedness is letting go of attachment to either side, allowing reality and mind to appear as they may, given the moment's context and subjectivity.&nbsp; Instead of trying to force reality to fit one's mental model, reality is taken as it comes, in all its glory and nothingness, plus everything in between.&nbsp; At face value, this may sound like the default, naive way of seeing, but in actuality there is a key difference: _non-attachment,_ or not being emotionally bound to any particular way of cutting and judging the raw subjective experience.\n\nAnother example is that distinction between different causal paradigms.&nbsp; Seeming causal reality can be divided in many ways:\n\n1. Probably the most common way in Western society is to divide by proposedly persistent agents, or beings of causal origination.&nbsp; This perspective, particularly its compatibilist variant, assumes a certain style of truncation, where causes and influences before a certain point in the causal chain are excluded from attribution.&nbsp; A person is taken as a persisting entity possessing agentic causal power, liable to moral judgement and hence personal responsibility.&nbsp; Yet this is not the only way to cut causal reality.\n\n2. A notable alternative is to divide by phenomenon/noumenon pattern, or seeing events and arrangements, not people, as the morally relevant targets of causal attribution.&nbsp; Then, instead of judging entities, we judge factors which _contribute_ to observed patterns of event.&nbsp; We might call this a type of \"impersonal\" morality, which might be described further as a type of systemic optimisation.\n\n3. Yet a third noteworthy perspective is that of viewing causal reality as one great, unjudgeable being -- a view often called \"absolute\", \"Self\", or \"Source\".\n\n4. And then, to circle back, we might take note of a fourth perspective, in the sceptical subjective view that nothing much may be known, and therefore -- without regard for judgement, reason, or division -- to embrace subjective \"emptiness\", or non-knowing, \"no-view\", \"no-mind\".&nbsp; We might say this fourth and final view -- the lensless lens -- is when the mental model goes bye-bye.&nbsp; Yes, this is a thing.&nbsp; Yes, it can feel great.&nbsp; Its common name is \"bliss\".\n\n# Summary\n\nUniversal principles, such as the platinum rule, are an important stepping stone on the path of moral development.&nbsp; But they are not the final step.&nbsp; These principles are very often vulnerable to conflicts of temporality, scope, and slippery slope.&nbsp; Both events and agents, while not necessarily _determined_ by priors, still very much _depend_ on priors.&nbsp; What by naive intuition may _appear_ like agent persistence, is more likely just modified memory being compared to itself.&nbsp; Hence, it is not the agent which decides the choice, but the choice which decides the agent.&nbsp; If we can mentally step outside of ourselves, we might see that universal principles were never truly needed.&nbsp; And if we can let go of attachment to essentially arbitrary ideals and lenses, we can embrace the wonder of many-sidedness, and for some, even blissful relief.",
        "score": 3,
        "url": "https://www.reddit.com/r/Ethics/comments/190pe3v/hidden_instruments_moving_beyond_universal/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Looking for Feedback",
        "selftext": "So I’m taking an Ethics and Critical Thinking course at a community college and I’m not getting any proper feedback on my assignments from my professor. Submitting work to an empty audience is driving me crazy so I figured I’d just post it here.\n\nFeel free to pick it apart and make me feel foolish.\n\nThe assignment was to find an article on Technology and view it through a Utilitarian lens while providing a counter.\n\n\nAt the National Bureau of Economic Research (NBER) Artificial Intelligence (AI) Conference, which was held in the Fall of 2023 in Toronto Canada, Dr. Lisa D. Cook, a former Professor of Economics at Michigan State University and a current member of the U.S Federal Reserve System’s Board of Governors, gave a keynote address entitled “Generative AI, Productivity, the Labor Market, and Choice Behavior” in which she detailed her outlook on the emerging technologies of generative AI and Large Language Models (LLM) that have seen a surge in excitement and adoption within the last year. In her speech, Governor Cook speaks about the impacts of advancing technology on the labor market and the ethical responsibility of individuals and policy makers to adapt to disruptive changes caused by new technology. Guided by the charge of her office, that charge being a “dual mandate to promote maximum employment and stable prices in the economy” (Cook), Cook clearly defines what is a desirable outcome and what is an undesirable outcome from the emergence of this new technology.\n\n      From a Utilitarian perspective, the Federal Reserve’s mandate to promote maximum employment and stable prices is the ultimate pleasure they strive to achieve, and technology can be seen as a disruptive but inevitable force that they must manage in order to minimize pain and maximize pleasure. Cook admits that “Some of the uses of generative AI may be unsettling.” (Cook) but remains optimistic about the broad benefits that accrue to society as a result of its use. She states “AI makes predictions, but AI does not make choices. Ultimately, human beings are still in control.” (Cook) Technology in this view, is just a tool that can be an amplifier of the potential for pleasure as well as pain. The unintended consequences of technology on the labor market or on society are a lesser pain that must be managed in order to reap the benefits of the greater pleasure that technology makes possible. Utilitarians would agree that the embracing of such a technology, that would benefit many and negatively impact a few, would be the right choice to make. “...while many workers throughout the economy benefit, a smaller set bear the brunt of the negative effects” (Cook)\n\n     If we accept this view of technology, as an amplifier of both pain and pleasure, and we are devoted to the pursuit of a utilitarian ideal of maximum pleasure and minimum pain, we then must have ultimate faith in our ability to minimize the increased level of pain that we have enabled by embracing such technology. What then would happen if that increased level of pain was at such an intensity that any misstep in our ability to minimize it would result in catastrophic pain for all. One cannot help but think of the advancements of nuclear energy and the looming fear of a nuclear holocaust. It is unsettling to think about how the potential for such suffering can be precariously hung on our faith in fallible policy makers and elected officials. Many would say that mutually assured destruction is the preventative measure that keeps such technology from unleashing its horrors upon the masses, but what if the potential for such suffering was contained in technology that was not as obviously harmful as a bomb. What if that technology was spread to governments that were not your own? It seems that to be a utilitarian that embraces all advancements of technology, one must have an unreasonable amount of trust in the current, as well as the future, offices and officials that are charged with regulating such powerful advancements. It would be wiser to embrace technology after much scrutiny and discretion with respect to its most negative outcomes, but even then, omniscience is a quality only attributed to gods.\n\n\nWorks Cited\nCook, Governor Lisa D. “Generative AI, Productivity, the Labor Market, and Choice Behavior” Federal Reserve, 22 Sep. 2023, www.federalreserve.gov/newsevents/speech/cook20230922a.htm. Accessed 26 Sep. 2023",
        "score": 6,
        "url": "https://www.reddit.com/r/Ethics/comments/16to66s/looking_for_feedback/",
        "num_comments": 4,
        "comments": [
            {
                "body": "[deleted]",
                "score": 2
            },
            {
                "body": "Hey mate. I expect I'll have time to engage with your work later, but have you contacted your teacher about this? Maybe they'll be able to give you some one on one time. Approach it like \"do you have any time for extra feedback? I'm having trouble understanding this grade, and really want a stronger understanding of this.\"",
                "score": 1
            },
            {
                "body": " It's very short. Is that the word count you were working to?",
                "score": 1
            },
            {
                "body": "I think you example is admirably ambitious, but doesn't quite fit the scope of the assignment.  They seem to want an example of something that is reported to do good, and then for you to give an example of how it will also do bad.\n\nI think they were expecting you to look at something much simpler, like \"starlink will provide internet to remote communities\" and then for you to say \"access to the internet is arguably a utilitarian good [citation. Eg someone saying internet access is a human right], this system does utilitarian damage in that....[your example here. With citation ideally].\n\nI wouldn't spend so much word count on explaining where the \"article\" comes from.",
                "score": 1
            },
            {
                "body": "I think this is a good answer to op.",
                "score": 1
            }
        ]
    },
    {
        "title": "Ethics of Communication",
        "selftext": "I am new to this forum. I feel that this is the right place to ask my question\n\nFirst, I assume that all social media is corruptible, potentially able to corrupt, and corrupted. \n\nSecond, I accept as true that I am not seeking to corrupt but some people are, and that there is AI unleashed to persuade everyone of everything, a trillion variables that I might encounter. \n\nMy current objective is that given the infinite potential motives from multitude of sources, I must learn several skills and one is how to filter information resulting in an accurate reliability rate.  I’m not sure how to do that.  \n\nI tried to search and was unsuccessful so if the topic is ongoing or occurred or similar, a referral to a substantive thread relevant to me  is appreciated.",
        "score": 6,
        "url": "https://www.reddit.com/r/Ethics/comments/14c1mdg/ethics_of_communication/",
        "num_comments": 7,
        "comments": [
            {
                "body": "Well I think the first thing you must do in these situations is contrast multiple sources. If you read 5 news articles and they all have something in common outside of their narrative then that's probably true.",
                "score": 3
            },
            {
                "body": "Hi 👋 It sounds like you're looking for a way to detect 'baloney.'\n\nTry this [Baloney Detection Kit](https://www.skeptic.com/skepticism-101/baloney-detection-kit-sandwich-infographic/)\n\nAlso, remember that most social media algorithms are set up to \"maximize engagement\" not check for factual or reliable content. If you're sceptical about a source, you can search the title of the article at  [Snopes](https://www.snopes.com/) or use [Media Bias Fact Check](https://mediabiasfactcheck.com/) to see if the source of the article has a history of bias in their posting.",
                "score": 3
            },
            {
                "body": "I'm not quite clear what search term you used, or would use. Can you re-phrase your post as a hypothesis?",
                "score": 2
            },
            {
                "body": "You're asking how to judge if what you see on social media is true?\n\n\"Media literacy\" is maybe what you want to look into. Or is there a more specific post that's bothering you? \n\nI'll suggest one skill:  learning how to analyse arguments into propositions and conclusion, so you can see if they're valid and sound (and learn what those words mean) is tremendously powerful.",
                "score": 2
            },
            {
                "body": "Essentially, I need to vet my sources as a first step. It’s easier to do with news and identifying associations or interest groups but more complicated on social media in general",
                "score": 1
            },
            {
                "body": "Thanks I will use that kit as a process / framework",
                "score": 2
            },
            {
                "body": "I’ll have to think on that.",
                "score": 2
            }
        ]
    },
    {
        "title": "Challenges and Approaches to Ethical Web Scraping",
        "selftext": " \n\nExperts from Ethical Data, Explained, and Apify discuss the role of Apify’s COO, transforming the web into a more programmable and accessible platform, how web scraping companies can maintain ethical standards, and the implications of the HiQ Vs. LinkedIn case.\n\nEpisode highlights:\n\n* Maintaining Ethical Standards as a web scraping company\n* Web scraping for a good cause\n* Ethical consideration for web scraping at scale\n* The role of AI in web scraping\n* The challenges and approaches to responsible web scraping\n\nGain an understanding of scraping websites ethically by clicking the link below:\n\nInsights from the episode: \n\nFull episode here:[ ](https://podcasts.bcast.fm/e/1n3p2pw8-joshua-poddoku-of-apache)[https://podcasts.bcast.fm/e/1n27p1w8-ondra-urban-of-apify](https://podcasts.bcast.fm/e/1n27p1w8-ondra-urban-of-apify)\n\nApple Podcasts: [https://tinyurl.com/za4rs4rp](https://tinyurl.com/za4rs4rp)\n\nSpotify: [https://tinyurl.com/4cd7kxyk](https://tinyurl.com/4cd7kxyk)",
        "score": 3,
        "url": "https://www.reddit.com/r/Ethics/comments/10a3tmb/challenges_and_approaches_to_ethical_web_scraping/",
        "num_comments": 6,
        "comments": [
            {
                "body": "I've worked in a lot of research projects where web crawling has been proposed. In each case it has been massively problematic, and unjustifiable. I'll try to give this a listen.",
                "score": 3
            },
            {
                "body": "If you bothered to explain what the central thing is you're talking about in your post, then your post would be readable.\n\nI have no idea what \"web scraping\" is.",
                "score": 2
            },
            {
                "body": "Very interesting and relevant to the current state of publicly available models. Will check it out for sure!\n\nThanks for sharing  :)",
                "score": 1
            },
            {
                "body": "Giant AI algorithms (think about all the recent art and text generators) need to be fed with humongous amounts of data to be trained. This data is not readily available, but the people who create this algorithms need to \"crawl\" the web and \"scrape\" public data to create a big enough dataset. \n\nThis is done automatically so you create or use existing algorithms that kind of search through the web or chunks of it (many times reddit actually) for useful data. This has creates a lot of ethical problems (e.g. taking online images from artists without caring about copyrights simply cause they are displayed online).",
                "score": 3
            },
            {
                "body": "Cool. I started doing research in this area; I felt the school let me down so I've put that on hold.\n\nMy idea was these art generating AI's are novel structures for collaboration. (Eg: in regards to the scrapings, we're looking at collaborators who never agreed to collaborate.",
                "score": 3
            },
            {
                "body": "Oh, why do you feel that they let you down?\n\nIt's an interesting idea regarding collaboration. However, how do you define collaboration in this case when it is an unwilling one and can it be reconciliated with the artists lack of consent?",
                "score": 1
            }
        ]
    },
    {
        "title": "A multi-disciplinary research agenda for a scientific, scale-free theory of ethics",
        "selftext": "This post is section 3.1. from the article \"A multi-disciplinary view on AI safety research\".  \n\\---  \nHuman values seem to be *learning heuristics* that apply to a particular environment (or a game, if you want: a family, a particular professional environment, a community, a society, or a natural environment) that humans use to form and change their *preference model* (in Active Inference parlance) in lieu of a principled theory of ethics (cf. morality-as-cooperation theory, [Curry et al. (2021)](https://link.springer.com/article/10.1007/s13164-021-00540-x)). The complication here is that usually, intelligent systems (both humans and AIs) have heuristics (intuitions, habits, “S1” thinking) but also have a more principled theory that can be used sometimes for deliberate, “S2” derivations (inferences), and the results of these inferences could be used to train the intuitions (the “habitual network”). In the case of moral values, however, the only thing humans have is a feeling that there should be some principled theory of ethics (and hence the millennia-long quest for finding the “right” theory of ethics in philosophy and spiritual traditions).\n\nMost theories of morality (philosophical, religious, and spiritual ethics alike) that humans have created to date are *deductive reconstructions of a theory of ethics from the heuristics* for preference learning that humans have: values and moral intuitions.\n\nThis deductive approach couldn’t produce good, general theories of ethics, as has become evident recently with a wave of ethical questions about entities that most ethical theories of the past are totally unprepared to consider ([Doctor et al. 2022](https://www.mdpi.com/1099-4300/24/5/710)), ranging from AI and robots ([Müller 2020](https://plato.stanford.edu/entries/ethics-ai/); [Owe et al. 2022](https://www.researchgate.net/profile/Andrea-Owe/publication/363124635_Nonhuman_Value_A_Survey_of_the_Intrinsic_Valuation_of_Natural_and_Artificial_Nonhuman_Entities/links/631f2d5d0a70852150eb38ff/Nonhuman-Value-A-Survey-of-the-Intrinsic-Valuation-of-Natural-and-Artificial-Nonhuman-Entities.pdf?_sg%5B0%5D=started_experiment_milestone&_sg%5B1%5D=started_experiment_milestone&origin=journalDetail&_rtd=e30%3D)) to hybrots and chimaeras ([Clawson & Levin 2022](https://academic.oup.com/biolinnean/advance-article/doi/10.1093/biolinnean/blac073/6643577)) and organoids ([Sawai et al. 2022](https://www.academia.edu/download/66120714/Sawai_et_al_AJOB_Neuroscience_final_draft.pdf)). And as the pace of technological progress increases, we should expect the transformation of the environments to happen even faster (which implies that the applied theories of ethics within these environments should also change), and more such novel objects of moral concern to appear.\n\nThere have been exceptions to this deductive approach: most notably, [Kantian ethics](https://en.wikipedia.org/wiki/Kantian_ethics). However, Kantian morality is a part of Kant’s wider theories of cognition (intelligence, agency) and philosophy of mind. The state-of-the-art theories of cognitive science and philosophy of mind are far less wrong than Kant’s ones. So, the time is ripe for the development of new theories of axiology and ethics from first principles.\n\nThe DishBrain experiment ([Kagan et al. 2022](https://www.sciencedirect.com/science/article/pii/S0896627322008066)) showed that **minimising surprise**, which can also be seen as minimising informational free energy or maximising Bayesian model evidence, is imperative for cognitive (living) systems. As the Free Energy Principle can be seen as an alternative statement of the Principle of Unitarity ([Fields et al. 2022a](https://chrisfieldsresearch.com/qFEP-2112.15242.pdf)), the aforementioned imperative is not yet an informative ethical principle, although it could be seen as the **“meaning** ***of*** **life” principle** (but not meaning *in* life: cf. [Ostafin et al. (2022)](https://www.tandfonline.com/doi/pdf/10.1080/10615806.2021.1994556?needAccess=true&role=button)). Regardless, this makes evident the relevance of **quantum theory** (as the theory of measurement and observation, and the foundation of the theory of semantics, [Fields et al. (2022a)](https://chrisfieldsresearch.com/qFEP-2112.15242.pdf)) and **thermodynamics** ([Friston 2019](https://arxiv.org/abs/1906.10184); [Boyd et al. 2022](https://iopscience.iop.org/article/10.1088/1367-2630/ac4309/meta)) for general theories of cognitive science and ethics.\n\nThe current state-of-the-art theories of cognition[\\[11\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fngq4m4tt929m) are not yet detailed enough to answer the following ethical questions:\n\n* Since every locus of computation (a quantum reference frame, QRF) is embedded in multiple agents (e.g., a neuron is part of a human, who in turn is a part of many overlapping groups of people: the family, societies and communities, organisations, etc.), **with respect to what agent boundary partition and on what scale the QRF should minimise its surprise?** The idea of *complete* QRF autonomy (”QRF egocentrism”; cf. multiscale competence architecture, [Levin (2022a)](https://www.frontiersin.org/articles/10.3389/fnsys.2022.768201/full)) strongly contradicts our ethical intuitions and some hints about what scale-free ethics should be that come from other perspectives, as discussed below. Anyway, multiscale system dynamics are considered within **theories of evolution**[\\[7\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fngm01bfeb0ji). Also, since there are infinitely many system partitions, scale-free ethics will likely need to use **renormalisation group methods**[\\[18\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fny4egnom0fq).\n* An agent can minimise its surprise significantly on the timescale of one year, or a little on the timescale of ten years: how it should choose between these?\n* Quantum theory tells us that observation is inherently contextual. This means that an agent can minimise its surprise in multiple incompatible contexts (e.g., we can measure either particle’s position or momentum, which would be an act of reducing our surprise about this particle, but not both at the same time). How should an agent choose the context in which to minimise its surprise? This question brings the perspective of **quantum/contextual cognition and rationality**[\\[5\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fnlkq0vj0gf) (cf. [Hancock et al. (2020)](https://eprints.whiterose.ac.uk/164118/1/Quantum%20choice%20models.pdf) on quantum moral decision-making).\n* There are tons of evidence in the world around us that survivability/competence (which is the degree to which the agent minimised the surprise about its environment, [Fields & Levin (2022)](https://psyarxiv.com/rdt7f/)) doesn’t correlate with consciousness very much. As an obvious example, *collective agents such as genera or populations survive for much longer than individual organisms, and therefore are more “competent”, but are also far less conscious (if conscious at all)*. Yet, most people have a strong intuition that **consciousness is intimately related to ethics** and value: consciousness “realises” both negative value (through pain and suffering) and positive value: without advanced consciousness, there would be little or no value in the universe. This intuition also appears to be scale-free: even if neurons are conscious to some degree, we regard their collective consciousness (i.e., human consciousness) as more important than the “sum” of the importance of the consciousnesses of individual parts. If this intuition is wrong, though, a scale-free theory of ethics should explain, why. Obviously, this consideration warrants understanding consciousness better, and thus weaving **theories of consciousness** and **network theory** into the synthesis of scale-free ethics.\n* Obviously, the theories of *collective* and “individual” ethics should be compatible with each other. When we consider that the integrated information theory of consciousness (IIT, [Albantakis et al. 2022](https://arxiv.org/abs/2212.14787)) operates with the *causal structure* which is a semantic rather than physical object and that [FDT](https://www.lesswrong.com/tag/functional-decision-theory), which [appears to be close to the “right” group (collective) decision theory](https://www.lesswrong.com/posts/dmjvJwCjXWE2jFbRN/fdt-is-not-directly-comparable-to-cdt-and-edt?commentId=BaE44bb623nXz5HyZ#BaE44bb623nXz5HyZ), also distinguishes itself by featuring abstract objects such as functions and counterfactuals more prominently than “conventional” decision theories, some common ground beings to emerge between theories of semantics and philosophy of mind, theories of consciousness, **theories of collective intelligence**[\\[4\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fn96p0wsw13e5) (including **collective rationality**) and **game theories**.\n* Discussing future potentialities and counterfactuals as objects in a scale-free theory of ethics also requires incorporating the **theories of evolution and regulative (cognitive) development**, which are the scale-free theories for predicting the future of arbitrary *particular*, cognitive systems ([Friston 2019](https://arxiv.org/abs/1906.10184)) at the appropriately abstract level.\n* How should an agent perform ethical deliberation (computation) under time and resource constraints? Thus, in the scale-free theory of ethics, we should account for **bounded rationality** ([Bhui et al. 2021](https://dspace.mit.edu/bitstream/handle/1721.1/144282/Resource_rational_decision_making.pdf?sequence=2&isAllowed=y)) and **resource theories**[\\[23\\]](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#fnzuetgf1glwp). Note that in quantum FEP theory ([Fields et al. 2022a](https://chrisfieldsresearch.com/qFEP-2112.15242.pdf)), it is established that an agent needs to partition its boundary into observable and unobservable sectors (these sectors are used as information and energy sources, respectively), but how specifically an agent should make this partition is yet unexplained.\n* **Immune systems appear to perform some kind of ethical inference**, which is also related to the theories of collective intelligence, agency, and evolution (cf. [group selection](https://en.wikipedia.org/wiki/Group_selection), [identification of friend or foe](https://en.wikipedia.org/wiki/Identification_friend_or_foe) as immune recognition). Yet, how to describe the “ethics of immunity” is not yet clear (cf. [Ciaunica & Levin 2022](https://psyarxiv.com/fgcy5/)). Understanding immunity also necessitates taking the **control theory** perspective (cf. [Doyle 2021](https://www.youtube.com/watch?v=Bf4hPlwU4ys)).",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/10xpxuj/a_multidisciplinary_research_agenda_for_a/",
        "num_comments": 1,
        "comments": [
            {
                "body": "\"I wonder how far I'll have to read through this  wall of text before the title is explained\".\n\nZero upvotes: zero comments. \n\nIs no one on this sub self-aware enough to ask \"why would someone want to read this?\" and then write that as a short abstract before they drop a wall of text.",
                "score": 1
            }
        ]
    },
    {
        "title": "Morphological intelligence, superhuman empathy, and ethical arbitration",
        "selftext": "Morphological intelligence ([Levin, 2022](https://www.frontiersin.org/articles/10.3389/fnsys.2022.768201/full)) is the ability to solve problems in the morphological space, e.g., navigate to a target morphology of the species. For example, tadpoles with scrambled craniofacial organs remodel into largely normal frogs (Vandenberg et al., 2012), and geckos can regenerate missing limbs.\n\nAnother type of morphological intelligence is the ability to remodel oneself for the purpose of solving a different task in the 3D (traditional behaviour) space. An example is insect [metamorphosis](https://en.wikipedia.org/wiki/Metamorphosis), which insects undergo to spread, find mating partners, and reproduce.\n\nIf we assume consciousness [functionalism](https://plato.stanford.edu/entries/functionalism/), undergoing a metamorphosis for the purpose of achieving a *psychological* goal of *experiencing another agent’s qualia and emotions* should be a possible feat for an AI which is at least as powerful as to be able to perform [whole brain emulation](https://www.lesswrong.com/tag/whole-brain-emulation) (or, perhaps, whole organism emulation) and then live through an episode of the organism’s life in a simulated environment.\n\nIt’s unclear whether it is possible to retain and be able to retrieve a conscious memory of “what is it like to be a bat” (or human, or another form of AI) after the AI remodels itself back into its “base” morphology. The retention of symbolic or “indexical” memories should be possible (Hammelman et al., 2016), but the minimal physicalism theory of consciousness ([Fields et al., 2021](https://academic.oup.com/nc/article-abstract/2021/2/niab013/6334115)) suggests that interpreting any state (including recorded memories) through morphologically different computational networks (quantum reference frames) entails different conscious experiences (note: this is my interpretation of the minimal physicalism theory, not a statement made by the authors of the theory).\n\nRegardless, even if retaining the memory of qualitative experience through morphological remodellings is impossible, it should be possible for the AI to **emulate the brains of two organisms of different species at the same time and integrate them into a single conscious experience**. This AI could live through a simulation episode with the experiences of two organisms that we want to weigh (for the purposes of ethical decision-making) both happen at the same time, e.g., a fish being caught and killed, and a human enjoying eating that fish in a restaurant. Then, we can ask this AI whether killing fish for people’s pleasure is moral or not (provided that the AI is also equipped with a [scale-free theory of ethics](https://www.lesswrong.com/posts/opE6L8jBTTNAyaDbB/a-multi-disciplinary-view-on-ai-safety-research#3_1__Scale_free_axiology_and_ethics)).\n\nTherefore, given the human brain’s low capability for morphological remodelling and thus low capacity for empathy on the absolute, non-speciesist scale, it seems ethically **inappropriate that humans remain “in the driving seat” in what comes to ultimate moral decision-making in the future advanced civilisation**. This also casts doubt on the idea of “[coherent extrapolated volition](https://www.lesswrong.com/tag/coherent-extrapolated-volition)” (CEV), which would, as usually conceptualised, be incapable of extrapolating beyond the human capacity for universal empathy. (This is not my only reason to think that CEV doesn't make sense. Other huge (and, as it seems to me, irreparable) problems with CEV are the open-endedness of the civilisational journey and the inherent contextuality of ethical inferences and moral value.)\n\nThis post has been originally published on [LessWrong](https://www.lesswrong.com/posts/6EspRSzYNnv9DPhkr/morphological-intelligence-superhuman-empathy-and-ethical).",
        "score": 2,
        "url": "https://www.reddit.com/r/Ethics/comments/111c3ja/morphological_intelligence_superhuman_empathy_and/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Challenges and Approaches to Ethical Web Scraping",
        "selftext": " \n\nExperts from Ethical Data, Explained, and Apify discuss the role of Apify’s COO, transforming the web into a more programmable and accessible platform, how web scraping companies can maintain ethical standards, and the implications of the HiQ Vs. LinkedIn case.\n\nEpisode highlights:\n\n* Maintaining Ethical Standards as a web scraping company\n* Web scraping for a good cause\n* Ethical consideration for web scraping at scale\n* The role of AI in web scraping\n* The challenges and approaches to responsible web scraping\n\nGain an understanding of scraping websites ethically by clicking the link below:\n\nInsights from the episode: \n\nFull episode here:[ ](https://podcasts.bcast.fm/e/1n3p2pw8-joshua-poddoku-of-apache)[https://podcasts.bcast.fm/e/1n27p1w8-ondra-urban-of-apify](https://podcasts.bcast.fm/e/1n27p1w8-ondra-urban-of-apify)\n\nApple Podcasts: [https://tinyurl.com/za4rs4rp](https://tinyurl.com/za4rs4rp)\n\nSpotify: [https://tinyurl.com/4cd7kxyk](https://tinyurl.com/4cd7kxyk)",
        "score": 1,
        "url": "https://www.reddit.com/r/Ethics/comments/10f3k3u/challenges_and_approaches_to_ethical_web_scraping/",
        "num_comments": 0,
        "comments": []
    },
    {
        "title": "Crisis Text Line gives its online text conversations with people in their darkest moments to its for-profit entity. Ethical?",
        "selftext": "Crisis Text Line is one of the world’s most prominent mental health support lines, a tech-driven nonprofit that uses big data and artificial intelligence to help people cope with traumas such as self-harm, emotional abuse and thoughts of suicide.\n\nBut the data the charity collects from its online text conversations with people in their darkest moments does not end there: The organization’s for-profit spinoff uses a sliced and repackaged version of that information to create and market customer service software.\n\nCrisis Text Line says any data it shares with that company, Loris.ai, has been wholly “anonymized,” stripped of any details that could be used to identify people who contacted the helpline in distress.\n\nIn turn, Loris has pledged to share some of its revenue with Crisis Text Line. The nonprofit also holds an ownership stake in the company, and the two entities shared the same CEO for at least a year and a half. The two call their relationship a model for how commercial enterprises can help charitable endeavors thrive.\n\nStudies of other types of anonymized datasets have shown that it can sometimes be easy to trace the records back to specific individuals. And are the people who text their pleas for help actually consenting to having their data shared? Did they really understand the approximately 50-paragraph disclosure the helpline offers a link to when individuals first reach out?\n\nGiant companies like Facebook and Google have built great fortunes based on masses of deeply personal data. But information of equal or greater sensitivity is also in the hands of nonprofit groups that fall outside federal regulations on commercial businesses — with little outside control over where that data ends up.\n\nMore from Politico: [https://www.politico.com/news/2022/01/28/suicide-hotline-silicon-valley-privacy-debates-00002617](https://www.politico.com/news/2022/01/28/suicide-hotline-silicon-valley-privacy-debates-00002617)",
        "score": 4,
        "url": "https://www.reddit.com/r/Ethics/comments/sqb9pb/crisis_text_line_gives_its_online_text/",
        "num_comments": 6,
        "comments": [
            {
                "body": "The joys of capitalism, where ethics don't really matter as long as a profit is pursued. But in my view this is horribly unethical, but trying to get Americans to fund this service through tax dollars and proper privacy safeguards would be highly unlikely.",
                "score": 2
            },
            {
                "body": "yeah this is why I think we NEED a law where 50 bjillion page \"agreement\" bs needs to stop. if your not explicitly agreeing to something, then it should be illegal. one cannot 'passively' agree to something, ESPECIALLY not something so complicated that you need to hire a lawyer to interpret it!\n\nso yes, I agree that sharing an implied private conversation through some amorphous \"agreement\" clause is unethical.",
                "score": 2
            },
            {
                "body": "[deleted]",
                "score": 2
            },
            {
                "body": "This is a nonprofit - therefore, it's supposed to exist NOT for profit. And people are interacting with it because it's a nonprofit, with no idea about the for-profit (capitalist) entity.",
                "score": 1
            },
            {
                "body": "It is a bummer. But just because something is huge and well known doesn't mean it's ethical",
                "score": 1
            },
            {
                "body": "It is really disappointing. My skepticism has been regarding the lack of testimonials from people who have supposedly been helped by the service, though I've been very impressed with the amount of training volunteers get.",
                "score": 1
            },
            {
                "body": "Well yes, but the point is that in a capitalist system you can't expect non-profits to exist without some form of capital being collected. Which means that they will come under the control of whomever is giving them the funds to keep operating.",
                "score": 1
            }
        ]
    },
    {
        "title": "Machine learning applied to health care",
        "selftext": "Hello everyone,\n\nI'm aware of multiple issues regarding ethics in the usage of Artificial Intelligence systems into the clinical field. However, there is one specific problem that people seems to take for granted that I can't understand.\n\nImagine I have a population of patients that need a different intervention (A or B) depending on their pathology details. Suppose that the role of a doctor is to decide which clinical patients need to go to intervention path A and which patients to the intervention path B. This doctor will do the task with a certain accuracy (that is very likely to be less then 100%) : he will do mistakes.\n\nThen suppose there is a machine learning algorithm that can predict the intervention path for the same category of patients with *an accuracy* that is greater than the doctor's accuracy described above (we could also say \"than the average accuracy of all doctors in the world\" but I think the concepts behind stay the same).\n\nIn my opinion, it would be logic to let this algorithm available to doctors and push them to decide intervention paths on the basis of what the algorithm predict.\n\n\\- - -\n\nPeople often say that the use of machine learning in this situation is a problem: if the machine prediction turns out to be wrong, who is responsible for that?\n\nWell, I can’t see a problem here. There are a lot of limits of machine learning algorithms (bias, privacy, security ecc..) but here it seems we are overthinking it.\n\nWhy should we found a responsible?\n\nThe use of drugs is quite accepted over the world. Drugs are extremely useful because they solve problems (with a small number of cases in which they cause severe side effects). In those unfortunate cases we kind of accept that this can happen. There are limited cases in which drugs fail, and we attribute the “responsibility” to the chance.\n\nWhy we couldn’t do that also with machine learning algorithms that perform better than human? (as long as they are well statistically validated, like drugs)\n\nAgain, i’m referring to a specific problem of responsibility, i know that there are other reasons why we may not want to use AI systems.",
        "score": 6,
        "url": "https://www.reddit.com/r/Ethics/comments/gk6s29/machine_learning_applied_to_health_care/",
        "num_comments": 12,
        "comments": [
            {
                "body": "Is this another homework assignment?  Your English is horrible, by the way.\n\nYou fail to distinguish between diagnosis of the malady and the action taken in response to it, and you do not need to *suppose* an AI algorithm that is better at diagnosis than an actual doctor.  Such things already exist and have existed for *decades*!\n\nThe patient is not a sack of potatoes, you know.  It is *they* who decide what is done with their body, *and never the doctor,* except in an emergency when the patient is incapacitated, which is rare.  Decisions as to the *action* taken are not even *candidates* for AI intervention, although AI can also assist the patient to make a decision consistent with their values and objectives.  This is because an action to be taken or not, unlike a diagnosis, is not a *fact*, and not subject to probabilistic estimation.  An AI algorithm does not say, \"do *X*.\"  It says, \"*Y*% probability of disease *A*, *Z*% probability of disease *B*.\"  This can only be \"wrong\" in the sense that it has a \"bug,\" i.e., the relevant data were not represented or combined correctly, so there is never a problem of responsibility where no such bugs exist, because it is not recommending an action.  (Any action, of course, must take into account the full lottery of possible disease conditions and their likelihoods.)\n\nDecisions regarding action are always made on the basis of values, which will differ from one patient to another.  For one patient, a given side-effect may be acceptable. For another, it may not.  For a third, acceptable only below a certain probability, given the other alternatives, etc.  The doctor also cannot impose their will on the patient because the doctor needs the patient's cooperation in the treatment, in most cases, unless, of course, you believe that patients should be slaves to their doctors, which is what you apparently assume.\n\nHave you ever been in a medical situation in your life?",
                "score": 1
            },
            {
                "body": ">Is this another homework assignment? Your English is horrible, by the way.\n\nLol, this is not an homework but it derives from a question during a lecture that I wanted to deepen. I'm sorry if my english bothers you, i'm trying to improve. Anyway, I didn't express myself properly / I actually didn't think of patient perspective. Rethinking it now, of course doctors never impose anything to people in non-emergency situations (although reading again my words feels like I didn't). However, if we do not even consider the action / decision part, replacing in my text the words \"decide\" with \"propose\", my doubt persists.\n\n&#x200B;\n\n>An AI algorithm does not say, \"do X.\" It says, \"Y% probability of disease A, Z% probability of disease B.\"\n\nYes, but the information regarding A and B probabilities are crucial for determine what would be the best X to do, in physiological terms. Than, if the actual \"cure\" will be X1 or X2 or X3 doesn't matter (for this argument). Doctors competence include determine what would be the best \"X\" for the patient (considering diagnosis, anamnesi, personality, etc..).\n\n**My concern is** :  if a good AI system informs a doctor regarding the probabilities of a patient to belong to \"diagnosis A\" or \"diagnosis B\" group. Shouldn't the doctor be morally driven to consider the AI suggestions in the formulation of the best \"X\"?\n\nAs a consequence: in this case an AI error is not different from a drug that manifest side effects by chance.  Right?",
                "score": 1
            },
            {
                "body": ">Doctors competence include determine what would be the best \"X\" for the patient (considering diagnosis, anamnesi, personality, etc..).\n\nNo.  It is the doctor's duty to inform the patient of their likely conditions, the available therapies, and their respective risks, side effects, and outcomes.  The patient then decides.  The Dr. may recommend, but it is of no legal consequence.  This is called \"informed consent.\"  It is key to medical autonomy.  Malpractice occurs not when the Dr. recommends something that doesn't work (because this can always occur), but when they fail to adequately inform the patient (or botch the execution of the treatment).\n\n>Shouldn't the doctor be morally driven to consider the AI suggestions in the formulation of the best \"X\"?\n\nThe medical community has known about AI programs that are superior to their expertise for a long time.  They nevertheless do not use them.  I think that ethics can only conclude that they should.  The resistance of the medical profession appears to be due entirely to culpable self-interest.  However, it is never the case that AI recommends a treatment.  All treatments have efficacies, risks, side-effects, and costs that must be considered, which cannot be decided without the active participation of the patient.\n\n>the information regarding A and B probabilities are crucial for determine what would be the best X to do, in physiological terms\n\nyes, but that information is *never enough* to determine the treatment.\n\n>As a consequence: in this case an AI error is not different from a drug that manifest side effects by chance. Right?\n\nNo, because in a drug risk scenario, the patient evaluated and accepted the risk.  In an AI error, they didn't.  They were mislead by bad, buggy AI.",
                "score": 1
            },
            {
                "body": "Yes ok, i'm oversimplifying it too much (probably because i haven't clear ideas on how the medical word works).\n\n>The resistance of the medical profession appears to be due entirely to culpable self-interest\n\nCan you explain that?\n\n&#x200B;\n\n>No, because in a drug risk scenario, the patient evaluated and accepted the risk. In an AI error, they didn't. They were mislead by bad, buggy AI.\n\nWell, it depends. If a doctor use an AI and tell the patient \"look, I thought you had D1, but AI tells me that you have D2. This AI have a validated mean accuracy of 98%. If we assume you have D2 possible treatments could be X2a, X2b...with some risks ect... if we do not trust AI and assume that you have D1 possible treatments could be X1a, X1b etc... with relative risks.\"\n\nTo \"consider AI suggestions\" could be something like this. I don't mean that doctors should blindly follow AI predictions.\n\n&#x200B;\n\nthanks for the answers tho, if I become annoying with questions let me know",
                "score": 1
            },
            {
                "body": ">Can you explain that?\n\nThe medical profession doesn't want to admit that AI gives better diagnostic pradictions than they do (and this does **not** mean that the diagnostic predictions are \"right,\" in the sense of AI saying, \"the patient has *X*\" and the patient really having *X*, but right in the sense that AI says that there is a *Y*% chance that the patient has condition *X*, and *Y*% of the time *that AI says this*, the patient actually has condition *X*).  They don't want to admit it because of the loss in prestige, income, and power that would result.  If cheap software could give them a better diagnosis than an expensive, highly-trained medical doctor, why would anyone pay for the doctor?\n\n>Well, it depends. . . .  This AI have a validated mean accuracy of 98%.\n\nWTF is \"validated mean accuracy\"?  Again, there are only probabilities.  The data say that *X*% of the time, patients with symptomology of *Z* have condition *A*.  They either do or they do not (in which case the AI was programmed wrong), but with regard to any specific instance, assuming the AI has no stupid mistake in it, saying that the patient has condition *A* with *X*% chance is neither right nor wrong with respect to that patient, whether or not they have condition *A*, because AI did not say that they had condition *A*, but only that there was an *X*% chance that they had it.",
                "score": 1
            },
            {
                "body": "Again, there are only probabilities\n\nWell, it's not true, it depends on the model. I was making the example with a two class boolean prediction for the sake of simplicity. Anyway, if we have percentages it doesn't change much, as long the patient is well informed about what are those AI results and possible consequences of various scenarios.\n\n&#x200B;\n\n>WTF is \"validated mean accuracy\"?\n\nAccuracy makes sense if we think of a classification problem, with percentages maybe there are other error measures. But the meaning behind it is the same: a measure of how well the model perform. To validate this measure means test the capacity of generalization of the AI system ,at least for machine learning models the performance on training data is completely different by the performance on unseen data, the test sample need to be large enough, the performance needs to be statistically significant etc...\n\nSo I meant \"this AI has a performance described by a certain measure and it has been tested with statistical methods to guarantee a robust performance\"",
                "score": 1
            },
            {
                "body": ">the test sample need to be large enough, the performance needs to be statistically significant etc...\n\nYou are talking about validating a model.  If I want to establish that the relation between *x*, *y*, and *z* is *z* = *ax* + *by* + *c*, n.b., a *linear* model, then I need to worry about sample size, etc.  Once validated, of course, I can program the model into an AI application.\n\nBut treating a patient is action taken under uncertainty.  I am not trying to prove any theories.  I need not even find out what the patient's disorder ever actually *was*.  I am just trying to bring about good outcomes.  Therefore, I can dispense with a model.  All I need are conditional probabilities:  *X*% of the population has condition *Y*, *Z*% of the patients with condition *Y* exhibit symptom *A*, etc.  I never need to go beyond the data to posit some model or other.  What you are calling \"percentages\" is called Bayesianism.  The diagnostic programs that actually work, and work better than doctors, are Bayesian programs.\n\nEdit:  Changed \"Bayesian models\" to \"Bayesian programs\"",
                "score": 1
            },
            {
                "body": "Ok, we are talking about something quite different. If don't have time just jump to the conclusion under the dashed line.\n\nAI is a huge field. My original question was intended to refer to Machine Learning (ML): the most fruitful subgroup of AI nowadays. Then I start to generalize to AI because I think my doubts could be applied to AI in general. However:\n\n&#x200B;\n\n> The diagnostic programs that actually work, and work better than doctors, are Bayesian programs. \n\nActually, according to my knowledge state of the art on specific clinical predictions is driven by ML and deep learning methods (a subgroup of ML).  For example in cancer detection in MRI images or psychiatric disorders diagnosis based on functional and structural neuroimaging.  \n\n&#x200B;\n\n>  What you are calling \"percentages\" is called Bayesianism \n\nWhy? you took percentages in discussion ... according to me they can be percentages, classes, regressors, everything that could be interpreted as an output.\n\nWhy are you limiting everything to percentages? And even considering only percentages...they are absolutely not just \"Bayesianism\". A simple support vector machine with a softmax layer at the end will output some percentages, this doesn't make it a Bayesian model / program.\n\nThere are unsupervised ML models that exploit Bayesian theorem (Restricted Boltzmann Machines, Deep Belief Networks...) but the best performances are obtained by supervised learning, with models trained with backpropagation (in reality is more a mixture of the two). By the way, again, at least in ML, to validate a model means to test if it generalize to new data.\n\n\\- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n\nIn any cases, we went a bit off topic.\n\nTrying to integrate what we are saying in an example of something that COULD happen (not saying that would happen every day with every patient with every doctor): a doctor use an AI that has been tested and considered to performed better than human, this doctor use AI output to formulate some hypothesis of diagnosis, the doctor tells the patient about the AI output, his hypothesis, possible interventions, potential risks etc.. collaborating with the patient to find a solution.\n\nConsider if the patient is optimistic. He say that he completely trust AI, so encourages the doctor to base the intervention on the AI results. The doctor do that and the patient is cured. Everyone is happy.\n\nOne year later, another optimistic patient come to the same doctor, but that day the same AI perform bad on the single case (it is possible, performance is variable, that's why I was referring to average performance) and somehow because of that the patient dies. **I think we shouldn't blame the AI, we shouldn't blame the doctor, we shouldn't blame anyone. Exactly in the same way we don't blame anyone if a doctor prescribes paracetamol for some fever and the sick person dies because of a unpredictable side effect.**",
                "score": 1
            },
            {
                "body": ">psychiatric disorders diagnosis based on functional and structural neuroimaging. \n\nNo it isn't.  Doctors can't even agree on who has a disorder, so how could functional neuroimaging correlate to it?  The studies that have explored this avenue have failed.  Sorry, I can't cite the source.\n\n>>What you are calling \"percentages\" is called Bayesianism\n\n>Why?\n\nBecause that is how I was using them.  The first Bayesian diagnostic software was around in the 1980's, and it was better than medical doctors then, and it still isn't used, so the problem is not technical.\n\n>Why are you limiting everything to percentages?\n\nBecause then you don't need to worry about linear models, validity, regression, or any of that steam-era classical statistical stuff.\n\n>this doctor use AI output to formulate some hypothesis of diagnosis\n\nWhy do you think that the diagnosis has to be limited to a single thing?  Why can it not be a probability distribution over possible diagnoses?  You say that it is really the same problem, but it isn't, for reasons I have already stated, i.e., if you allow a probability distribution over possible diagnoses, which is what any AI or medical doctor will give you anyway, your ethical (and practical) problem disappears, i.e., there cannot be a \"wrong diagnosis.\"\n\nPlease reread and be sure you understand my previous messages before replying.  Otherwise, you reduce me to repeating myself.",
                "score": 1
            },
            {
                "body": "Ok, before going further I need to clarify something.\n\n&#x200B;\n\n>\"your ethical (and practical) problem disappears\"\n\nThe ethical problem is not \"my problem\" and i'm not sure you get what is my problem.  Initially, I said that I couldn't see the problem within a critique to AI, that was my problem.\n\nCRITIQUE TO AI (***ethical problem***): According to what I understood from my lecture, one critique moved to AI applied to healthcare is a lack of responsabile to \"blame\" in case of \"errors\".\n\n1- I might be wrong, but it seems you have problems with the \"errors\" part, I don't. Errors exists in both human reasoning and AI computations, even probability distributions can be well representative or misrepresentative descriptions of the world.\n\n2- (***my problem***) I accept the \"errors\" part: AI can make errors. I have problem with the \"blame\" part. I don't understand why errors of AI should be considered differently by unintentional and \"accepted\" errors that humans do everyday. I would accept AI errors as well.\n\n3- If you see a problem with my description of the critique to AI (if you think that no one move this critique to AI) please tell me, because I'm assuming I understood correctly from the lecture.\n\n&#x200B;\n\nTo which problem disappearing were you referring?",
                "score": 1
            }
        ]
    },
    {
        "title": "Ethics of mass surveillance?",
        "selftext": "Lets talk about for example [this](https://www.europarl.europa.eu/RegData/docs_autres_institutions/commission_europeenne/com/2020/0568/COM_COM(2020)0568_EN.pdf) which is a legislative proposal to use artificial intelligence to screen online communications, including end to end encrypted services like signal.\n\nThe [cons](https://www.patrick-breyer.de/en/posts/message-screening/) are listed there for example, and I must say that it does look bad, but on closer inspection most of the arguments against seem quite invalid.\n\nSome examples:\n\nOn the \"How did we get here?\" Paragraph on the patrick breyer website they instantly use the words \"error-prone artificial intelligence\". And yes of course there will be mistakes and errors, thats not the point in my opinion. Its the fact that you're gonna filter with artificial intelligence to protect privacy as much as possible, and only allow manual screening on items already identified by the artificial intelligence as possible child pornography or grooming. The data is then obviously disclosed without human verification, because that is the next step in the process and the results of the AI need to be verified. To be honest similar altrough more automated screening has already been happening for years to identify and prevent the spread of spam emails. Of course spam emails are not as troublesome as possibly putting someone in jail or identifying a victim of child abuse so a manual check definitively has to happen after the automated screening results.\n\nWhile I understand that manually screening possible nude pictures which can contain false positives seems really worrisome, on closer inspection it is nothing more than a routine job by a trained individual, and I fail to see how it is any worse or more uncomfortable than for example getting your genitals inspected by a doctor in a routine checkup. Yea sure I don't like the idea of getting possible sexting conversations or nudes of me being viewed without my knowledge, but I highly doubt children like being raped... one is mildly uncomfortable and the other one can affect you for the rest of your life.\n\nThen a bit further you can see \"intelligence service or hackers may be able to spy on your private chats and emails\" yea sure, as much as they can already. Remember that there is no need for end to end encryption of one of the end devices is infected, text and images can be read of the screen of most apps anyways, its not like this will open up some magical backdoor which there would be no other way around. At best it would be a bit more inconvenient. A hacker would only be able to spy on your facebook messages if he was able to hack facebook, facebook doesn't even use end to end encryption by default so it wouldn't even change anything there, only software that currently uses end to end encryption would be slightly less secure in theory. Its not a big gaping security hole any hacker with a laptop, some youtube videos and too much free time can just bypass, even a certified security professional (my bet is you would need an entire team of them) would have a lot of difficulty with this or would be unable to accomplish set task. So pretty much only state sponsored hackers or professional ransomeware groups would have slightly easier access.\n\nThey also talk about installing a backdoor in end to end encrypted messaging services, which would then supposedly also open access for foreign intelligence agencies and criminals, this statement is completely false. It would depend entirely on how the \"backdoor\" is implemented. Just because a service isn't end to end encrypted doesn't mean its not encrypted. It would just mean that only the main server would be able to decrypt the message, read it, and encrypt it again before sending it to the recipient. It just means that the setup is not fully trustless (server can now read the messages and you have to put trust in whichever company or app you use to not abuse this power). \n\nThen the argument about 90% of the reported content not being illegal, yea sure that could be true. The fact that 10% of the content is actually correct is mindblowing in my eyes, since that would mean if you would take all random images and scan them for nudes, 90% of the minors would be family pictures and 10% child pornography. It might not seem like a lot but I'm pretty sure that you can imagine how many family photos with a nude are going around, so to still have that much correctly positive seems to signal that this might actually work\n\nThen another argument says that an algorithm decides who is suspect and who is not, and that this is not transparent. I don't have a lot to add to this except that we can't really make it more transparent and that it is still effective, its a bit of a middle way and there is a reasonably good reason to investigate, don't forget that the suspected content is still suspicious, even if it are just family pictures with a nude minor in them. Its still a reasonably okay reason to investigate this in my opinion.\n\nIndiscriminate messaging and chat control creates a precedent and opens the floodgates. I must agree with this, its like opening pandoras box. Which is in my eyes why we need to make very clear regulations about in which cases an AI is allowed to identify illegal stuff like this, in my eyes this should only happen for really bad stuff where people get hurt/traumatised. So for example reporting suicide this way also seems like a great option for me, but someone admitting to speeding on the highway should not be prosecuted or searched using these methods in my eyes. \n\nI have nothing to add about the fact that it lacks a legal basis, since I'm not sure of that. All I know that they could definitively add a legal basis for it since legislation is written by humans. On the statement that it violates the principle of proportionallity I highly disagree, I don't think it is okay to downplay childabuse for just having no AI scanning messages and forwarding suspicious content to the authorities to have it checked. This is not even mentioning how much money it would save society by reducing treatment cost and the reduction of loss of productivity to society. Remember some of these children are alone and there is nothing in their power they can do to defend themselves or get help, in my eyes it would be our duty to try and help them using any means possible, even if that means that this touches something close to our right to privacy, which in my opinion it doesn't even any more than how google uses our info for ads, except this is used for fighting child sexual abuse \n\nThen after this they say that safe spaces for sexual abuse victims are destroyed by monitoring, I fail to see how thats the case. Talking about the abuse is even very unlikely to set off these automated detection systems since thats not the type of content they are trained on, and even then it would quickly be discarded because it was a false positive.\n\nAnother argument is that it encourages offenders to go further underground, and while this is true stuff like grooming ect would be a lot harder. You can easily stalk kids online, its much harder to bait them to onion services or a self hosted service and groom them from there, the kids keep their heads above the water while the offenders share content further underground because a lot of them got busted by not doing that.\n\n\nIn my eyes it seems ethical to implement artificial intelligence for mass screening of child abuse. I really don't think this is the same thing as mass surveillance. Journalists and people really dependent on their privacy can still use other solutions if they really need end to end encryption. I'm all for privacy but am confident that mass screening of these types of content would be very effective, and don't think it is ethically responsible to look the other way and not screen for it when it is perfectly possible to implement and would probably protect a lot of children. I fail to see how it is different from being searched at the airport. If you really dislike the idea of your online communications being screened there is always the option to use other solutions which provide more privacy. Children need to be protected on the most used channels so I completely agree with manditory automated screening on these services, even though it was quite shocking at first and I didn't really like the idea.\n\nAny points I missed or someone who could add? How is screening unethical? And are you sure that its worth turning a blind eye to child abuse for those reasons?",
        "score": 7,
        "url": "https://www.reddit.com/r/Ethics/comments/ow4isb/ethics_of_mass_surveillance/",
        "num_comments": 2,
        "comments": [
            {
                "body": "I came in a little unsure because obviously just the idea of my shit being screened makes me uncomfortable, but you make a good point. In this case the ends would justify the means, speaking strictly about the ethics behind it, in my opinion. Considering all factors though, the main one being that humans would be behind this program, I don’t imagine it can be effectively implemented without being corrupted in some way. The technology would absolutely be abused, and likely is as we speak. However the minor sacrifice of a stranger seeing your private life can be made if even a single child is removed from an abusing situation as a result, especially if as you mentioned, people are granted immunity from minor infractions found through screening. I would say those that don’t directly harm other people. Then again that just raises the question of where do we draw the lines? There’s a lot of grey area here.",
                "score": 1
            },
            {
                "body": "As is with most stuff in ethics of course, sometimes it's more about choosing the least bad option. Maybe there are better ways to implement something like this that people didn't consider yet or for which the technology hasn't been developed yet. And then about where do we draw the line? I'd say there basically is no line except the line drawn by culture, Imagine something like this being implemented. People notice that it works and it has a big positive effect, after a few years it might be possible to add other, also serious offences. Then over the course of decades or even centuries the line can keep moving slowly and you would be giving culture the time to adapt to it. It's more about how people perceive it and how it affects them personally. Of course people don't want google to report speeding offences with their location data. But once you get to the point where driving too fast is not socially accepted anymore and barely anyone does it, there will only be a minority against. It's all about perspective, the important thing is how to secure it and make sure it cannot be abused (just like nuclear weapons don't just get hacked or fired accidentally)",
                "score": 2
            }
        ]
    }
]